{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "It'sOnlyaMungerofTime_8-31-18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "1k0RM2bQyXgd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In [my last Colaboratory notebook](https://colab.research.google.com/drive/1tm9mnmoF1mxPIiyJo1gp9xfCxBxJcrfu) I drew up some functions for wrangling structured datasets. An extension of this method could be to incorporate a function that evaluates columns in a dataframe to identify the presence of date or time series data. In this notebook we'll create this function to automate the identification of time series data and update our automunge(.) function to include this new category of data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ALvzpOmY0WX4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1) Import data pre-processing functions from last notebook"
      ]
    },
    {
      "metadata": {
        "id": "kGXhOsSe0dso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3UZJg1D-0ja8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#process_numerical_class(mdf_train, mdf_test, column)\n",
        "#function to normalize data to mean of 0 and standard deviation of 1 from training distribution\n",
        "#takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\\\n",
        "#and the name of the column string ('column') \n",
        "#replaces missing or improperly formatted data with mean of remaining values\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#expect this approach works better when the numerical distribution is thin tailed\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "#imports\n",
        "from pandas import Series\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def process_numerical_class(mdf_train, mdf_test, column):\n",
        "     \n",
        "    \n",
        "  #convert all values to either numeric or NaN\n",
        "  mdf_train[column] = pd.to_numeric(mdf_train[column], errors='coerce')\n",
        "  mdf_test[column] = pd.to_numeric(mdf_test[column], errors='coerce')\n",
        "\n",
        "  #get mean of training data\n",
        "  mean = mdf_train[column].mean()    \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_train[column] = mdf_train[column].fillna(mean)\n",
        "  mdf_test[column] = mdf_test[column].fillna(mean)\n",
        "\n",
        "  #subtract mean from column for both train and test\n",
        "  mdf_train[column] = mdf_train[column] - mean\n",
        "  mdf_test[column] = mdf_test[column] - mean\n",
        "\n",
        "  #get standard deviation of training data\n",
        "  std = mdf_train[column].std()\n",
        "\n",
        "  #divide column values by std for both training and test data\n",
        "  mdf_train[column] = mdf_train[column] / std\n",
        "  mdf_test[column] = mdf_test[column] / std\n",
        "\n",
        "\n",
        "  return mdf_train, mdf_test\n",
        "  \n",
        "\n",
        "  \n",
        "#process_binary_class(mdf, column, missing)\n",
        "#converts binary classification values to 0 or 1\n",
        "#takes as arguement a pandas dataframe (mdf), \\\n",
        "#the name of the column string ('column') \\\n",
        "#and the string classification to assign to missing data ('missing')\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#missing category must be identical to one of the two existing categories\n",
        "#returns error message if more than two categories remain\n",
        "\n",
        "\n",
        "def process_binary_class(mdf, column, missing):\n",
        "    \n",
        "  #replace missing data with specified classification\n",
        "  mdf[column] = mdf[column].fillna(missing)\n",
        "\n",
        "  #if more than two remaining classifications, return error message    \n",
        "  if len(mdf[column].unique()) > 2:\n",
        "      print('ERROR: number of categories in column for process_binary_class() call >2')\n",
        "      return mdf\n",
        "\n",
        "  #convert column to binary 0/1 classification\n",
        "  lb = preprocessing.LabelBinarizer()\n",
        "  mdf[column] = lb.fit_transform(mdf[column])\n",
        "\n",
        "  return mdf\n",
        "\n",
        "  \n",
        "#process_text_class(mdf_train, mdf_test, column)\n",
        "#preprocess column with text classifications\n",
        "#takes as arguement two pandas dataframe containing training and test data respectively \n",
        "#(mdf_train, mdf_test), and the name of the column string ('column')\n",
        "\n",
        "#note this trains both training and test data simultaneously due to unique treatment if any category\n",
        "#missing from training set but not from test set to ensure consistent formatting \n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with onehot encodings\n",
        "#with columns named after column_ + text classifications\n",
        "#missing data replaced with category label 'missing'+column\n",
        "#any categories missing from the training set removed from test set\n",
        "#any category present in training but missing from test set given a column of zeros for consistent formatting\n",
        "#ensures order of all new columns consistent between both sets\n",
        "#returns two transformed dataframe (mdf_train, mdf_test)\n",
        "\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "\n",
        "def process_text_class(mdf_train, mdf_test, column):\n",
        "\n",
        "  #replace NA with a dummy variable\n",
        "  mdf_train[column] = mdf_train[column].fillna('_missing')\n",
        "  mdf_test[column] = mdf_test[column].fillna('_missing')\n",
        "\n",
        "\n",
        "  #extract categories for column labels\n",
        "  #note that .unique() extracts the labels as a numpy array\n",
        "  labels_train = mdf_train[column].unique()\n",
        "  labels_train.sort(axis=0)\n",
        "  labels_test = mdf_test[column].unique()\n",
        "  labels_test.sort(axis=0)\n",
        "\n",
        "  #transform text classifications to numerical id\n",
        "  encoder = LabelEncoder()\n",
        "  cat_train = mdf_train[column]\n",
        "  cat_train_encoded = encoder.fit_transform(cat_train)\n",
        "\n",
        "  cat_test = mdf_test[column]\n",
        "  cat_test_encoded = encoder.fit_transform(cat_test)\n",
        "\n",
        "\n",
        "  #apply onehotencoding\n",
        "  onehotencoder = OneHotEncoder()\n",
        "  cat_train_1hot = onehotencoder.fit_transform(cat_train_encoded.reshape(-1,1))\n",
        "  cat_test_1hot = onehotencoder.fit_transform(cat_test_encoded.reshape(-1,1))\n",
        "\n",
        "  #append column header name to each category listing\n",
        "  #note the iteration is over a numpy array hence the [...] approach\n",
        "  labels_train[...] = column + '_' + labels_train[...]\n",
        "  labels_test[...] = column + '_' + labels_test[...]\n",
        "\n",
        "\n",
        "  #convert sparse array to pandas dataframe with column labels\n",
        "  df_train_cat = pd.DataFrame(cat_train_1hot.toarray(), columns=labels_train)\n",
        "  df_test_cat = pd.DataFrame(cat_test_1hot.toarray(), columns=labels_test)\n",
        "\n",
        "\n",
        "  #Get missing columns in test set that are present in training set\n",
        "  missing_cols = set( df_train_cat.columns ) - set( df_test_cat.columns )\n",
        "  #Add a missing column in test set with default value equal to 0\n",
        "  for c in missing_cols:\n",
        "      df_test_cat[c] = 0\n",
        "  #Ensure the order of column in the test set is in the same order than in train set\n",
        "  #Note this also removes categories in test set that aren't present in training set\n",
        "  df_test_cat = df_test_cat[df_train_cat.columns]\n",
        "\n",
        "\n",
        "  #concatinate the sparse set with the rest of our training data\n",
        "  mdf_train = pd.concat([df_train_cat, mdf_train], axis=1)\n",
        "  mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)\n",
        "\n",
        "\n",
        "  #delete original column from training data\n",
        "  del mdf_train[column]    \n",
        "  del mdf_test[column]\n",
        "\n",
        "\n",
        "  return mdf_train, mdf_test\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ddKagyPMl0s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2) Define process_time_class(.) function"
      ]
    },
    {
      "metadata": {
        "id": "aPsJ7GF3tgFa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#process_time_class(mdf_train, mdf_test, column)\n",
        "#preprocess column with time classifications\n",
        "#takes as arguement two pandas dataframe containing training and test data respectively \n",
        "#(mdf_train, mdf_test), and the name of the column string ('column')\n",
        "\n",
        "#note this trains both training and test data simultaneously due to unique treatment if any category\n",
        "#missing from training set but not from test set to ensure consistent formatting \n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with distinct columns for year, month, day, hour, minute, second\n",
        "#each normalized to the mean and std, with missing values plugged with the mean\n",
        "#with columns named after column_ + time category\n",
        "#returns two transformed dataframe (mdf_train, mdf_test)\n",
        "\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "def process_time_class(mdf_train, mdf_test, column):\n",
        "  \n",
        "  #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data\n",
        "  mdf_train[column] = pd.to_datetime(mdf_train[column], errors = 'coerce')\n",
        "  mdf_test[column] = pd.to_datetime(mdf_test[column], errors = 'coerce')\n",
        "  \n",
        "  #mdf_train[column].replace(-np.Inf, np.nan)\n",
        "  #mdf_test[column].replace(-np.Inf, np.nan)\n",
        "  \n",
        "  #get mean of various categories of datetime objects to use to plug in missing cells\n",
        "  meanyear = mdf_train[column].dt.year.mean()    \n",
        "  meanmonth = mdf_train[column].dt.month.mean()\n",
        "  meanday = mdf_train[column].dt.day.mean()\n",
        "  meanhour = mdf_train[column].dt.hour.mean()\n",
        "  meanminute = mdf_train[column].dt.minute.mean()\n",
        "  meansecond = mdf_train[column].dt.second.mean()\n",
        "  \n",
        "  #get standard deviation of training data\n",
        "  stdyear = mdf_train[column].dt.year.std()  \n",
        "  stdmonth = mdf_train[column].dt.month.std()\n",
        "  stdday = mdf_train[column].dt.day.std()\n",
        "  stdhour = mdf_train[column].dt.hour.std()\n",
        "  stdminute = mdf_train[column].dt.minute.std()\n",
        "  stdsecond = mdf_train[column].dt.second.std()\n",
        "  \n",
        "  \n",
        "  #create new columns for each category in train set\n",
        "  mdf_train[column + '_year'] = mdf_train[column].dt.year\n",
        "  mdf_train[column + '_month'] = mdf_train[column].dt.month\n",
        "  mdf_train[column + '_day'] = mdf_train[column].dt.day\n",
        "  mdf_train[column + '_hour'] = mdf_train[column].dt.hour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column].dt.minute\n",
        "  mdf_train[column + '_second'] = mdf_train[column].dt.second\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column].dt.year\n",
        "  mdf_test[column + '_month'] = mdf_test[column].dt.month\n",
        "  mdf_test[column + '_day'] = mdf_test[column].dt.day\n",
        "  mdf_test[column + '_hour'] = mdf_test[column].dt.hour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column].dt.minute \n",
        "  mdf_test[column + '_second'] = mdf_test[column].dt.second\n",
        "  \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(meanyear)\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(meanmonth)\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(meanday)\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(meanhour)\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(meanminute)\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(meanyear)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(meanmonth)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(meanday)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(meanhour)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(meanminute)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #subtract mean from column for both train and test\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'] - meanyear\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'] - meanmonth\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'] - meanday\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'] - meanhour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'] - meanminute\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'] - meansecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] - meanyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] - meanmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] - meanday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] - meanhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] - meanminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] - meansecond\n",
        "  \n",
        "  \n",
        "  #divide column values by std for both training and test data\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'] / stdyear\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'] / stdmonth\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'] / stdday\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'] / stdhour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'] / stdminute\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'] / stdsecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] / stdyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] / stdmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] / stdday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] / stdhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] / stdminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] / stdsecond\n",
        "  \n",
        "  \n",
        "  #now replace NaN with 0\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(0)\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(0)\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(0)\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(0)\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(0)\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(0)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(0)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(0)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(0)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(0)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(0)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(0)\n",
        "  \n",
        "  \n",
        "  \n",
        "  #this is to address an issue I found when parsing columns with only time no date\n",
        "  #which returned -inf vlaues\n",
        "  checkyear = np.isinf(mdf_train[column + '_year'][0])\n",
        "  if checkyear:\n",
        "    del mdf_train[column + '_year']\n",
        "    if column + '_year' in mdf_test.columns:\n",
        "      del mdf_test[column + '_year']\n",
        "\n",
        "  checkmonth = np.isinf(mdf_train[column + '_month'][0])\n",
        "  if checkmonth:\n",
        "    del mdf_train[column + '_month']\n",
        "    if column + '_month' in mdf_test.columns:\n",
        "      del mdf_test[column + '_month']\n",
        "\n",
        "  checkday = np.isinf(mdf_train[column + '_day'][0])\n",
        "  if checkmonth:\n",
        "    del mdf_train[column + '_day']\n",
        "    if column + '_day' in mdf_test.columns:\n",
        "      del mdf_test[column + '_day']\n",
        "  \n",
        "  \n",
        "  #delete original column from training data\n",
        "  del mdf_train[column]    \n",
        "  if column in mdf_test.columns:\n",
        "    del mdf_test[column]  \n",
        "\n",
        "  \n",
        "  return mdf_train, mdf_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3xPDcW2b1S9S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3) Define evalcategory(.) and automunge(.) functions"
      ]
    },
    {
      "metadata": {
        "id": "xdD_XVrLGtEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#evalcategory(df, column)\n",
        "#Function that dakes as input a dataframe and associated column id \\\n",
        "#evaluates the contents of cells and classifies the column into one of four categories\n",
        "#category 1, 'binary', is for columns with only two categorys of text or integer\n",
        "#category 2, 'number', is for columns with numerical integer or float values\n",
        "#category 3, 'text', is for columns with multiple categories appropriate for one-hot\n",
        "#category 4, 'date', is for columns with Timestamp data\n",
        "#returns category id as a string\n",
        "\n",
        "import collections\n",
        "import datetime as dt\n",
        "\n",
        "def evalcategory(df, column):\n",
        "  \n",
        "  \n",
        "  #I couldn't find a good pandas tool for evaluating data class, \\\n",
        "  #So will iterate an array through each row of the dataframe column and \\\n",
        "  #evaluation for most common variable using the collections library \\\n",
        "  #this probably isn't extremely efficient for big data scale\n",
        "  array = []\n",
        "  for index, row in df.iterrows():\n",
        "    array = np.append(array, type(row[column]))\n",
        "      \n",
        "  c = collections.Counter(array)\n",
        "  mc = c.most_common(1)\n",
        "  \n",
        "  #additional array needed to check for time series\n",
        "  datearray = []\n",
        "  for index, row in df.iterrows():\n",
        "    datearray = np.append(datearray,type(pd.to_datetime(row[column], errors = 'coerce')))\n",
        "  \n",
        "  datec = collections.Counter(datearray)\n",
        "  datemc = datec.most_common(1)\n",
        "  \n",
        "    \n",
        "  #This is kind of hack to evaluate class by comparing these with output of mc\n",
        "  checkint = 1\n",
        "  checkfloat = 1.1\n",
        "  checkstring = 'string'\n",
        "  checkNAN = float('NaN')\n",
        "\n",
        "  #there's probably easier way to do this, here will create a check for date\n",
        "  df_checkdate = pd.DataFrame([{'checkdate' : '7/4/2018'}])\n",
        "  df_checkdate['checkdate'] = pd.to_datetime(df_checkdate['checkdate'], errors = 'coerce')\n",
        "  \n",
        "\n",
        "  #create dummy variable to store determined class (default is text class)\n",
        "  category = 'text'\n",
        "  \n",
        "  #if most common in column is string and > two values, set category to text\n",
        "  if isinstance(checkstring, mc[0][0]) and df[column].nunique() > 2:\n",
        "    category = 'text'\n",
        "  \n",
        "  #if most common is date, set category to date\n",
        "  if isinstance(df_checkdate['checkdate'][0], datemc[0][0]):\n",
        "    category = 'date'\n",
        "  \n",
        "  #if most common in column is integer and > two values, set category to number\n",
        "  if isinstance(checkint, mc[0][0]) and df[column].nunique() > 2:\n",
        "    category = 'number'\n",
        "    \n",
        "  #if most common in column is float, set category to number\n",
        "  if isinstance(checkfloat, mc[0][0]):\n",
        "    category = 'number'\n",
        "    \n",
        "  #if most common in column is NaN, set category to number\n",
        "  if isinstance(checkNAN, mc[0][0]):\n",
        "    category = 'number'\n",
        "  \n",
        "  #if most common in column is integer and only two values, set category to binary\n",
        "  if isinstance(checkint, mc[0][0]) and df[column].nunique() == 2:\n",
        "    category = 'binary'\n",
        "  \n",
        "  #if most common in column is string and only two values, set category to binary\n",
        "  if isinstance(checkstring, mc[0][0]) and df[column].nunique() == 2:\n",
        "    category = 'binary'\n",
        "  \n",
        "  return category\n",
        "\n",
        "#automunge(df_train, df_test, labels_column, valpercent=0.20)\n",
        "#Function that when fed a train and test data set automates the process \\\n",
        "#of evaluating each column for determination and applicaiton of appropriate preprocessing.\n",
        "#Takes as arguement pandas dataframes of training and test data (mdf_train), (mdf_test)\\\n",
        "#the name of the column from train set containing labels, \\\n",
        "#a value identifying th labels column from train dataset, \\\n",
        "#and a value for percent of training data to be applied to a validation set.\n",
        "#Based on an evaluation of columns selectively applies one of four preprocessing functions to each.\n",
        "#Shuffles the data and splits the training set into train and validation sets.\n",
        "#Returns following sets as numpy arrays: train, labels, validation, validationlabels, test\n",
        "\n",
        "#Note that this approach assumes that the test data is available at time of training\n",
        "#A different approach may be required if processing of test data is not simultaneous\n",
        "#although one potential solution is to apply this function intiially with a dummy\\\n",
        "#dataframe for test set and then when test data becomes available reapply \\\n",
        "#with original train set used for training the model along with the test set\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def automunge(df_train, df_test, labels_column, valpercent=0.20):\n",
        "  \n",
        "  \n",
        "  #extract labels from train set\n",
        "  df_labels = pd.DataFrame(df_train[labels_column])\n",
        "  del df_train[labels_column]\n",
        "  \n",
        "  \n",
        "  #confirm consistency of train an test sets\n",
        "  \n",
        "  #check number of columns is consistent\n",
        "  if df_train.shape[1] != df_test.shape[1]:\n",
        "    print(\"error, different number of columns in train and test sets\")\n",
        "    return\n",
        "  \n",
        "  #check column headers are consistent (this works independent of order)\n",
        "  columns_train = set(list(df_train))\n",
        "  columns_test = set(list(df_test))\n",
        "  if columns_train != columns_test:\n",
        "    print(\"error, different column labels in the train and test set\")\n",
        "    return\n",
        "   \n",
        "  #sort columns alphabetically to ensure same order\n",
        "  #to be honest I'm not positive that this piece is working correctly\n",
        "  df_train = df_train.sort_index(axis=0)\n",
        "  df_test = df_test.sort_index(axis=0)\n",
        "  \n",
        "  \n",
        "  #extract column lists again but this time as a list\n",
        "  columns_train = list(df_train)\n",
        "  columns_test = list(df_test)\n",
        "  \n",
        "  \n",
        "  #For each column, determine appropriate processing function\n",
        "  #processing function will be based on evaluation of train set\n",
        "  for column in columns_train:\n",
        "\n",
        "    category = evalcategory(df_train, column)\n",
        "\n",
        "    #for binary class use the majority field for missing plug value\n",
        "    if category == 'binary':\n",
        "      binary_missing_plug = df_train['Y/N'].value_counts().index.tolist()[0]\n",
        "    \n",
        "    \n",
        "    #apply appropriate processing function to this column based on the result\n",
        "    if category == 'binary':\n",
        "      df_train = process_binary_class(df_train, column, binary_missing_plug)\n",
        "      df_test = process_binary_class(df_test, column, binary_missing_plug)\n",
        "      \n",
        "    if category == 'number':\n",
        "      df_train, df_test = process_numerical_class(df_train, df_test, column)\n",
        "      \n",
        "    if category == 'text':\n",
        "      df_train, df_test = process_text_class(df_train, df_test, column)\n",
        "    \n",
        "    if category == 'date':\n",
        "      df_train, df_test = process_time_class(df_train, df_test, column)\n",
        "      \n",
        "      \n",
        "  \n",
        "  #determine labels category and apply appropriate function\n",
        "  labelscategory = evalcategory(df_labels, labels_column)\n",
        "  \n",
        "  #empty dummy labels \"test\" df for our preprocessing functions\n",
        "  labelsdummy = pd.DataFrame()\n",
        "  \n",
        "  #apply appropriate processing function to this column based on the result\n",
        "  if labelscategory == 'binary':\n",
        "    df_labels = process_binary_class(df_labels, labels_column, binary_missing_plug)\n",
        "      \n",
        "  if labelscategory == 'number':\n",
        "    df_labels, labelsdummy = process_numerical_class(df_labels, labelsdummy, labels_column)\n",
        " \n",
        "  #it occurs to me there might be an argument for preferring a single numerical \\\n",
        "  #classifier for labels to keep this to a single column, if so scikitlearn's \\\n",
        "  #LabelEcncoder could be used here, will assume that onehot encoding is acceptable\n",
        "  if labelscategory == 'text':\n",
        "    df_labels, labelsdummy = process_text_class(df_labels, labelsdummy, column)\n",
        "  \n",
        "  \n",
        "  #great the data is processed now let's do a few moore global training preps\n",
        "  \n",
        "  #convert all of our dataframes to numpy arrays (train, test, and labels)\n",
        "  np_train = df_train.values\n",
        "  np_test = df_test.values\n",
        "  np_labels = df_labels.values\n",
        "  \n",
        "  \n",
        "  #set randomness seed number\n",
        "  answer = 42\n",
        "  \n",
        "  #shuffle training set and labels\n",
        "  np_train = shuffle(np_train, random_state = answer)\n",
        "  np_test = shuffle(np_test, random_state = answer)\n",
        "  np_labels = shuffle(np_labels, random_state = answer)\n",
        "  \n",
        "  \n",
        "  #split validation sets from training and labels\n",
        "  train, validation, labels, validationlabels = \\\n",
        "  train_test_split(np_train, np_labels, test_size=valpercent, random_state=answer)\n",
        "  test = np_test\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  return train, labels, validation, validationlabels, test\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lmSPH6Deckgq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3) Test FunctionsÂ¶"
      ]
    },
    {
      "metadata": {
        "id": "TPnGh7REca1N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create sample test and train data for demonstration purposes\n",
        "\n",
        "#train data set from list of dictionaries\n",
        "train = [{'number': 1, 'Y/N': 'Y', 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'number': 2, 'Y/N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'number': None, 'Y/N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'}, \n",
        "         {'number': 3.1, 'Y/N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'number': -1, 'Y/N': 'N', 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'number': 'Q', 'Y/N': 'N', 'shape': 'oval', 'date' : '2015', 'label': 'dog'}]\n",
        "\n",
        "#convert train data to pandas dataframe\n",
        "df_train = pd.DataFrame(train)\n",
        "\n",
        "#test data set from list of dictionaries\n",
        "test = [{'number': 2.1, 'Y/N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'number': -1, 'Y/N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'number': 1, 'Y/N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'number': None, 'Y/N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'number': 3, 'Y/N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'number': 0, 'Y/N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'number': 'Q', 'Y/N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'}]\n",
        "\n",
        "#convert test data to pandas dataframe\n",
        "df_test = pd.DataFrame(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sJqAh2tDHLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "35698865-66ca-4663-8ce2-7a2673d6c235"
      },
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y/N</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "      <th>number</th>\n",
              "      <th>shape</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Y</td>\n",
              "      <td>2/12/18</td>\n",
              "      <td>cat</td>\n",
              "      <td>1</td>\n",
              "      <td>circle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>N</td>\n",
              "      <td>August 12, 2016</td>\n",
              "      <td>dog</td>\n",
              "      <td>2</td>\n",
              "      <td>square</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Y</td>\n",
              "      <td>None</td>\n",
              "      <td>cat</td>\n",
              "      <td>None</td>\n",
              "      <td>circle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "      <td>July 4, 2016</td>\n",
              "      <td>cat</td>\n",
              "      <td>3.1</td>\n",
              "      <td>square</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>N</td>\n",
              "      <td>Jul 4, 2018</td>\n",
              "      <td>dog</td>\n",
              "      <td>-1</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>N</td>\n",
              "      <td>2015</td>\n",
              "      <td>dog</td>\n",
              "      <td>Q</td>\n",
              "      <td>oval</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Y/N             date label number   shape\n",
              "0     Y          2/12/18   cat      1  circle\n",
              "1     N  August 12, 2016   dog      2  square\n",
              "2     Y             None   cat   None  circle\n",
              "3  None     July 4, 2016   cat    3.1  square\n",
              "4     N      Jul 4, 2018   dog     -1    None\n",
              "5     N             2015   dog      Q    oval"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "VoSSKDeIczHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#apply automunge\n",
        "\n",
        "train, labels, validation, validationlabels, test = \\\n",
        "automunge(df_train, df_test, labels_column = 'label')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I7nanooF2Nna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1b7a3670-2585-4687-f547-d71c23ef3594"
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         1.35222288, -0.4472136 ,  0.6172134 , -0.5118745 ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        , -1.19256959, -1.2344268 , -1.10249892,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        -1.6856477 ,  1.04349839,  0.6172134 , -0.5118745 ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "0IaAeKlhdREW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ab53373b-7d0d-4bfe-b0aa-7389fc781057"
      },
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "Bp-qgpWRlFq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "23513f17-785e-4403-a44d-6114c5ea2fc2"
      },
      "cell_type": "code",
      "source": [
        "validation"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "        -0.20375961,  1.04349839, -0.9258201 ,  1.06312396,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         0.53718443, -0.4472136 ,  0.9258201 ,  1.06312396,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "CfRGG7p0yglT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e356f365-ab6b-43c2-82ef-19c5eb6c65e2"
      },
      "cell_type": "code",
      "source": [
        "validationlabels"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "0W8GuZuFying",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1f5d5f0e-cd64-4be4-ca4c-94e85f29538d"
      },
      "cell_type": "code",
      "source": [
        "test"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         0.61127884,  1.04349839, -0.3086067 ,  1.45687358,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        -1.6856477 , -0.4472136 ,  0.9258201 ,  1.06312396,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        -0.94470366,  0.2981424 , -1.2344268 , -1.10249892,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "        -0.20375961,  1.04349839,  0.6172134 , -0.5118745 ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
              "         1.27812848,  1.04349839,  0.9258201 ,  4.80374531,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
              "         0.        ,  1.78885438, -1.2344268 , -1.10249892,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "wthwPnx5ylaW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
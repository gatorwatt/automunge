{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automunge&Postmunge_Speedup_1.3_9-27-18_Final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hQJGsdymZDsD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#for measuring time duration of operations\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "#for file downloads\n",
        "import pickle\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmjR5Ze0U-JC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1) insert preprocess functions from prior notebook. "
      ]
    },
    {
      "metadata": {
        "id": "psx9E_ckUsPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#process_numerical_class(mdf_train, mdf_test, column)\n",
        "#function to normalize data to mean of 0 and standard deviation of 1 from training distribution\n",
        "#takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\\\n",
        "#and the name of the column string ('column') \n",
        "#replaces missing or improperly formatted data with mean of remaining values\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#expect this approach works better when the numerical distribution is thin tailed\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "#imports\n",
        "from pandas import Series\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def process_numerical_class(mdf_train, mdf_test, column):\n",
        "     \n",
        "    \n",
        "  #convert all values to either numeric or NaN\n",
        "  mdf_train[column] = pd.to_numeric(mdf_train[column], errors='coerce')\n",
        "  mdf_test[column] = pd.to_numeric(mdf_test[column], errors='coerce')\n",
        "\n",
        "  #get mean of training data\n",
        "  mean = mdf_train[column].mean()    \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_train[column] = mdf_train[column].fillna(mean)\n",
        "  mdf_test[column] = mdf_test[column].fillna(mean)\n",
        "\n",
        "  #subtract mean from column for both train and test\n",
        "  mdf_train[column] = mdf_train[column] - mean\n",
        "  mdf_test[column] = mdf_test[column] - mean\n",
        "\n",
        "  #get standard deviation of training data\n",
        "  std = mdf_train[column].std()\n",
        "\n",
        "  #divide column values by std for both training and test data\n",
        "  mdf_train[column] = mdf_train[column] / std\n",
        "  mdf_test[column] = mdf_test[column] / std\n",
        "\n",
        "\n",
        "  return mdf_train, mdf_test, mean, std\n",
        "  \n",
        "\n",
        "  \n",
        "#process_binary_class(mdf, column, missing)\n",
        "#converts binary classification values to 0 or 1\n",
        "#takes as arguement a pandas dataframe (mdf), \\\n",
        "#the name of the column string ('column') \\\n",
        "#and the string classification to assign to missing data ('missing')\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#missing category must be identical to one of the two existing categories\n",
        "#returns error message if more than two categories remain\n",
        "\n",
        "\n",
        "def process_binary_class(mdf, column, missing):\n",
        "    \n",
        "  #replace missing data with specified classification\n",
        "  mdf[column] = mdf[column].fillna(missing)\n",
        "\n",
        "  #if more than two remaining classifications, return error message    \n",
        "  if len(mdf[column].unique()) > 2:\n",
        "      print('ERROR: number of categories in column for process_binary_class() call >2')\n",
        "      return mdf\n",
        "\n",
        "  #convert column to binary 0/1 classification\n",
        "  lb = preprocessing.LabelBinarizer()\n",
        "  mdf[column] = lb.fit_transform(mdf[column])\n",
        "\n",
        "  return mdf\n",
        "\n",
        "  \n",
        "#process_text_class(mdf_train, mdf_test, column)\n",
        "#preprocess column with text classifications\n",
        "#takes as arguement two pandas dataframe containing training and test data respectively \n",
        "#(mdf_train, mdf_test), and the name of the column string ('column')\n",
        "\n",
        "#note this trains both training and test data simultaneously due to unique treatment if any category\n",
        "#missing from training set but not from test set to ensure consistent formatting \n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with onehot encodings\n",
        "#with columns named after column_ + text classifications\n",
        "#missing data replaced with category label 'missing'+column\n",
        "#any categories missing from the training set removed from test set\n",
        "#any category present in training but missing from test set given a column of zeros for consistent formatting\n",
        "#ensures order of all new columns consistent between both sets\n",
        "#returns two transformed dataframe (mdf_train, mdf_test) \\\n",
        "#and a list of the new column names (textcolumns)\n",
        "\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "#note it is kind of a hack here to create a column for missing values with \\\n",
        "#two underscores (__) in the column name to ensure appropriate order for cases\\\n",
        "#where NaN present in test data but not train data, if a category starts with|\n",
        "#an underscore such that it preceeds '__missing' alphabetically in this scenario\\\n",
        "#this might create error due to different order of columns, address of this \\\n",
        "#potential issue will be a future extension\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def process_text_class(mdf_train, mdf_test, column):\n",
        "\n",
        "  #replace NA with a dummy variable\n",
        "  mdf_train[column] = mdf_train[column].fillna('_missing')\n",
        "  mdf_test[column] = mdf_test[column].fillna('_missing')\n",
        "\n",
        "\n",
        "  #extract categories for column labels\n",
        "  #note that .unique() extracts the labels as a numpy array\n",
        "  labels_train = mdf_train[column].unique()\n",
        "  labels_train.sort(axis=0)\n",
        "  labels_test = mdf_test[column].unique()\n",
        "  labels_test.sort(axis=0)\n",
        "\n",
        "  #transform text classifications to numerical id\n",
        "  encoder = LabelEncoder()\n",
        "  cat_train = mdf_train[column]\n",
        "  cat_train_encoded = encoder.fit_transform(cat_train)\n",
        "\n",
        "  cat_test = mdf_test[column]\n",
        "  cat_test_encoded = encoder.fit_transform(cat_test)\n",
        "\n",
        "\n",
        "  #apply onehotencoding\n",
        "  onehotencoder = OneHotEncoder()\n",
        "  cat_train_1hot = onehotencoder.fit_transform(cat_train_encoded.reshape(-1,1))\n",
        "  cat_test_1hot = onehotencoder.fit_transform(cat_test_encoded.reshape(-1,1))\n",
        "\n",
        "  #append column header name to each category listing\n",
        "  #note the iteration is over a numpy array hence the [...] approach  \n",
        "  labels_train[...] = column + '_' + labels_train[...]\n",
        "  labels_test[...] = column + '_' + labels_test[...]\n",
        "\n",
        "\n",
        "  #convert sparse array to pandas dataframe with column labels\n",
        "  df_train_cat = pd.DataFrame(cat_train_1hot.toarray(), columns=labels_train)\n",
        "  df_test_cat = pd.DataFrame(cat_test_1hot.toarray(), columns=labels_test)\n",
        "  \n",
        "  #add a missing column to train if it's not present\n",
        "  if column + '__missing' not in df_train_cat.columns:\n",
        "    missingcolumn = pd.DataFrame(0, index=np.arange(df_train_cat.shape[0]), columns=[column+'__missing'])\n",
        "    df_train_cat = pd.concat([missingcolumn, df_train_cat], axis=1)\n",
        "\n",
        "\n",
        "  #Get missing columns in test set that are present in training set\n",
        "  missing_cols = set( df_train_cat.columns ) - set( df_test_cat.columns )\n",
        "  \n",
        "  #Add a missing column in test set with default value equal to 0\n",
        "  for c in missing_cols:\n",
        "      df_test_cat[c] = 0\n",
        "  #Ensure the order of column in the test set is in the same order than in train set\n",
        "  #Note this also removes categories in test set that aren't present in training set\n",
        "  df_test_cat = df_test_cat[df_train_cat.columns]\n",
        "\n",
        "\n",
        "  #concatinate the sparse set with the rest of our training data\n",
        "  mdf_train = pd.concat([df_train_cat, mdf_train], axis=1)\n",
        "  mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)\n",
        "\n",
        "\n",
        "  #delete original column from training data\n",
        "  del mdf_train[column]    \n",
        "  del mdf_test[column]\n",
        "  \n",
        "  #create output of a list of the created column names\n",
        "  labels_train = list(df_train_cat)\n",
        "  textcolumns = labels_train\n",
        "  \n",
        "  \n",
        "\n",
        "  return mdf_train, mdf_test, textcolumns\n",
        "\n",
        "\n",
        "\n",
        "#process_time_class(mdf_train, mdf_test, column)\n",
        "#preprocess column with time classifications\n",
        "#takes as arguement two pandas dataframe containing training and test data respectively \n",
        "#(mdf_train, mdf_test), and the name of the column string ('column')\n",
        "\n",
        "#note this trains both training and test data simultaneously due to unique treatment if any category\n",
        "#missing from training set but not from test set to ensure consistent formatting \n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with distinct columns for year, month, day, hour, minute, second\n",
        "#each normalized to the mean and std, with missing values plugged with the mean\n",
        "#with columns named after column_ + time category\n",
        "#returns two transformed dataframe (mdf_train, mdf_test)\n",
        "\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "def process_time_class(mdf_train, mdf_test, column):\n",
        "  \n",
        "  #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data\n",
        "  mdf_train[column] = pd.to_datetime(mdf_train[column], errors = 'coerce')\n",
        "  mdf_test[column] = pd.to_datetime(mdf_test[column], errors = 'coerce')\n",
        "  \n",
        "  #mdf_train[column].replace(-np.Inf, np.nan)\n",
        "  #mdf_test[column].replace(-np.Inf, np.nan)\n",
        "  \n",
        "  #get mean of various categories of datetime objects to use to plug in missing cells\n",
        "  meanyear = mdf_train[column].dt.year.mean()    \n",
        "  meanmonth = mdf_train[column].dt.month.mean()\n",
        "  meanday = mdf_train[column].dt.day.mean()\n",
        "  meanhour = mdf_train[column].dt.hour.mean()\n",
        "  meanminute = mdf_train[column].dt.minute.mean()\n",
        "  meansecond = mdf_train[column].dt.second.mean()\n",
        "  \n",
        "  #get standard deviation of training data\n",
        "  stdyear = mdf_train[column].dt.year.std()  \n",
        "  stdmonth = mdf_train[column].dt.month.std()\n",
        "  stdday = mdf_train[column].dt.day.std()\n",
        "  stdhour = mdf_train[column].dt.hour.std()\n",
        "  stdminute = mdf_train[column].dt.minute.std()\n",
        "  stdsecond = mdf_train[column].dt.second.std()\n",
        "  \n",
        "  \n",
        "  #create new columns for each category in train set\n",
        "  mdf_train[column + '_year'] = mdf_train[column].dt.year\n",
        "  mdf_train[column + '_month'] = mdf_train[column].dt.month\n",
        "  mdf_train[column + '_day'] = mdf_train[column].dt.day\n",
        "  mdf_train[column + '_hour'] = mdf_train[column].dt.hour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column].dt.minute\n",
        "  mdf_train[column + '_second'] = mdf_train[column].dt.second\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column].dt.year\n",
        "  mdf_test[column + '_month'] = mdf_test[column].dt.month\n",
        "  mdf_test[column + '_day'] = mdf_test[column].dt.day\n",
        "  mdf_test[column + '_hour'] = mdf_test[column].dt.hour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column].dt.minute \n",
        "  mdf_test[column + '_second'] = mdf_test[column].dt.second\n",
        "  \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(meanyear)\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(meanmonth)\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(meanday)\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(meanhour)\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(meanminute)\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(meanyear)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(meanmonth)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(meanday)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(meanhour)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(meanminute)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #subtract mean from column for both train and test\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'] - meanyear\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'] - meanmonth\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'] - meanday\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'] - meanhour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'] - meanminute\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'] - meansecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] - meanyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] - meanmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] - meanday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] - meanhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] - meanminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] - meansecond\n",
        "  \n",
        "  \n",
        "  #divide column values by std for both training and test data\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'] / stdyear\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'] / stdmonth\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'] / stdday\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'] / stdhour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'] / stdminute\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'] / stdsecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] / stdyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] / stdmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] / stdday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] / stdhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] / stdminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] / stdsecond\n",
        "  \n",
        "  \n",
        "  #now replace NaN with 0\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(0)\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(0)\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(0)\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(0)\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(0)\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(0)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(0)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(0)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(0)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(0)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(0)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(0)\n",
        "  \n",
        "  #output of a list of the created column names\n",
        "  datecolumns = [column + '_year', column + '_month', column + '_day', \\\n",
        "                column + '_hour', column + '_minute', column + '_second']\n",
        "  \n",
        "  #this is to address an issue I found when parsing columns with only time no date\n",
        "  #which returned -inf vlaues, so if an issue will just delete the associated \n",
        "  #column along with the entry in datecolumns\n",
        "  checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])\n",
        "  if checkyear:\n",
        "    del mdf_train[column + '_year']\n",
        "    datecolumns.remove(column + '_year')\n",
        "    if column + '_year' in mdf_test.columns:\n",
        "      del mdf_test[column + '_year']\n",
        "\n",
        "  checkmonth = np.isinf(mdf_train.iloc[0][column + '_month'])\n",
        "  if checkmonth:\n",
        "    del mdf_train[column + '_month']\n",
        "    datecolumns.remove(column + '_month')\n",
        "    if column + '_month' in mdf_test.columns:\n",
        "      del mdf_test[column + '_month']\n",
        "\n",
        "  checkday = np.isinf(mdf_train.iloc[0][column + '_day'])\n",
        "  if checkday:\n",
        "    del mdf_train[column + '_day']\n",
        "    datecolumns.remove(column + '_day')\n",
        "    if column + '_day' in mdf_test.columns:\n",
        "      del mdf_test[column + '_day']\n",
        "  \n",
        "  \n",
        "  #delete original column from training data\n",
        "  del mdf_train[column]    \n",
        "  if column in mdf_test.columns:\n",
        "    del mdf_test[column]  \n",
        "  \n",
        "\n",
        "  \n",
        "  #output a dictionary of the associated column mean and std\n",
        "  \n",
        "  timenormalization_dict = {'meanyear' : meanyear, 'meanmonth' : meanmonth, \\\n",
        "                            'meanday' : meanday, 'meanhour' : meanhour, \\\n",
        "                            'meanminute' : meanminute, 'meansecond' : meansecond,\\\n",
        "                            'stdyear' : stdyear, 'stdmonth' : stdmonth, \\\n",
        "                            'stdday' : stdday, 'stdhour' : stdhour, \\\n",
        "                            'stdminute' : stdminute, 'stdsecond' : stdsecond}\n",
        "  \n",
        "  \n",
        "  return mdf_train, mdf_test, datecolumns, timenormalization_dict\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Nd6_y_jVcO3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2) Update evalcategory function for speed"
      ]
    },
    {
      "metadata": {
        "id": "Sks-OLMoGQEi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#evalcategory(df, column)\n",
        "#Function that dakes as input a dataframe and associated column id \\\n",
        "#evaluates the contents of cells and classifies the column into one of four categories\n",
        "#category 1, 'bnry', is for columns with only two categorys of text or integer\n",
        "#category 2, 'nmbr', is for columns with numerical integer or float values\n",
        "#category 3, 'text', is for columns with multiple categories appropriate for one-hot\n",
        "#category 4, 'date', is for columns with Timestamp data\n",
        "#category 5, 'null', is for columns with >85% null values (arbitrary figure)\n",
        "#returns category id as a string\n",
        "\n",
        "import collections\n",
        "import datetime as dt\n",
        "\n",
        "def evalcategory(df, column):\n",
        "  \n",
        "  \n",
        "  #I couldn't find a good pandas tool for evaluating data class, \\\n",
        "  #So will produce an array containing data types of each cell and \\\n",
        "  #evaluate for most common variable using the collections library\n",
        "  \n",
        "  type1_df = df[column].apply(lambda x: type(x)).values\n",
        "  \n",
        "  c = collections.Counter(type1_df)\n",
        "  mc = c.most_common(1)\n",
        "  mc2 = c.most_common(2)\n",
        "  \n",
        "  #free memory (dtypes are memory hogs)\n",
        "  type1_df = None\n",
        "\n",
        "  \n",
        "  #additional array needed to check for time series\n",
        "  \n",
        "  #df['typecolumn2'] = df[column].apply(lambda x: type(pd.to_datetime(x, errors = 'coerce')))\n",
        "  type2_df = df[column].apply(lambda x: type(pd.to_datetime(x, errors = 'coerce'))).values\n",
        "  \n",
        "  datec = collections.Counter(type2_df)\n",
        "  datemc = datec.most_common(1)\n",
        "  datemc2 = datec.most_common(2)\n",
        "  \n",
        "  #free memory (dtypes are memory hogs)\n",
        "  type2_df = None\n",
        "  \n",
        "  \n",
        "  #an extension of this approach could be for those columns that produce a text\\\n",
        "  #category to implement an additional text to determine the number of \\\n",
        "  #common groupings / or the amount of uniquity. For example if every row has\\\n",
        "  #a unique value then one-hot-encoding would not be appropriate. It would \\\n",
        "  #probably be apopropraite to either return an error message if this is found \\\n",
        "  #or alternatively find a furhter way to automate this processing such as \\\n",
        "  #look for contextual clues to groupings that can be inferred.\n",
        "    \n",
        "  #This is kind of hack to evaluate class by comparing these with output of mc\n",
        "  checkint = 1\n",
        "  checkfloat = 1.1\n",
        "  checkstring = 'string'\n",
        "  checkNAN = None\n",
        "\n",
        "  #there's probably easier way to do this, here will create a check for date\n",
        "  df_checkdate = pd.DataFrame([{'checkdate' : '7/4/2018'}])\n",
        "  df_checkdate['checkdate'] = pd.to_datetime(df_checkdate['checkdate'], errors = 'coerce')\n",
        "  \n",
        "\n",
        "  #create dummy variable to store determined class (default is text class)\n",
        "  category = 'text'\n",
        "\n",
        "\n",
        "  #if most common in column is string and > two values, set category to text\n",
        "  if isinstance(checkstring, mc[0][0]) and df[column].nunique() > 2:\n",
        "    category = 'text'\n",
        "  \n",
        "  #if most common is date, set category to date\n",
        "  if isinstance(df_checkdate['checkdate'][0], datemc[0][0]):\n",
        "    category = 'date'\n",
        "  \n",
        "  #if most common in column is integer and > two values, set category to number\n",
        "  if isinstance(checkint, mc[0][0]) and df[column].nunique() > 2:\n",
        "    category = 'nmbr'\n",
        "    \n",
        "  #if most common in column is float, set category to number\n",
        "  if isinstance(checkfloat, mc[0][0]):\n",
        "    category = 'nmbr'\n",
        "  \n",
        "  #if most common in column is integer and <= two values, set category to binary\n",
        "  if isinstance(checkint, mc[0][0]) and df[column].nunique() <= 2:\n",
        "    category = 'bnry'\n",
        "  \n",
        "  #if most common in column is string and <= two values, set category to binary\n",
        "  if isinstance(checkstring, mc[0][0]) and df[column].nunique() <= 2:\n",
        "    category = 'bnry'\n",
        "    \n",
        "      \n",
        "  #if > 80% (ARBITRARY FIGURE) are NaN we'll just delete the column\n",
        "  if df[column].isna().sum() >= df.shape[0] * 0.80:\n",
        "    category = 'null'\n",
        "  \n",
        "  #else if most common in column is NaN, re-evaluate using the second most common type\n",
        "  #(I suspect the below might have a bug somewhere but is working on my current \n",
        "  #tests so will leave be for now)\n",
        "  elif df[column].isna().sum() >= df.shape[0] / 2:\n",
        "    \n",
        "    #if 2nd most common in column is string and > two values, set category to text\n",
        "    if isinstance(checkstring, mc2[1][0]) and df[column].nunique() > 2:\n",
        "      category = 'text'\n",
        "  \n",
        "    #if 2nd most common is date, set category to date   \n",
        "    if isinstance(df_checkdate['checkdate'][0], datemc2[0][0]):\n",
        "      category = 'date'\n",
        "  \n",
        "    #if 2nd most common in column is integer and > two values, set category to number\n",
        "    if isinstance(checkint, mc2[1][0]) and df[column].nunique() > 2:\n",
        "      category = 'nmbr'\n",
        "    \n",
        "    #if 2nd most common in column is float, set category to number\n",
        "    if isinstance(checkfloat, mc2[1][0]):\n",
        "      category = 'nmbr'\n",
        "  \n",
        "    #if 2nd most common in column is integer and <= two values, set category to binary\n",
        "    if isinstance(checkint, mc2[1][0]) and df[column].nunique() <= 2:\n",
        "      category = 'bnry'\n",
        "  \n",
        "    #if 2nd most common in column is string and <= two values, set category to binary\n",
        "    if isinstance(checkstring, mc2[1][0]) and df[column].nunique() <= 2:\n",
        "      category = 'bnry'\n",
        "    \n",
        "     \n",
        "  \n",
        "  return category"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "05IuLqR-vU72",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #here is the original version of the function included here for validation \\\n",
        "# #of speedup\n",
        "\n",
        "# #evalcategory(df, column)\n",
        "# #Function that dakes as input a dataframe and associated column id \\\n",
        "# #evaluates the contents of cells and classifies the column into one of four categories\n",
        "# #category 1, 'bnry', is for columns with only two categorys of text or integer\n",
        "# #category 2, 'nmbr', is for columns with numerical integer or float values\n",
        "# #category 3, 'text', is for columns with multiple categories appropriate for one-hot\n",
        "# #category 4, 'date', is for columns with Timestamp data\n",
        "# #category 5, 'null', is for columns with >85% null values (arbitrary figure)\n",
        "# #returns category id as a string\n",
        "\n",
        "# import collections\n",
        "# import datetime as dt\n",
        "\n",
        "# def evalcategory(df, column):\n",
        "  \n",
        "  \n",
        "#   #I couldn't find a good pandas tool for evaluating data class, \\\n",
        "#   #So will iterate an array through each row of the dataframe column and \\\n",
        "#   #evaluation for most common variable using the collections library \\\n",
        "#   #this probably isn't extremely efficient for big data scale\n",
        "  \n",
        "#   #the if/else here is to address a bug I found when iterating through \\\n",
        "#   #in a dataframe with single column vs one with multiple columns\n",
        "  \n",
        "#   array = []\n",
        "  \n",
        "#   if df.shape[1] > 1:\n",
        "#     for index, row in df.iterrows():\n",
        "#       array = np.append(array, type(row[column]))\n",
        "      \n",
        "#   else:\n",
        "#     for row in df.iterrows():\n",
        "#       array = np.append(array, type(row[0]))\n",
        "\n",
        "#   c = collections.Counter(array)\n",
        "#   mc = c.most_common(1)\n",
        "#   mc2 = c.most_common(2)\n",
        "  \n",
        "#   #additional array needed to check for time series\n",
        "#   datearray = []\n",
        "  \n",
        "#   if df.shape[1] > 1:\n",
        "#     for index, row in df.iterrows():\n",
        "#       datearray = np.append(datearray,type(pd.to_datetime(row[column], errors = 'coerce')))\n",
        "  \n",
        "#   else:\n",
        "#     for row in df.iterrows():\n",
        "#       datearray = np.append(datearray,type(pd.to_datetime(row[0], errors = 'coerce')))\n",
        "  \n",
        "#   datec = collections.Counter(datearray)\n",
        "#   datemc = datec.most_common(1)\n",
        "#   datemc2 = datec.most_common(2)\n",
        "  \n",
        "#   #an extension of this approach could be for those columns that produce a text\\\n",
        "#   #category to implement an additional text to determine the number of \\\n",
        "#   #common groupings / or the amount of uniquity. For example if every row has\\\n",
        "#   #a unique value then one-hot-encoding would not be appropriate. It would \\\n",
        "#   #probably be apopropraite to either return an error message if this is found \\\n",
        "#   #or alternatively find a furhter way to automate this processing such as \\\n",
        "#   #look for contextual clues to groupings that can be inferred.\n",
        "    \n",
        "#   #This is kind of hack to evaluate class by comparing these with output of mc\n",
        "#   checkint = 1\n",
        "#   checkfloat = 1.1\n",
        "#   checkstring = 'string'\n",
        "#   checkNAN = None\n",
        "\n",
        "#   #there's probably easier way to do this, here will create a check for date\n",
        "#   df_checkdate = pd.DataFrame([{'checkdate' : '7/4/2018'}])\n",
        "#   df_checkdate['checkdate'] = pd.to_datetime(df_checkdate['checkdate'], errors = 'coerce')\n",
        "  \n",
        "\n",
        "#   #create dummy variable to store determined class (default is text class)\n",
        "#   category = 'text'\n",
        "\n",
        "\n",
        "#   #if most common in column is string and > two values, set category to text\n",
        "#   if isinstance(checkstring, mc[0][0]) and df[column].nunique() > 2:\n",
        "#     category = 'text'\n",
        "  \n",
        "#   #if most common is date, set category to date\n",
        "#   if isinstance(df_checkdate['checkdate'][0], datemc[0][0]):\n",
        "#     category = 'date'\n",
        "  \n",
        "#   #if most common in column is integer and > two values, set category to number\n",
        "#   if isinstance(checkint, mc[0][0]) and df[column].nunique() > 2:\n",
        "#     category = 'nmbr'\n",
        "    \n",
        "#   #if most common in column is float, set category to number\n",
        "#   if isinstance(checkfloat, mc[0][0]):\n",
        "#     category = 'nmbr'\n",
        "  \n",
        "#   #if most common in column is integer and <= two values, set category to binary\n",
        "#   if isinstance(checkint, mc[0][0]) and df[column].nunique() <= 2:\n",
        "#     category = 'bnry'\n",
        "  \n",
        "#   #if most common in column is string and <= two values, set category to binary\n",
        "#   if isinstance(checkstring, mc[0][0]) and df[column].nunique() <= 2:\n",
        "#     category = 'bnry'\n",
        "    \n",
        "      \n",
        "#   #if > 80% (ARBITRARY FIGURE) are NaN we'll just delete the column\n",
        "#   if df[column].isna().sum() >= df.shape[0] * 0.80:\n",
        "#     category = 'null'\n",
        "  \n",
        "#   #else if most common in column is NaN, re-evaluate using the second most common type\n",
        "#   #(I suspect the below might have a bug somewhere but is working on my current \n",
        "#   #tests so will leave be for now)\n",
        "#   elif df[column].isna().sum() >= df.shape[0] / 2:\n",
        "    \n",
        "#     #if 2nd most common in column is string and > two values, set category to text\n",
        "#     if isinstance(checkstring, mc2[1][0]) and df[column].nunique() > 2:\n",
        "#       category = 'text'\n",
        "  \n",
        "#     #if 2nd most common is date, set category to date   \n",
        "#     if isinstance(df_checkdate['checkdate'][0], datemc2[0][0]):\n",
        "#       category = 'date'\n",
        "  \n",
        "#     #if 2nd most common in column is integer and > two values, set category to number\n",
        "#     if isinstance(checkint, mc2[1][0]) and df[column].nunique() > 2:\n",
        "#       category = 'nmbr'\n",
        "    \n",
        "#     #if 2nd most common in column is float, set category to number\n",
        "#     if isinstance(checkfloat, mc2[1][0]):\n",
        "#       category = 'nmbr'\n",
        "  \n",
        "#     #if 2nd most common in column is integer and <= two values, set category to binary\n",
        "#     if isinstance(checkint, mc2[1][0]) and df[column].nunique() <= 2:\n",
        "#       category = 'bnry'\n",
        "  \n",
        "#     #if 2nd most common in column is string and <= two values, set category to binary\n",
        "#     if isinstance(checkstring, mc2[1][0]) and df[column].nunique() <= 2:\n",
        "#       category = 'bnry'\n",
        "    \n",
        "     \n",
        "  \n",
        "#   return category"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CSWHrHUvYrYo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3) insert MLinfill support functions from prior notebook"
      ]
    },
    {
      "metadata": {
        "id": "VCm4Vo6zVlv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#NArows(df, column), function that when fed a dataframe, \\\n",
        "#column id, and category label outputs a single column dataframe composed of \\\n",
        "#True and False with the same number of rows as the input and the True's \\\n",
        "#coresponding to those rows of the input that had missing or NaN data. This \\\n",
        "#output can later be used to identify which rows for a column to infill with ML\\\n",
        "# derived plug data\n",
        "\n",
        "\n",
        "def NArows(df, column, category):\n",
        "  \n",
        "\n",
        "  \n",
        "  if category == 'text':\n",
        "  \n",
        "    #returns dataframe of True and False, where True coresponds to the NaN's\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.isna(df[column])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "  \n",
        "  if category == 'bnry':\n",
        "    \n",
        "    #returns dataframe of True and False, where True coresponds to the NaN's\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.isna(df[column])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "    \n",
        "  if category == 'nmbr':\n",
        "  \n",
        "    #convert all values to either numeric or NaN\n",
        "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "    \n",
        "    #returns dataframe of True and False, where True coresponds to the NaN's\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.isna(df[column])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "    \n",
        "  if category == 'date':\n",
        "    \n",
        "    #returns dataframe column of all False\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.DataFrame(False, index=np.arange(df.shape[0]), columns=[column+'NA'])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "  \n",
        "\n",
        "  return NArows\n",
        "\n",
        "\n",
        "#createMLinfillsets(df_train, df_test, column, trainNArows, testNArows, \\\n",
        "#category, textcolumnslist = []) function that when fed dataframes of train and\\\n",
        "#test sets, column id, df of True/False corresponding to rows from original \\\n",
        "#sets with missing values, a string category of 'text', 'date', 'nmbr', or \\\n",
        "#'bnry', and a list of column id's for the text category if applicable. The \\\n",
        "#function returns a seris of dataframes which can be applied to training a \\\n",
        "#machine learning model to predict apppropriate infill values for those points \\\n",
        "#that had missing values from the original sets, indlucing returns of \\\n",
        "#df_train_filltrain, df_train_filllabel, df_train_fillfeatures, \\\n",
        "#and df_test_fillfeatures\n",
        "\n",
        "\n",
        "def createMLinfillsets(df_train, df_test, column, trainNArows, testNArows, \\\n",
        "                       category, textcolumnslist = []):\n",
        "\n",
        "  \n",
        "  \n",
        "  #create 3 new dataframes for each train column - the train and labels \\\n",
        "  #for rows not needing infill, and the features for rows needing infill \\\n",
        "  #also create a test features column \n",
        "\n",
        "  #reminder:\n",
        "    #for numerical there won't be a new column\n",
        "    #for binary there won't be a new column\n",
        "    #for text the new column has a defined name as column+'_missing'\n",
        "\n",
        "  #note that for text class the labels will be a little more complicated \\\n",
        "  #since will be multi-column\n",
        "\n",
        "  if category == 'nmbr' or category == 'bnry':\n",
        "\n",
        "    #first concatinate the NArows True/False designations to df_train & df_test\n",
        "    df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "    df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "    \n",
        "    #create copy of df_train to serve as training set for fill\n",
        "    df_train_filltrain = df_train.copy()\n",
        "    #now delete rows coresponding to True\n",
        "    df_train_filltrain = df_train_filltrain[df_train_filltrain[column+'_NArows'] == False]\n",
        "    \n",
        "    #now delete [column] and the NA labels (column+'NA') from this df\n",
        "    df_train_filltrain = df_train_filltrain.drop([column, column+'_NArows'], axis=1)\n",
        "    \n",
        "    #create a copy of df_train[column] for fill train labels\n",
        "    df_train_filllabel = pd.DataFrame(df_train[column].copy())\n",
        "    #concatinate with the NArows\n",
        "    df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "    #drop rows corresponding to True\n",
        "    df_train_filllabel = df_train_filllabel[df_train_filllabel[column+'_NArows'] == False]\n",
        "    \n",
        "    #delete the NArows column\n",
        "    df_train_filllabel = df_train_filllabel.drop([column+'_NArows'], axis=1)\n",
        "\n",
        "    #create features df_train for rows needing infill\n",
        "    #create copy of df_train (note it already has NArows included)\n",
        "    df_train_fillfeatures = df_train.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[column+'_NArows'])]\n",
        "    #delete column and column+'_NArows'\n",
        "    df_train_fillfeatures = df_train_fillfeatures.drop([column, column+'_NArows'], axis=1)\n",
        "\n",
        "    #create features df_test for rows needing infill\n",
        "    #create copy of df_test (note it already has NArows included)\n",
        "    df_test_fillfeatures = df_test.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[column+'_NArows'])]\n",
        "    #delete column and column+'_NArows'\n",
        "    df_test_fillfeatures = df_test_fillfeatures.drop([column, column+'_NArows'], axis=1)\n",
        "    \n",
        "\n",
        "    #delete NArows from df_train, df_test\n",
        "    df_train = df_train.drop([column+'_NArows'], axis=1)\n",
        "    df_test = df_test.drop([column+'_NArows'], axis=1)\n",
        "\n",
        "\n",
        "  if category == 'text':\n",
        "\n",
        "    #first concatinate the NArows True/False designations to df_train & df_test\n",
        "    df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "    df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "\n",
        "    #create copy of df_train to serve as training set for fill\n",
        "    df_train_filltrain = df_train.copy()\n",
        "    #now delete rows coresponding to True\n",
        "    df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns.get_values()[0]] == False]\n",
        "    \n",
        "    #now delete columns = textcolumnslist and the NA labels (orig column+'_NArows') from this df\n",
        "    df_train_filltrain = df_train_filltrain.drop(textcolumnslist, axis=1)\n",
        "    df_train_filltrain = df_train_filltrain.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "    #create a copy of df_train[textcolumnslist] for fill train labels\n",
        "    df_train_filllabel = df_train[textcolumnslist].copy()\n",
        "    #concatinate with the NArows\n",
        "    df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "    #drop rows corresponding to True\n",
        "    df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns.get_values()[0]] == False]\n",
        "    \n",
        "    #delete the NArows column\n",
        "    df_train_filllabel = df_train_filllabel.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "    #create features df_train for rows needing infill\n",
        "    #create copy of df_train (note it already has NArows included)\n",
        "    df_train_fillfeatures = df_train.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns.get_values()[0]])]\n",
        "    #delete textcolumnslist and column+'_NArows'\n",
        "    df_train_fillfeatures = df_train_fillfeatures.drop(textcolumnslist, axis=1)\n",
        "    df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "    \n",
        "\n",
        "    \n",
        "    #create features df_test for rows needing infill\n",
        "    #create copy of df_test (note it already has NArows included)\n",
        "    df_test_fillfeatures = df_test.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns.get_values()[0]])]\n",
        "    #delete column and column+'_NArows'\n",
        "    df_test_fillfeatures = df_test_fillfeatures.drop(textcolumnslist, axis=1)\n",
        "    df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "    #delete NArows from df_train, df_test\n",
        "    df_train = df_train.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "    df_test = df_test.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "  if category == 'date':\n",
        "\n",
        "    #create empty sets for now\n",
        "    #an extension of this method would be to implement a comparable method \\\n",
        "    #for the time category, based on the columns output from the preprocessing\n",
        "    df_train_filltrain = pd.DataFrame({'foo' : []}) \n",
        "    df_train_filllabel = pd.DataFrame({'foo' : []})\n",
        "    df_train_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "    df_test_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "\n",
        "\n",
        "  return df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures\n",
        "\n",
        "\n",
        "#labelbinarizercorrect(npinput, columnslist), function that takes as input the output\\\n",
        "#array from scikit learn's LabelBinarizer() and ensures that the re-encoding is\\\n",
        "#consistent with the original array prior to performing the argmax. This is \\\n",
        "#needed because LabelBinarizer automatically takes two class sets to a binary\\\n",
        "#setting and doesn't account for columns above index of active values based on\\\n",
        "#my understanding. For a large enough dataset this probably won't be an issue \\\n",
        "#but just trying to be thorough. Outputs a one-hot encoded array comparable to \\\n",
        "#the format of our input to argmax.\n",
        "\n",
        "def labelbinarizercorrect(npinput, columnslist):\n",
        "  \n",
        "  \n",
        "  #if our array post application of LabelBinarizer has few coloumns than our \\\n",
        "  #column list then run through these loops\n",
        "  if npinput.shape[1] < len(columnslist):\n",
        "    \n",
        "    #if only one column in our array means LabelEncoder must have binarized \\\n",
        "    #since we already established that there are more columns\n",
        "    if npinput.shape[1] == 1:\n",
        "      \n",
        "      #this transfers from the binary encoding to two columns of one hot\n",
        "      npinput = np.hstack((1 - npinput, npinput))\n",
        "      \n",
        "      np_corrected = npinput\n",
        "      \n",
        "    #if we still have fewer columns than the column list, means we'll need to \\\n",
        "    #pad out with columns containing zeros\n",
        "    if npinput.shape[1] < len(columnslist):\n",
        "      missingcols = len(columnslist) - npinput.shape[1]\n",
        "      append = np.zeros((npinput.shape[0], missingcols))\n",
        "      np_corrected = np.concatenate((npinput, append), axis=1)\n",
        "  \n",
        "  else:\n",
        "    #otherwise just return the input array because it is in good shape\n",
        "    np_corrected = npinput\n",
        "\n",
        "  \n",
        "  return np_corrected\n",
        "\n",
        "\n",
        "\n",
        "#predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "#df_train_fillfeatures, df_test_fillfeatures, randomseed, textcolumnslist), \\\n",
        "#function that takes as input \\\n",
        "#a category string, the output of createMLinfillsets(.), a seed for randomness \\\n",
        "#and a list of columns produced by a text class preprocessor when applicable and \n",
        "#returns predicted infills for the train and test feature sets as df_traininfill, \\\n",
        "#df_testinfill based on derivations using scikit-learn, with the lenth of \\\n",
        "#infill consistent with the number of True values from NArows, and the trained \\\n",
        "#model\n",
        "\n",
        "\n",
        "#imports for numerical class training\n",
        "#from sklearn.linear_model import LinearRegression\n",
        "#from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "#from sklearn.linear_model import Ridge\n",
        "#from sklearn.linear_model import RidgeCV\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "#imports for binary and text class training\n",
        "from sklearn import preprocessing\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "def predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                  df_train_fillfeatures, df_test_fillfeatures, randomseed, \\\n",
        "                  textcolumnslist = []):\n",
        "  \n",
        "  \n",
        "  #a reasonable extension of this funciton would be to allow ML inference with \\\n",
        "  #other ML architectures such a SVM or something SGD based for instance\n",
        "  \n",
        "  #convert dataframes to numpy arrays\n",
        "  np_train_filltrain = df_train_filltrain.values\n",
        "  np_train_filllabel = df_train_filllabel.values\n",
        "  np_train_fillfeatures = df_train_fillfeatures.values\n",
        "  np_test_fillfeatures = df_test_fillfeatures.values\n",
        "  \n",
        "  #ony run the following if we have any rows needing infill\n",
        "  if df_train_fillfeatures.shape[0] > 0:\n",
        "\n",
        "    if category == 'nmbr':\n",
        "\n",
        "      #train linear regression model using scikit-learn for numerical prediction\n",
        "      #model = LinearRegression()\n",
        "      #model = PassiveAggressiveRegressor(random_state = randomseed)\n",
        "      #model = Ridge(random_state = randomseed)\n",
        "      #model = RidgeCV()\n",
        "      #note that SVR doesn't have an argument for random_state\n",
        "      model = SVR()\n",
        "      model.fit(np_train_filltrain, np_train_filllabel)    \n",
        "      \n",
        "      \n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "#       print('category is nmbr, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "  \n",
        "    if category == 'bnry':\n",
        "\n",
        "      #train logistic regression model using scikit-learn for binary classifier\n",
        "      #model = LogisticRegression()\n",
        "      #model = LogisticRegression(random_state = randomseed)\n",
        "      #model = SGDClassifier(random_state = randomseed)\n",
        "      model = SVC(random_state = randomseed)\n",
        "      \n",
        "      model.fit(np_train_filltrain, np_train_filllabel)\n",
        "\n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "#       print('category is bnry, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'text':\n",
        "\n",
        "      #first convert the one-hot encoded set via argmax to a 1D array\n",
        "      np_train_filllabel_argmax = np.argmax(np_train_filllabel, axis=1)\n",
        "\n",
        "      #train logistic regression model using scikit-learn for binary classifier\n",
        "      #with multi_class argument activated\n",
        "      #model = LogisticRegression()\n",
        "      #model = SGDClassifier(random_state = randomseed)\n",
        "      model = SVC(random_state = randomseed)\n",
        "      \n",
        "      model.fit(np_train_filltrain, np_train_filllabel_argmax)\n",
        "\n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        #this needs to have same number of columns as text category\n",
        "        np_testinfill = np.zeros(shape=(1,len(textcolumnslist)))\n",
        "\n",
        "      #convert the 1D arrary back to one hot encoding\n",
        "      labelbinarizertrain = preprocessing.LabelBinarizer()\n",
        "      labelbinarizertrain.fit(np_traininfill)\n",
        "      np_traininfill = labelbinarizertrain.transform(np_traininfill)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        labelbinarizertest = preprocessing.LabelBinarizer()\n",
        "        labelbinarizertest.fit(np_testinfill)\n",
        "        np_testinfill = labelbinarizertest.transform(np_testinfill)\n",
        "\n",
        "\n",
        "\n",
        "      #run function to ensure correct dimensions of re-encoded classifier array\n",
        "      np_traininfill = labelbinarizercorrect(np_traininfill, textcolumnslist)\n",
        "      \n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = labelbinarizercorrect(np_testinfill, textcolumnslist)\n",
        "\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = [textcolumnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [textcolumnslist]) \n",
        "      \n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'date':\n",
        "\n",
        "      #create empty sets for now\n",
        "      #an extension of this method would be to implement a comparable infill \\\n",
        "      #method for the time category, based on the columns output from the \\\n",
        "      #preprocessing\n",
        "      df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "      \n",
        "      model = False\n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "  \n",
        "  \n",
        "  #else if we didn't have any infill rows let's create some plug values\n",
        "  else:\n",
        "    \n",
        "    if category == 'text':\n",
        "      np_traininfill = np.zeros(shape=(1,len(textcolumnslist)))\n",
        "      np_testinfill = np.zeros(shape=(1,len(textcolumnslist)))\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = [textcolumnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [textcolumnslist]) \n",
        "    \n",
        "    else :\n",
        "      df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "    \n",
        "    #set model to False, this will be be needed for this eventiality in \n",
        "    #test set post-processing\n",
        "    model = False\n",
        "  \n",
        "  return df_traininfill, df_testinfill, model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3SRiIvDHz_jK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4) Update insertinfill function for speed"
      ]
    },
    {
      "metadata": {
        "id": "dpexmQGRHhgN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#insertinfill(df, column, infill, category, NArows, textcolumnslist = [])\n",
        "#function that takes as input a dataframe, column id, category string of either\\\n",
        "#'nmbr'/'text'/'bnry'/'date', a df column of True/False identifiying row id of\\\n",
        "#rows that will recieve infill, and and a list of columns produced by a text \\\n",
        "#class preprocessor when applicable. Replaces the column cells in rows \\\n",
        "#coresponding to the NArows True values with the values from infill, returns\\\n",
        "#the associated transformed dataframe.\n",
        "\n",
        "\n",
        "def insertinfill(df, column, infill, category, NArows, textcolumnslist = []):\n",
        "\n",
        "  if category == 'nmbr' or category == 'bnry':\n",
        "    \n",
        "    #create new dataframe for infills wherein the infill values are placed in \\\n",
        "    #rows coresponding to NArows True values and rows coresponding to NArows \\\n",
        "    #False values are filled with a 0\n",
        "\n",
        "    #assign index values to a column\n",
        "    df['tempindex1'] = df.index\n",
        "    \n",
        "    #concatinate our df with NArows\n",
        "    df = pd.concat([df, NArows], axis=1)\n",
        "    \n",
        "    #create list of index numbers coresponding to the NArows True values\n",
        "    infillindex = df.loc[df[column+'_NArows']]['tempindex1']\n",
        "    \n",
        "    #create a dictionary for use to insert infill using df's index as the key\n",
        "    infill_dict = dict(zip(infillindex, infill.values))\n",
        "    \n",
        "    #replace 'tempindex1' column with infill in rows where NArows is True\n",
        "    df['tempindex1'] = np.where(df[column+'_NArows'], df['tempindex1'].replace(infill_dict), 'fill')\n",
        "    \n",
        "    #now carry that infill over to the target column for rows where NArows is True\n",
        "    df[column] = np.where(df[column+'_NArows'], df['tempindex1'], df[column])\n",
        "    \n",
        "    #remove the temporary columns from df\n",
        "    df = df.drop(['tempindex1'], axis=1)\n",
        "    df = df.drop([column+'_NArows'], axis=1)\n",
        "    \n",
        "    \n",
        "    \n",
        "  if category == 'text':  \n",
        "\n",
        "    \n",
        "    #create new dataframe for infills wherein the infill values are placed in \\\n",
        "    #rows coresponding to NArows True values and rows coresponding to NArows \\\n",
        "    #False values are filled with a 0\n",
        "    \n",
        "    #text infill contains multiple columns for each predicted calssification\n",
        "    #which were derived from one-hot encoding the original column in preprocessing\n",
        "    for textcolumnname in textcolumnslist:\n",
        "      \n",
        "      #create newcolumn which will serve as the NArows specific to textcolumnname\n",
        "      df['textNArows'] = NArows\n",
        "      \n",
        "      df['textNArows'] = df['textNArows'].replace(0, False)\n",
        "      df['textNArows'] = df['textNArows'].replace(1, True)\n",
        "      \n",
        "      #assign index values to a column\n",
        "      df['tempindex1'] = df.index\n",
        "    \n",
        "    \n",
        "      #create list of index numbers coresponding to the NArows True values\n",
        "      textinfillindex = pd.DataFrame(df.loc[df['textNArows']]['tempindex1'])\n",
        "      #reset the index\n",
        "      textinfillindex = textinfillindex.reset_index()\n",
        "      \n",
        "      #now before we create our infill dicitonaries, we're going to need to\n",
        "      #create a seperate textinfillindex for each category\n",
        "      \n",
        "      infill['tempindex1'] = textinfillindex['tempindex1']\n",
        "      \n",
        "      #first let's create a copy of this textcolumn's infill column replacing \n",
        "      #0/1 with True False (this works because we are one hot encoding)\n",
        "      infill[textcolumnname + '_bool'] = infill[textcolumnname].astype('bool')\n",
        "      \n",
        "      #we'll use the mask feature to create infillindex which only contains \\\n",
        "      #rows coresponding to the True value in the column we just created\n",
        "\n",
        "      mask = (infill[[textcolumnname + '_bool']]==True).all(1)\n",
        "      infillindex = infill[mask]['tempindex1']\n",
        "      \n",
        "      \n",
        "      \n",
        "      #we're only going to insert the infill to column textcolumnname if we \\\n",
        "      #have infill to insert\n",
        "      \n",
        "      if len(infillindex.values) > 0:\n",
        "        \n",
        "        df.loc[infillindex.values[0], textcolumnname] = 1\n",
        "        \n",
        "\n",
        "      \n",
        "      \n",
        "      #now we'll delete temporary support columns associated with textcolumnname\n",
        "      infill = infill.drop([textcolumnname + '_bool'], axis=1)\n",
        "      infill = infill.drop(['tempindex1'], axis=1)\n",
        "      df = df.drop(['textNArows'], axis=1)\n",
        "      df = df.drop(['tempindex1'], axis=1)\n",
        "      \n",
        "  \n",
        "  if category == 'date':\n",
        "    #this spot reserved for future update to incorporate address of datetime\\\n",
        "    #category data\n",
        "    df = df\n",
        "  \n",
        "  \n",
        "  return df "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "04J0VIoj2bbW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #here is the original version of the function included here for validation \\\n",
        "# #of speedup\n",
        "\n",
        "# #insertinfill(df, column, infill, category, NArows, textcolumnslist = [])\n",
        "# #function that takes as input a dataframe, column id, category string of either\\\n",
        "# #'nmbr'/'text'/'bnry'/'date', a df column of True/False identifiying row id of\\\n",
        "# #rows that will recieve infill, and and a list of columns produced by a text \\\n",
        "# #class preprocessor when applicable. Replaces the column cells in rows \\\n",
        "# #coresponding to the NArows True values with the values from infill, returns\\\n",
        "# #the associated transformed dataframe.\n",
        "\n",
        "\n",
        "# def insertinfill(df, column, infill, category, NArows, textcolumnslist = []):\n",
        "  \n",
        "  \n",
        "#   if category == 'nmbr' or category == 'bnry':\n",
        "    \n",
        "#     #create new dataframe for infills wherein the infill values are placed in \\\n",
        "#     #rows coresponding to NArows True values and rows coresponding to NArows \\\n",
        "#     #False values are filled with a 0\n",
        "#     NAarray = []\n",
        "#     i=0\n",
        "#     for index, row in NArows.iterrows():\n",
        "#       if row[column+'_NArows'] == False:\n",
        "#         NAarray = np.append(NAarray, 0)\n",
        "#       if row[column+'_NArows'] == True:\n",
        "#         NAarray = np.append(NAarray, infill.iloc[i]['infill'])\n",
        "#         i += 1\n",
        "#     df_infill_full = pd.DataFrame(NAarray, columns = ['infill'])\n",
        "      \n",
        "\n",
        "#     #concatinate the dataframes df, NArows, and infill\n",
        "#     df = pd.concat([df, NArows], axis=1)\n",
        "#     df = pd.concat([df, df_infill_full], axis=1)\n",
        "    \n",
        "#     #for rows where NArows is true, replace value in column column with the \\\n",
        "#     #value from infill column\n",
        "#     df.loc[df[column+'_NArows'], column] = df['infill']\n",
        "    \n",
        "#     #now delete the helper columns\n",
        "#     df = df.drop([column+'_NArows'], axis=1)\n",
        "#     df = df.drop(['infill'], axis=1)\n",
        "    \n",
        "    \n",
        "#   if category == 'text':  \n",
        "\n",
        "    \n",
        "#     #create new dataframe for infills wherein the infill values are placed in \\\n",
        "#     #rows coresponding to NArows True values and rows coresponding to NArows \\\n",
        "#     #False values are filled with a 0\n",
        "    \n",
        "#     #text infill contains multiple columns for each predicted calssification\n",
        "#     #which were derived from one-hot encoding the original column in preprocessing\n",
        "#     for textcolumnname in textcolumnslist:\n",
        "      \n",
        "#       #create newcolumn which will serve as the NArows specific to textcolumnname\n",
        "#       df['textNArows'] = NArows\n",
        "      \n",
        "#       df['textNArows'] = df['textNArows'].replace(0, False)\n",
        "#       df['textNArows'] = df['textNArows'].replace(1, True)\n",
        "      \n",
        "      \n",
        "#       #this will give us an infill array specific to textcolumnname without 0's\n",
        "#       textarray = []\n",
        "#       i=0\n",
        "#       for index, row in df.iterrows():\n",
        "#         if row['textNArows'] == True:\n",
        "#           textarray = np.append(textarray, row[textcolumnname])\n",
        "#           i += 1\n",
        "      \n",
        "#       #now we'll use a comparable approach as we did for 'nmbr' and 'bnry'\n",
        "#       NAarray = []\n",
        "#       i=0\n",
        "#       j = infill.columns.get_loc(textcolumnname)\n",
        "      \n",
        "      \n",
        "#       for index, row in df.iterrows():\n",
        "      \n",
        "#         if row['textNArows'] == False:\n",
        "#           NAarray = np.append(NAarray, 0)\n",
        "        \n",
        "#         if row['textNArows'] == True:\n",
        "#           NAarray = np.append(NAarray, infill.iloc[i][textcolumnname][0])\n",
        "#           i += 1\n",
        "      \n",
        "#       df_infill_full = pd.DataFrame(NAarray, columns = ['infill'])\n",
        "    \n",
        "#       #concatinate the dataframes df, NArows, and infill\n",
        "#       #note we won't need to concatinate the NArows this time since we created\\\n",
        "#       #a column specific one called 'textNArows' which is already in place\n",
        "#       df = pd.concat([df, df_infill_full], axis=1)\n",
        "      \n",
        "#       #for rows where textNArows is true, replace value in column column with \\\n",
        "#       #the value from infill column\n",
        "#       df.loc[df['textNArows'], textcolumnname] = df['infill']\n",
        "      \n",
        "#       #now delete the helper columns\n",
        "#       df = df.drop(['textNArows'], axis=1)\n",
        "#       df = df.drop(['infill'], axis=1)\n",
        "  \n",
        "  \n",
        "#   if category == 'date':\n",
        "#     #this spot reserved for future update to incorporate address of datetime\\\n",
        "#     #category data\n",
        "#     df = df\n",
        "  \n",
        "#   return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MS4ulMW843U0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5) insert automunge(.) function"
      ]
    },
    {
      "metadata": {
        "id": "6gLKZbliVumS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#automunge(df_train, df_test, labels_column, valpercent=0.20, MLinfill = True, \\\n",
        "#infilliterate=1, randomseed = 42, excludetransformscolumns = []) \\\n",
        "#Function that when fed a train and test data set automates the process \\\n",
        "#of evaluating each column for determination and applicaiton of appropriate \\\n",
        "#preprocessing. Takes as arguement pandas dataframes of training and test data \\\n",
        "#(mdf_train), (mdf_test), the name of the column from train set containing \\\n",
        "#labels, a string identifying the ID column for train and test, a value for \\\n",
        "#percent of training data to be applied to a validation set, a True/False \\\n",
        "#selector to determine if MLinfill methods will be applied to any missing \\\n",
        "#points, an integer indication how many iterations of infill predfictions to \\\n",
        "#run, a random seed integer, and a list of any stroing column names that are to\\\n",
        "#be excluded from processing. (If MLinfill = False, missing points are addressed \\\n",
        "#with mean for numerical, most common value for binary, new column for one-hot \\\n",
        "#encoding, and mean for datetime). Note that the ML method for datetime data is \\\n",
        "#future extension. Based on an evaluation of columns selectively applies one of \\\n",
        "#four preprocessing functions to each. Shuffles the data and splits the training \\\n",
        "#set into train and validation sets. Returns following sets as numpy arrays: \\\n",
        "#train, trainID, labels, validation, validationID, validationlabels, test, testID\n",
        "\n",
        "#Note that this approach assumes that the test data is available at time of training\n",
        "#A different approach may be required if processing of test data is not simultaneous\n",
        "#although one potential solution is to apply this function intiially with a dummy\\\n",
        "#dataframe for test set and then when test data becomes available reapply \\\n",
        "#with original train set used for training the model along with the test set.\n",
        "\n",
        "#The thinking with the infilliterate approach is that for particularly messy \\\n",
        "#sets the predictinfill method will be influenced by the initial plug value \\\n",
        "#for missing cells, and so multiple iterations of the predictinfill should \\\n",
        "#trend towards better predictions. Initial tests of this iteration did not \\\n",
        "#demonstrate much effect so this probably is not neccesary for common use.\n",
        "\n",
        "#a word of caution: if you are excluding any columns from processing via \\\n",
        "#excludetransformscolumns list make sure they are already in a suitable state \\\n",
        "#for application of ML (e.g. numerical) otherwise the MLinfill technique will \\\n",
        "#return errors\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def automunge(df_train, df_test, labels_column, trainID_column = False, \\\n",
        "              testID_column = False, valpercent=0.20, MLinfill = True, \\\n",
        "              infilliterate=1, randomseed = 42, excludetransformscolumns = []):\n",
        "  \n",
        "  #An extension could be to test the input data here for non-dataframe format \\\n",
        "  #(such as csv) to convert it to pandas within the function. \n",
        "  \n",
        "  #my understanding is it is good practice to convert any None values into NaN \\\n",
        "  #so I'll just get that out of the way\n",
        "  df_train.fillna(value=float('nan'), inplace=True)\n",
        "  df_test.fillna(value=float('nan'), inplace=True)\n",
        "  \n",
        "  #we'll delete any rows from training set missing values in the labels column\n",
        "  df_train = df_train.dropna(subset=[labels_column])\n",
        "  \n",
        "  #extract the ID columns from train and test set\n",
        "  if trainID_column != False:\n",
        "    df_trainID = pd.DataFrame(df_train[trainID_column])\n",
        "    del df_train[trainID_column]\n",
        "    \n",
        "  if testID_column != False:\n",
        "    df_testID = pd.DataFrame(df_test[testID_column])\n",
        "    del df_test[testID_column]\n",
        "  \n",
        "  #extract labels from train set\n",
        "  #an extension to this function could be to delete the training set rows\\\n",
        "  #where the labels are missing or improperly formatted prior to performing\\\n",
        "  #this step\n",
        "  df_labels = pd.DataFrame(df_train[labels_column])\n",
        "  \n",
        "  #create copy of labels to support the translation dicitonary for use after \\\n",
        "  #prediction to convert encoded predictions back to the original label\n",
        "  df_labels2 = pd.DataFrame(df_labels.copy())\n",
        "  \n",
        "  del df_train[labels_column]\n",
        "  \n",
        "  \n",
        "  #confirm consistency of train an test sets\n",
        "  \n",
        "  #check number of columns is consistent\n",
        "  if df_train.shape[1] != df_test.shape[1]:\n",
        "    print(\"error, different number of columns in train and test sets\")\n",
        "    return\n",
        "  \n",
        "  #check column headers are consistent (this works independent of order)\n",
        "  columns_train = set(list(df_train))\n",
        "  columns_test = set(list(df_test))\n",
        "  if columns_train != columns_test:\n",
        "    print(\"error, different column labels in the train and test set\")\n",
        "    return\n",
        "\n",
        "  columns_train = list(df_train)\n",
        "  columns_test = list(df_test)\n",
        "  if columns_train != columns_test:\n",
        "    print(\"error, different order of column labels in the train and test set\")\n",
        "    return\n",
        "  \n",
        "  #extract column lists again but this time as a list\n",
        "  columns_train = list(df_train)\n",
        "  columns_test = list(df_test)\n",
        "\n",
        "  \n",
        "  #create an empty dataframe to serve as a store for each column's NArows\n",
        "  #the column id's for this df will follow convention from NArows of \n",
        "  #column+'_NArows' for each column in columns_train\n",
        "  #these are used in the ML infill methods\n",
        "  masterNArows_train = pd.DataFrame()\n",
        "  masterNArows_test = pd.DataFrame()\n",
        "  \n",
        "  #create an empty dictionary to serve as a store for each column's category\n",
        "  #this dictionary will store the key of the original column id with entry of \\\n",
        "  #the associated category string - these are used in the ML infill methods\n",
        "  mastercategory_dict = {}\n",
        "  \n",
        "  #create an empty dictionary to serve as a store specific to the text category\n",
        "  #our entries to this dictionary will store a master key from each point in \\\n",
        "  #the textcolumns array, with a nested name of original column under \\\n",
        "  #'origcolumn', the full textcolumns array under 'textcolumnsarray', and a \\\n",
        "  #True/False marker we'll call 'infillcomplete' for use in the ML infill methods\n",
        "  text_dict = {}\n",
        "  \n",
        "  #create an empty dictionary to serve as a store specific to the date category\n",
        "  #our entries to this dictionary will store a master key from each point in \\\n",
        "  #the datecolumns array, with a nested name of original column under \\\n",
        "  #'origcolumn', the full datecolumns array under 'textcolumnsarray', and a \\\n",
        "  #True/False marker we'll call 'infillcomplete' for use in the ML infill methods\n",
        "  date_dict = {}\n",
        "  \n",
        "  #create an empty dictionary to serve as a store of processing variables from \\\n",
        "  #processing that were specific to the train dataset. These can be used for \\\n",
        "  #future processing of a later test set without the need to reprocess the \\\n",
        "  #original train. The dictionary will be populated with an entry for each \\\n",
        "  #column post processing, and will contain a column specific and category \\\n",
        "  #specific (i.e. nmbr, bnry, text, date) set of variable.\n",
        "  postprocess_dict = {}\n",
        "  \n",
        "  \n",
        "  #For each column, determine appropriate processing function\n",
        "  #processing function will be based on evaluation of train set\n",
        "  for column in columns_train:\n",
        "    \n",
        "    #re-initialize the column specific dictionary for later insertion into\n",
        "    #our postprocess_dict\n",
        "    column_dict = {}\n",
        "    \n",
        "    #we're only going to process columns that weren't in our excluded set\n",
        "    if column not in excludetransformscolumns:\n",
        "      \n",
        "      category = evalcategory(df_train, column)\n",
        "      \n",
        "      #let's make sure the category is consistent between train and test sets\n",
        "      category_test = evalcategory(df_test, column)\n",
        "      if category != category_test:\n",
        "        print('error - different category between train and test sets for column ',\\\n",
        "             column)\n",
        "        \n",
        "      #here we'll delete any columns that returned a 'null' category\n",
        "      if category == 'null':\n",
        "        df_train = df_train.drop([column], axis=1)\n",
        "        df_test = df_test.drop([column], axis=1)\n",
        "        \n",
        "        column_dict = { column : {'category' : 'null'}}\n",
        "        \n",
        "        #now append column_dict onto postprocess_dict\n",
        "        postprocess_dict.update(column_dict)\n",
        "\n",
        "      \n",
        "      #so if we didn't delete the column let's proceed\n",
        "      else:\n",
        "        \n",
        "        #append this category onto our mastercategory_dict\n",
        "        mastercategory_dict.update({column+'cat': category})\n",
        "\n",
        "\n",
        "\n",
        "        #create NArows (column of True/False where True coresponds to missing data)\n",
        "        trainNArows = NArows(df_train, column, category)\n",
        "        testNArows = NArows(df_test, column, category)\n",
        "\n",
        "        #now append that NArows onto a master NA rows df\n",
        "        masterNArows_train = pd.concat([masterNArows_train, trainNArows], axis=1)\n",
        "        masterNArows_test = pd.concat([masterNArows_test, testNArows], axis=1)\n",
        "\n",
        "\n",
        "        #(now normalize as would normally)\n",
        "\n",
        "\n",
        "\n",
        "        #for binary class use the majority field for missing plug value\n",
        "        if category == 'bnry':\n",
        "          binary_missing_plug = df_train[column].value_counts().index.tolist()[0]\n",
        "\n",
        "\n",
        "        #apply appropriate processing function to this column based on the result\n",
        "        if category == 'bnry':\n",
        "          df_train = process_binary_class(df_train, column, binary_missing_plug)\n",
        "          df_test = process_binary_class(df_test, column, binary_missing_plug)\n",
        "          \n",
        "          #now create our column_dict for bnry category \n",
        "          column_dict = { column : {'category' : 'bnry', \\\n",
        "                                   'missing' : binary_missing_plug, \\\n",
        "                                   'infillmodel' : False}}\n",
        "          \n",
        "          #now append column_dict onto postprocess_dict\n",
        "          postprocess_dict.update(column_dict)\n",
        "          \n",
        "          \n",
        "        if category == 'nmbr':\n",
        "          df_train, df_test, mean, std = process_numerical_class(df_train, df_test, column)\n",
        "          \n",
        "          #now create our column_dict for nmbr category \n",
        "          column_dict = { column : {'category' : 'nmbr' , 'mean' : mean, \\\n",
        "                         'std' : std, 'infillmodel' : False}}\n",
        "          \n",
        "          #now append column_dict onto postprocess_dict\n",
        "          postprocess_dict.update(column_dict)\n",
        "          \n",
        "\n",
        "        if category == 'text':\n",
        "          df_train, df_test, textcolumns = process_text_class(df_train, df_test, column)\n",
        "          \n",
        "          #store some values in the text_dict{} for use later in ML infill methods\n",
        "          for tc in textcolumns:\n",
        "            text_dict.update({tc : {'origcolumn' : column, 'textcolumnsarray' : \\\n",
        "                                   textcolumns, 'infillcomplete' : False}})\n",
        "          \n",
        "            #now create our column_dict for text category \n",
        "            column_dict = {tc : {'category' : 'text', \\\n",
        "                           'origcolumn' : column, \\\n",
        "                           'textcolumnsarray' : textcolumns, \\\n",
        "                           'infillcoplete' : False, \\\n",
        "                           'infillmodel' : False}}\n",
        "          \n",
        "            #now append column_dict onto postprocess_dict\n",
        "            postprocess_dict.update(column_dict)\n",
        "          \n",
        "\n",
        "        if category == 'date':\n",
        "          df_train, df_test, datecolumns, timenormalization_dict = \\\n",
        "          process_time_class(df_train, df_test, column)\n",
        "\n",
        "          #store some values in the date_dict{} for use later in ML infill methods\n",
        "\n",
        "          for dc in datecolumns:\n",
        "            date_dict.update({dc : {'origcolumn' : column, 'datecolumnsarray' : \\\n",
        "                                   datecolumns, 'infillcomplete' : False}})\n",
        "          \n",
        "            #now create our column_dict for date category\n",
        "            column_dict = {dc :  {'category' : 'date', \\\n",
        "                                      'timenormalization_dict' : timenormalization_dict,\\\n",
        "                                      'origcolumn' : column, \\\n",
        "                                      'datecolumnsarray' : datecolumns, \\\n",
        "                                      'infillcoplete' : False, \\\n",
        "                                      'infillmodel' : False}}\n",
        "\n",
        "            #now append column_dict onto postprocess_dict\n",
        "            postprocess_dict.update(column_dict)\n",
        "\n",
        "  \n",
        "  #now that we've pre-processed all of the columns, let's run through them again\\\n",
        "  #using ML to derive plug values for the previously missing cells\n",
        "\n",
        "  \n",
        "  if MLinfill == True:\n",
        "    \n",
        "    \n",
        "    columns_train_ML = list(df_train)\n",
        "    columns_test_ML = list(df_test)\n",
        "    \n",
        "    \n",
        "    iteration = 0\n",
        "    \n",
        "    while iteration < infilliterate:\n",
        "      \n",
        "      #re-initialize the infillcomplete marker in text_dict and date_dict\n",
        "      for key in text_dict:\n",
        "        text_dict[key]['infillcomplete'] = False\n",
        "      for key in date_dict:\n",
        "        date_dict[key]['infillcomplete'] = False\n",
        "      \n",
        "      for column in columns_train_ML:\n",
        "\n",
        "        #we're only going to process columns that weren't in our excluded set\n",
        "        if column not in excludetransformscolumns:\n",
        "\n",
        "\n",
        "          #If column id is found in the text_dict then will require different \\\n",
        "          #type of address since this category won't be found in our \\\n",
        "          #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "          #collective group of columns from the associated textcolumns array.\n",
        "\n",
        "          if column in text_dict:\n",
        "\n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            if text_dict[column]['infillcomplete'] == False:\n",
        "\n",
        "              #pull this column's textcolumns array\n",
        "              textcolumns = text_dict[column]['textcolumnsarray']\n",
        "\n",
        "              category = 'text'\n",
        "\n",
        "              #now let's apply our functions for ML infill\n",
        "\n",
        "              #createMLinfillsets(df_train, df_test, column, trainNArows, \\\n",
        "              #testNArows, category, textcolumnslist = []), return \\\n",
        "              #df_train_filltrain, df_train_filllabel, df_train_fillfeatures, \\\n",
        "              #df_test_fillfeatures\n",
        "              df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures = \\\n",
        "              createMLinfillsets(df_train, df_test, column, pd.DataFrame(masterNArows_train[text_dict[column]['origcolumn']+'_NArows']), \\\n",
        "                                 pd.DataFrame(masterNArows_test[text_dict[column]['origcolumn']+'_NArows']), category, \\\n",
        "                                 textcolumnslist = textcolumns)          \n",
        "\n",
        "\n",
        "              #predict infill values using defined function predictinfill(.)\n",
        "              df_traininfill, df_testinfill, model = \\\n",
        "              predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                            df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                            randomseed, textcolumnslist = textcolumns)\n",
        "              \n",
        "              #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "              #to df's associated column\n",
        "              \n",
        "              df_train = insertinfill(df_train, column, df_traininfill, category, \\\n",
        "                                      pd.DataFrame(masterNArows_train[text_dict[column]['origcolumn']+'_NArows']), \\\n",
        "                                      textcolumnslist = textcolumns)\n",
        "              \n",
        "              #now we'll add our trained text model to the postprocess_dict\n",
        "              #postprocess_dict[text_dict[column]['origcolumn']]['infillmodel'] \\\n",
        "              #postprocess_dict[text_dict[column]['origcolumn']['infillmodel']] \\\n",
        "              postprocess_dict[column]['infillmodel'] \\\n",
        "              = model\n",
        "\n",
        "              \n",
        "              \n",
        "              #it's a quirk of the ML models that if we don't train the\n",
        "              #train set model on any features, that we won't be able to apply\n",
        "              #the model to predict the test set infill. \n",
        "              #For now we'll only use insertilnfill if we had\n",
        "              #some missing points in the train set, a future extension would be\n",
        "              #to update our createMLinfillsets and predictinfill to create \n",
        "              #some arbitrary features to train the infill on train set for\n",
        "              #cases where there are NaN values in test set but not in train\n",
        "              #such that we could insert infill for missing values in test set. \n",
        "              if any(x == True for x in masterNArows_train[text_dict[column]['origcolumn']+'_NArows']):\n",
        "              \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[text_dict[column]['origcolumn']+'_NArows']), \\\n",
        "                                       textcolumnslist = textcolumns)\n",
        "\n",
        "              #now change the infillcomplete marker in the text_dict for each \\\n",
        "              #associated text column\n",
        "              for textcolumnname in textcolumns:\n",
        "                text_dict[textcolumnname]['infillcomplete'] = True\n",
        "\n",
        "\n",
        "          #If column id is found in the date_dict then will require different \\\n",
        "          #type of address since this category won't be found in our \\\n",
        "          #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "          #collective group of columns from the associated datecolumns array. \\\n",
        "          #The development of this address for date columns is a future extension.\n",
        "          elif column in date_dict:\n",
        "\n",
        "            #this section to be a future extension.\n",
        "            pass\n",
        "\n",
        "            \n",
        "          else:\n",
        "            #this is for columns that weren't found in the text_dict or date_dict\n",
        "            #For each column, determine appropriate processing function\n",
        "            #processing function will be based on evaluation of train set\n",
        "\n",
        "            #pull category from dictionary\n",
        "            category = mastercategory_dict[column+'cat']\n",
        "\n",
        "\n",
        "            #going to break number and binary into seperate process\n",
        "            \n",
        "            if category == 'bnry':\n",
        "              \n",
        "              #create MLinfill sets using defined function createMLinfillsets(.)\n",
        "              df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures = \\\n",
        "              createMLinfillsets(df_train, df_test, column, pd.DataFrame(masterNArows_train[column+'_NArows']), \\\n",
        "                                 masterNArows_test[column+'_NArows'], category)\n",
        "              \n",
        "              #predict infill values using defined function predictinfill(.)\n",
        "              df_traininfill, df_testinfill, model = \\\n",
        "              predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                            df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                            randomseed, textcolumnslist = [])\n",
        "              \n",
        "              #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "              #to df's associated column\n",
        "              df_train = insertinfill(df_train, column, df_traininfill, category, \\\n",
        "                                      pd.DataFrame(masterNArows_train[column+'_NArows']), \\\n",
        "                                      textcolumnslist = [])\n",
        "              \n",
        "              #(only insert infill to test set if we had NA rows in train set)\n",
        "              #note comments above in text class for potential future expansion\n",
        "              if any(x == True for x in masterNArows_train[column+'_NArows']):\n",
        "                \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[column+'_NArows']), \\\n",
        "                                       textcolumnslist = [])\n",
        "              \n",
        "              #now we'll add our trained bnry model to the postprocess_dict\n",
        "              postprocess_dict[column]['infillmodel'] = model\n",
        "              \n",
        "            if category == 'nmbr':\n",
        "              \n",
        "              #create MLinfill sets using defined function createMLinfillsets(.)\n",
        "              df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures = \\\n",
        "              createMLinfillsets(df_train, df_test, column, pd.DataFrame(masterNArows_train[column+'_NArows']), \\\n",
        "                                 masterNArows_test[column+'_NArows'], category)\n",
        "              \n",
        "              #predict infill values using defined function predictinfill(.)\n",
        "              df_traininfill, df_testinfill, model = \\\n",
        "              predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                            df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                            randomseed, textcolumnslist = [])\n",
        "              \n",
        "              #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "              #to df's associated column\n",
        "              df_train = insertinfill(df_train, column, df_traininfill, category, \\\n",
        "                                      pd.DataFrame(masterNArows_train[column+'_NArows']), \\\n",
        "                                      textcolumnslist = [])\n",
        "              \n",
        "              #(only insert infill to test set if we had NA rows in train set)\n",
        "              #note comments above in text class for potential future expansion\n",
        "              if any(x == True for x in masterNArows_train[column+'_NArows']):\n",
        "                \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[column+'_NArows']), \\\n",
        "                                       textcolumnslist = [])\n",
        "              \n",
        "              #now we'll add our trained nmbr model to the postprocess_dict\n",
        "              postprocess_dict[column]['infillmodel'] = model\n",
        "            \n",
        "      iteration += 1\n",
        "\n",
        "  \n",
        "  #determine labels category and apply appropriate function\n",
        "  labelscategory = evalcategory(df_labels, labels_column)\n",
        "  \n",
        "  #empty dummy labels \"test\" df for our preprocessing functions\n",
        "  labelsdummy = pd.DataFrame()\n",
        "  \n",
        "  #initialize a dictionary to serve as the store between labels and their \\\n",
        "  #associated encoding\n",
        "  labelsencoding_dict = {labelscategory:{}}\n",
        "  \n",
        "  #apply appropriate processing function to this column based on the result\n",
        "  if labelscategory == 'bnry':\n",
        "    labels_binary_missing_plug = df_labels[labels_column].value_counts().index.tolist()[0]\n",
        "    df_labels = process_binary_class(df_labels, labels_column, labels_binary_missing_plug)\n",
        "    \n",
        "    #here we'll populate the dictionery pairing values from the encoded labels \\\n",
        "    #column with the original value for transformation post prediciton\n",
        "    \n",
        "    \n",
        "    i = 0\n",
        "    \n",
        "    for row in df_labels.iterrows():\n",
        "      if row[1][0] in labelsencoding_dict[labelscategory].keys():\n",
        "          i += 1\n",
        "      else:\n",
        "          labelsencoding_dict[labelscategory].update({row[1][0] : df_labels2.iloc[i][0]})\n",
        "          i += 1\n",
        "\n",
        "      \n",
        "  if labelscategory == 'nmbr':\n",
        "    \n",
        "    #if labels category is 'nmbr' we won't apply any further processing to the \\\n",
        "    #column as my experience with linear regression methods is that this is not\\\n",
        "    #required. Further processing of numerical labels would need to be addressed\\\n",
        "    #by returning mean and std from the process_numerical_class method so as to\\\n",
        "    #potentially store in our labelsencoding_dict\n",
        "    pass\n",
        "    \n",
        "    \n",
        "  #it occurs to me there might be an argument for preferring a single numerical \\\n",
        "  #classifier for labels to keep this to a single column, if so scikitlearn's \\\n",
        "  #LabelEcncoder could be used here, will assume that onehot encoding is acceptable\n",
        "  if labelscategory == 'text':\n",
        "    \n",
        "    df_labels, labelsdummy, labelcolumnsdummy = \\\n",
        "    process_text_class(df_labels, labelsdummy, labels_column)\n",
        "  \n",
        "    i = 0\n",
        "    \n",
        "    for row in df_labels2.iterrows():\n",
        "      if row[1][0] in labelsencoding_dict[labelscategory].keys():\n",
        "          i += 1\n",
        "      else:\n",
        "          labelsencoding_dict[labelscategory].\\\n",
        "          update({row[1][0] : labels_column+'_'+row[1][0]})\n",
        "          i += 1\n",
        "    \n",
        "  \n",
        "  #great the data is processed now let's do a few moore global training preps\n",
        "  \n",
        "  \n",
        "  #here's a list of final column names saving here since the translation to \\\n",
        "  #numpy arrays scrubs the column names\n",
        "  finalcolumns_train = list(df_train)\n",
        "  finalcolumns_test = list(df_test)\n",
        "  \n",
        "  \n",
        "  #convert all of our dataframes to numpy arrays (train, test, labels, and ID)\n",
        "  #    df_trainID, df_testID\n",
        "  np_train = df_train.values\n",
        "  np_test = df_test.values\n",
        "  np_labels = df_labels.values\n",
        "  \n",
        "  if trainID_column != False:\n",
        "    np_trainID = df_trainID.values\n",
        "  if testID_column != False:\n",
        "    np_testID = df_testID.values\n",
        "  \n",
        "  \n",
        "  #set randomness seed number\n",
        "  answer = randomseed\n",
        "  #a reasonable extension would be to tie this in with randomness seed for \\\n",
        "  #ML infill methods calls to scikit learn\n",
        "  \n",
        "  #shuffle training set and labels\n",
        "  np_train = shuffle(np_train, random_state = answer)\n",
        "  np_labels = shuffle(np_labels, random_state = answer)\n",
        "  \n",
        "  if trainID_column != False:\n",
        "    np_trainID = shuffle(np_trainID, random_state = answer)\n",
        "  \n",
        "  \n",
        "  #split validation sets from training and labels\n",
        "  train, validation, labels, validationlabels = \\\n",
        "  train_test_split(np_train, np_labels, test_size=valpercent, shuffle = False)\n",
        "  \n",
        "  if trainID_column != False:\n",
        "    trainID, validationID = \\\n",
        "    train_test_split(np_trainID, test_size=valpercent, shuffle = False)\n",
        "  else:\n",
        "    trainID = []\n",
        "    validationID = []\n",
        "  if testID_column != False:\n",
        "    testID = np_testID\n",
        "  else:\n",
        "    testID = []\n",
        "  \n",
        "  test = np_test\n",
        "  \n",
        "  #now let's store some additional global parameters into our postprocess_dict\n",
        "  postprocess_dict.update({'origtraincolumns' : columns_train, \\\n",
        "                           'finalcolumns_train' : finalcolumns_train, \\\n",
        "                           'testID_column' : testID_column, \\\n",
        "                           'mastercategory_dict' : mastercategory_dict, \\\n",
        "                           'text_dict' : text_dict, \\\n",
        "                           'date_dict' : date_dict, \\\n",
        "                           'MLinfill' : MLinfill, \\\n",
        "                           'infilliterate' : infilliterate, \\\n",
        "                           'randomseed' : randomseed, \\\n",
        "                           'excludetransformscolumns' : excludetransformscolumns,\\\n",
        "                           'labelsencoding_dict' : labelsencoding_dict, \\\n",
        "                           'automungeversion' : '1.1' })\n",
        "  \n",
        "  \n",
        "  #a reasonable extension would be to perform some validation functions on the\\\n",
        "  #sets here (or also prior to transofrm to numpuy arrays) and confirm things \\\n",
        "  #like consistency between format of columns and data between our train and \\\n",
        "  #test sets and if any issues return a coresponding error message to alert user\n",
        "  \n",
        "  \n",
        "  return train, trainID, labels, validation, validationID, validationlabels, \\\n",
        "  test, testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test,  \\\n",
        "  postprocess_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JEOu259JTF66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that a future extension will be to create some standardized functions recreatelabels(.) for post prediction transforms of predicitons from numerical to string values if applicable."
      ]
    },
    {
      "metadata": {
        "id": "bmsGjmPt0_xC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6) insert postprocess functions for test set"
      ]
    },
    {
      "metadata": {
        "id": "_V4UL-91jA5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #Here is a summary of the postprocess_dict structure from automunge:\n",
        "\n",
        "  \n",
        "# postprocess_dict.update({'origtraincolumns' : columns_train, \\\n",
        "#                        'finalcolumns_train' : finalcolumns_train, \\\n",
        "#                        'testID_column' : testID_column, \\\n",
        "#                        'mastercategory_dict' : mastercategory_dict, \\\n",
        "#                        'text_dict' : text_dict, \\\n",
        "#                        'date_dict' : date_dict, \\\n",
        "#                        'MLinfill' : MLinfill, \\\n",
        "#                        'infilliterate' : infilliterate, \\\n",
        "#                        'randomseed' : randomseed, \\\n",
        "#                        'excludetransformscolumns' : excludetransformscolumns,\\\n",
        "#                        'labelsencoding_dict' : labelsencoding_dict, \\\n",
        "#                        'automungeversion' : '1.2', \n",
        "#                        column_dict })\n",
        "\n",
        "          \n",
        "# #column_dict for bnry category \n",
        "# column_dict = { column : {'category' : 'bnry', \\\n",
        "#                          'missing' : binary_missing_plug, \\\n",
        "#                          'infillmodel' : False}}\n",
        "\n",
        "# #column_dict for nmbr category \n",
        "# column_dict = { column : {'category' : 'nmbr' , 'mean' : mean, \\\n",
        "#                'std' : std, 'infillmodel' : False}}\n",
        "\n",
        "\n",
        "# #column_dict for text category \n",
        "# column_dict = {column : {'category' : 'text', \\\n",
        "#                'origcolumn' : column, \\\n",
        "#                'textcolumnsarray' : textcolumns, \\\n",
        "#                'infillcoplete' : False,\n",
        "#                'infillmodel' : False}}\n",
        "\n",
        "# #column_dict for date category\n",
        "# column_dict = { column :  {'category' : 'date', \n",
        "#                           'timenormalization_dict' : timenormalization_dict,\\\n",
        "#                           'origcolumn' : column, \\\n",
        "#                           'datecolumnsarray' : datecolumns, \\\n",
        "#                           'infillcoplete' : False,\n",
        "#                           'infillmodel' : False}}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HKPfQyjI4-FM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#postprocess_numerical_class(mdf_test, column, mean, std)\n",
        "#function to normalize data to mean of 0 and standard deviation of 1 from training distribution\n",
        "#takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\\\n",
        "#and the name of the column string ('column'), and the mean and std from the train set \\\n",
        "#replaces missing or improperly formatted data with mean of remaining values\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#expect this approach works better when the numerical distribution is thin tailed\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "#imports\n",
        "from pandas import Series\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def postprocess_numerical_class(mdf_test, column, mean, std):\n",
        "     \n",
        "    \n",
        "  #convert all values to either numeric or NaN\n",
        "  mdf_test[column] = pd.to_numeric(mdf_test[column], errors='coerce')\n",
        "\n",
        "  #get mean of training data\n",
        "  mean = mean  \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_test[column] = mdf_test[column].fillna(mean)\n",
        "\n",
        "  #subtract mean from column\n",
        "  mdf_test[column] = mdf_test[column] - mean\n",
        "\n",
        "  #get standard deviation of training data\n",
        "  std = std\n",
        "\n",
        "  #divide column values by std\n",
        "  mdf_test[column] = mdf_test[column] / std\n",
        "\n",
        "\n",
        "  return mdf_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#postprocess_text_class(mdf_test, column, textcolumns)\n",
        "#process column with text classifications\n",
        "#takes as arguement pandas dataframe containing test data  \n",
        "#()mdf_test), and the name of the column string ('column'), and an array of\n",
        "#the associated transformed column s from the train set (textcolumns)\n",
        "\n",
        "#note this aligns formatting of transformed columns to the original train set\n",
        "#fromt he original treatment with automunge\n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with onehot encodings\n",
        "#with columns named after column_ + text classifications\n",
        "#missing data replaced with category label 'missing'+column\n",
        "#any categories missing from the training set removed from test set\n",
        "#any category present in training but missing from test set given a column of zeros for consistent formatting\n",
        "#ensures order of all new columns consistent between both sets\n",
        "#returns two transformed dataframe (mdf_train, mdf_test) \\\n",
        "#and a list of the new column names (textcolumns)\n",
        "\n",
        "#note it is kind of a hack here to create a column for missing values with \\\n",
        "#two underscores (__) in the column name to ensure appropriate order for cases\\\n",
        "#where NaN present in test data but not train data, if a category starts with|\n",
        "#an underscore such that it preceeds '__missing' alphabetically in this scenario\\\n",
        "#this might create error due to different order of columns, address of this \\\n",
        "#potential issue will be a future extension\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def postprocess_text_class(mdf_test, column, textcolumns):\n",
        "\n",
        "  #replace NA with a dummy variable\n",
        "  mdf_test[column] = mdf_test[column].fillna('_missing')\n",
        "\n",
        "\n",
        "  #extract categories for column labels\n",
        "  #note that .unique() extracts the labels as a numpy array\n",
        "  \n",
        "  #we'll get the category names from the textcolumns array by stripping the \\\n",
        "  #prefixes of column name + '_'\n",
        "  prefixlength = len(column)+1\n",
        "  labels_train = textcolumns[:]\n",
        "  for textcolumn in labels_train:\n",
        "    textcolumn = textcolumn[prefixlength :]\n",
        "  #labels_train.sort(axis=0)\n",
        "  labels_train.sort()\n",
        "  labels_test = mdf_test[column].unique()\n",
        "  labels_test.sort(axis=0)\n",
        "\n",
        "  #transform text classifications to numerical id\n",
        "  encoder = LabelEncoder()\n",
        "#   cat_train = mdf_train[column]\n",
        "#   cat_train_encoded = encoder.fit_transform(cat_train)\n",
        "\n",
        "  cat_test = mdf_test[column]\n",
        "  cat_test_encoded = encoder.fit_transform(cat_test)\n",
        "\n",
        "\n",
        "  #apply onehotencoding\n",
        "  onehotencoder = OneHotEncoder()\n",
        "#   cat_train_1hot = onehotencoder.fit_transform(cat_train_encoded.reshape(-1,1))\n",
        "  cat_test_1hot = onehotencoder.fit_transform(cat_test_encoded.reshape(-1,1))\n",
        "\n",
        "  #append column header name to each category listing\n",
        "  #note the iteration is over a numpy array hence the [...] approach  \n",
        "#   labels_train[...] = column + '_' + labels_train[...]\n",
        "  labels_test[...] = column + '_' + labels_test[...]\n",
        "\n",
        "\n",
        "  #convert sparse array to pandas dataframe with column labels\n",
        "#   df_train_cat = pd.DataFrame(cat_train_1hot.toarray(), columns=labels_train)\n",
        "  df_test_cat = pd.DataFrame(cat_test_1hot.toarray(), columns=labels_test)\n",
        "  \n",
        "#   #add a missing column to train if it's not present\n",
        "#   if column + '__missing' not in df_train_cat.columns:\n",
        "#     missingcolumn = pd.DataFrame(0, index=np.arange(df_train_cat.shape[0]), columns=[column+'__missing'])\n",
        "#     df_train_cat = pd.concat([missingcolumn, df_train_cat], axis=1)\n",
        "\n",
        "\n",
        "  #Get missing columns in test set that are present in training set\n",
        "  missing_cols = set( textcolumns ) - set( df_test_cat.columns )\n",
        "  \n",
        "  #Add a missing column in test set with default value equal to 0\n",
        "  for c in missing_cols:\n",
        "      df_test_cat[c] = 0\n",
        "  \n",
        "  #Ensure the order of column in the test set is in the same order than in train set\n",
        "  #Note this also removes categories in test set that aren't present in training set\n",
        "  df_test_cat = df_test_cat[textcolumns]\n",
        "\n",
        "\n",
        "  #concatinate the sparse set with the rest of our training data\n",
        "#   mdf_train = pd.concat([df_train_cat, mdf_train], axis=1)\n",
        "  mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)\n",
        "\n",
        "\n",
        "  #delete original column from training data\n",
        "#   del mdf_train[column]    \n",
        "  del mdf_test[column]\n",
        "  \n",
        "#   #create output of a list of the created column names\n",
        "#   labels_train = list(df_train_cat)\n",
        "#   textcolumns = labels_train\n",
        "  \n",
        "  \n",
        "\n",
        "  return mdf_test\n",
        "\n",
        "\n",
        "\n",
        "#postprocess_time_class(mdf_test, column, datecolumns, timenormalization_dict)\n",
        "#postprocess test column with of date category\n",
        "#takes as arguement pandas dataframe containing test data \n",
        "#(mdf_test), the name of the column string ('column'), and the timenormalization_dict \n",
        "#from the original application of automunge to the associated date column from train set\n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with distinct columns for year, month, day, hour, minute, second\n",
        "#each normalized to the mean and std from original train set, \n",
        "#with missing values plugged with the mean\n",
        "#with columns named after column_ + time category\n",
        "#returns two transformed dataframe (mdf_train, mdf_test)\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "def postprocess_time_class(mdf_test, column, datecolumns, timenormalization_dict):\n",
        "  \n",
        "  #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data\n",
        "#   mdf_train[column] = pd.to_datetime(mdf_train[column], errors = 'coerce')\n",
        "  mdf_test[column] = pd.to_datetime(mdf_test[column], errors = 'coerce')\n",
        "  \n",
        "  #mdf_train[column].replace(-np.Inf, np.nan)\n",
        "  #mdf_test[column].replace(-np.Inf, np.nan)\n",
        "  \n",
        "  #get mean of various categories of datetime objects to use to plug in missing cells\n",
        "#   meanyear = mdf_train[column].dt.year.mean()    \n",
        "#   meanmonth = mdf_train[column].dt.month.mean()\n",
        "#   meanday = mdf_train[column].dt.day.mean()\n",
        "#   meanhour = mdf_train[column].dt.hour.mean()\n",
        "#   meanminute = mdf_train[column].dt.minute.mean()\n",
        "#   meansecond = mdf_train[column].dt.second.mean()\n",
        "  \n",
        "  meanyear = timenormalization_dict['meanyear']\n",
        "  meanmonth = timenormalization_dict['meanmonth']\n",
        "  meanday = timenormalization_dict['meanday']\n",
        "  meanhour = timenormalization_dict['meanhour']\n",
        "  meanminute = timenormalization_dict['meanminute']\n",
        "  meansecond = timenormalization_dict['meansecond']\n",
        "  \n",
        "\n",
        "  #get standard deviation of training data\n",
        "#   stdyear = mdf_train[column].dt.year.std()  \n",
        "#   stdmonth = mdf_train[column].dt.month.std()\n",
        "#   stdday = mdf_train[column].dt.day.std()\n",
        "#   stdhour = mdf_train[column].dt.hour.std()\n",
        "#   stdminute = mdf_train[column].dt.minute.std()\n",
        "#   stdsecond = mdf_train[column].dt.second.std()\n",
        "  \n",
        "  stdyear = timenormalization_dict['stdyear']\n",
        "  stdmonth = timenormalization_dict['stdmonth']\n",
        "  stdday = timenormalization_dict['stdday']\n",
        "  stdhour = timenormalization_dict['stdhour']\n",
        "  stdminute = timenormalization_dict['stdminute']\n",
        "  stdsecond = timenormalization_dict['stdsecond']\n",
        "  \n",
        "  \n",
        "#   #create new columns for each category in train set\n",
        "#   mdf_train[column + '_year'] = mdf_train[column].dt.year\n",
        "#   mdf_train[column + '_month'] = mdf_train[column].dt.month\n",
        "#   mdf_train[column + '_day'] = mdf_train[column].dt.day\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column].dt.hour\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column].dt.minute\n",
        "#   mdf_train[column + '_second'] = mdf_train[column].dt.second\n",
        "  \n",
        "  #create new columns for each category in test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column].dt.year\n",
        "  mdf_test[column + '_month'] = mdf_test[column].dt.month\n",
        "  mdf_test[column + '_day'] = mdf_test[column].dt.day\n",
        "  mdf_test[column + '_hour'] = mdf_test[column].dt.hour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column].dt.minute \n",
        "  mdf_test[column + '_second'] = mdf_test[column].dt.second\n",
        "  \n",
        "\n",
        "#   #replace missing data with training set mean\n",
        "#   mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(meanyear)\n",
        "#   mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(meanmonth)\n",
        "#   mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(meanday)\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(meanhour)\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(meanminute)\n",
        "#   mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #do same for test set (replace missing data with training set mean)\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(meanyear)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(meanmonth)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(meanday)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(meanhour)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(meanminute)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #subtract mean from column for both train and test\n",
        "#   mdf_train[column + '_year'] = mdf_train[column + '_year'] - meanyear\n",
        "#   mdf_train[column + '_month'] = mdf_train[column + '_month'] - meanmonth\n",
        "#   mdf_train[column + '_day'] = mdf_train[column + '_day'] - meanday\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column + '_hour'] - meanhour\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column + '_minute'] - meanminute\n",
        "#   mdf_train[column + '_second'] = mdf_train[column + '_second'] - meansecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] - meanyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] - meanmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] - meanday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] - meanhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] - meanminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] - meansecond\n",
        "  \n",
        "  \n",
        "  #divide column values by std for both training and test data\n",
        "#   mdf_train[column + '_year'] = mdf_train[column + '_year'] / stdyear\n",
        "#   mdf_train[column + '_month'] = mdf_train[column + '_month'] / stdmonth\n",
        "#   mdf_train[column + '_day'] = mdf_train[column + '_day'] / stdday\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column + '_hour'] / stdhour\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column + '_minute'] / stdminute\n",
        "#   mdf_train[column + '_second'] = mdf_train[column + '_second'] / stdsecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] / stdyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] / stdmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] / stdday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] / stdhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] / stdminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] / stdsecond\n",
        "  \n",
        "  \n",
        "  #now replace NaN with 0\n",
        "#   mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(0)\n",
        "#   mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(0)\n",
        "#   mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(0)\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(0)\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(0)\n",
        "#   mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(0)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(0)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(0)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(0)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(0)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(0)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(0)\n",
        "  \n",
        "    \n",
        "#   #output of a list of the created column names\n",
        "#   datecolumns = [column + '_year', column + '_month', column + '_day', \\\n",
        "#                 column + '_hour', column + '_minute', column + '_second']\n",
        "  \n",
        "  #this is to address an issue I found when parsing columns with only time no date\n",
        "  #which returned -inf vlaues, so if an issue will just delete the associated \n",
        "  #column along with the entry in datecolumns\n",
        "#   checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])\n",
        "#   if checkyear:\n",
        "#     del mdf_train[column + '_year']\n",
        "#     if column + '_year' in mdf_test.columns:\n",
        "#       del mdf_test[column + '_year']\n",
        "\n",
        "#   checkmonth = np.isinf(mdf_train.iloc[0][column + '_month'])\n",
        "#   if checkmonth:\n",
        "#     del mdf_train[column + '_month']\n",
        "#     if column + '_month' in mdf_test.columns:\n",
        "#       del mdf_test[column + '_month']\n",
        "\n",
        "#   checkday = np.isinf(mdf_train.iloc[0][column + '_day'])\n",
        "#   if checkmonth:\n",
        "#     del mdf_train[column + '_day']\n",
        "#     if column + '_day' in mdf_test.columns:\n",
        "#       del mdf_test[column + '_day']\n",
        "\n",
        "  #instead we'll just delete a column from test set if not found in train set\n",
        "  if column + '_year' not in datecolumns:\n",
        "    del mdf_test[column + '_year']\n",
        "#     datecolumns.remove(column + '_year')\n",
        "  if column + '_month' not in datecolumns:\n",
        "    del mdf_test[column + '_month'] \n",
        "#     datecolumns.remove(column + '_month')\n",
        "  if column + '_day' not in datecolumns:\n",
        "    del mdf_test[column + '_day']  \n",
        "#     datecolumns.remove(column + '_day')\n",
        "  if column + '_hour' not in datecolumns:\n",
        "    del mdf_test[column + '_hour']\n",
        "#     datecolumns.remove(column + '_hour')\n",
        "  if column + '_minute' not in datecolumns:\n",
        "    del mdf_test[column + '_minute'] \n",
        "#     datecolumns.remove(column + '_minute')\n",
        "  if column + '_second' not in datecolumns:\n",
        "    del mdf_test[column + '_second'] \n",
        "#     datecolumns.remove(column + '_second')\n",
        "  \n",
        "  \n",
        "  #delete original column from training data\n",
        "  if column in mdf_test.columns:\n",
        "    del mdf_test[column]  \n",
        "\n",
        "  \n",
        "#   #output a dictionary of the associated column mean and std\n",
        "  \n",
        "#   timenormalization_dict = {'meanyear' : meanyear, 'meanmonth' : meanmonth, \\\n",
        "#                             'meanday' : meanday, 'meanhour' : meanhour, \\\n",
        "#                             'meanminute' : meanminute, 'meansecond' : meansecond,\\\n",
        "#                             'stdyear' : stdyear, 'stdmonth' : stdmonth, \\\n",
        "#                             'stdday' : stdday, 'stdhour' : stdhour, \\\n",
        "#                             'stdminute' : stdminute, 'stdsecond' : stdsecond}\n",
        "  \n",
        "  \n",
        "  return mdf_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lv1J5tknpULt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 7) insert postprocess ML infill functions for test set"
      ]
    },
    {
      "metadata": {
        "id": "xbC327kRH3bV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#createpostMLinfillsets(df_test, column, testNArows, category, \\\n",
        "#textcolumnslist = []) function that when fed dataframe of\n",
        "#test set, column id, df of True/False corresponding to rows from original \\\n",
        "#sets with missing values, a string category of 'text', 'date', 'nmbr', or \\\n",
        "#'bnry', and a list of column id's for the text category if applicable. The \\\n",
        "#function returns a series of dataframes which can be applied to apply a \\\n",
        "#machine learning model previously trained on our train set as part of the \n",
        "#original automunge application to predict apppropriate infill values for those\\\n",
        "#points that had missing values from the original sets, returning the dataframe\\\n",
        "#df_test_fillfeatures\n",
        "\n",
        "\n",
        "def createpostMLinfillsets(df_test, column, testNArows, category, textcolumnslist = []):\n",
        "\n",
        "  \n",
        "  #create a test features column \n",
        "\n",
        "  #reminder:\n",
        "    #for numerical there won't be a new column\n",
        "    #for binary there won't be a new column\n",
        "    #for text the new column has a defined name as column+'_missing'\n",
        "\n",
        "  #note that for text class the labels will be a little more complicated \\\n",
        "  #since will be multi-column\n",
        "\n",
        "  if category == 'nmbr' or category == 'bnry':\n",
        "\n",
        "    #first concatinate the NArows True/False designations to df_test\n",
        "#     df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "    df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "    \n",
        "#     #create copy of df_train to serve as training set for fill\n",
        "#     df_train_filltrain = df_train.copy()\n",
        "#     #now delete rows coresponding to True\n",
        "#     df_train_filltrain = df_train_filltrain[df_train_filltrain[column+'_NArows'] == False]\n",
        "    \n",
        "#     #now delete [column] and the NA labels (column+'NA') from this df\n",
        "#     df_train_filltrain = df_train_filltrain.drop([column, column+'_NArows'], axis=1)\n",
        "    \n",
        "#     #create a copy of df_train[column] for fill train labels\n",
        "#     df_train_filllabel = pd.DataFrame(df_train[column].copy())\n",
        "#     #concatinate with the NArows\n",
        "#     df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "#     #drop rows corresponding to True\n",
        "#     df_train_filllabel = df_train_filllabel[df_train_filllabel[column+'_NArows'] == False]\n",
        "    \n",
        "#     #delete the NArows column\n",
        "#     df_train_filllabel = df_train_filllabel.drop([column+'_NArows'], axis=1)\n",
        "\n",
        "#     #create features df_train for rows needing infill\n",
        "#     #create copy of df_train (note it already has NArows included)\n",
        "#     df_train_fillfeatures = df_train.copy()\n",
        "#     #delete rows coresponding to False\n",
        "#     df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[column+'_NArows'])]\n",
        "#     #delete column and column+'_NArows'\n",
        "#     df_train_fillfeatures = df_train_fillfeatures.drop([column, column+'_NArows'], axis=1)\n",
        "\n",
        "    #create features df_test for rows needing infill\n",
        "    #create copy of df_test (note it already has NArows included)\n",
        "    df_test_fillfeatures = df_test.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[column+'_NArows'])]\n",
        "    #delete column and column+'_NArows'\n",
        "    df_test_fillfeatures = df_test_fillfeatures.drop([column, column+'_NArows'], axis=1)\n",
        "    \n",
        "\n",
        "    #delete NArows from df_test\n",
        "#     df_train = df_train.drop([column+'_NArows'], axis=1)\n",
        "    df_test = df_test.drop([column+'_NArows'], axis=1)\n",
        "\n",
        "\n",
        "  if category == 'text':\n",
        "\n",
        "    #first concatinate the NArows True/False designations to df_test\n",
        "#     df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "    df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "\n",
        "#     #create copy of df_train to serve as training set for fill\n",
        "#     df_train_filltrain = df_train.copy()\n",
        "#     #now delete rows coresponding to True\n",
        "#     df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns.get_values()[0]] == False]\n",
        "    \n",
        "#     #now delete columns = textcolumnslist and the NA labels (orig column+'_NArows') from this df\n",
        "#     df_train_filltrain = df_train_filltrain.drop(textcolumnslist, axis=1)\n",
        "#     df_train_filltrain = df_train_filltrain.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "#     #create a copy of df_train[textcolumnslist] for fill train labels\n",
        "#     df_train_filllabel = df_train[textcolumnslist].copy()\n",
        "#     #concatinate with the NArows\n",
        "#     df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "#     #drop rows corresponding to True\n",
        "#     df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns.get_values()[0]] == False]\n",
        "    \n",
        "#     #delete the NArows column\n",
        "#     df_train_filllabel = df_train_filllabel.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "#     #create features df_train for rows needing infill\n",
        "#     #create copy of df_train (note it already has NArows included)\n",
        "#     df_train_fillfeatures = df_train.copy()\n",
        "#     #delete rows coresponding to False\n",
        "#     df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns.get_values()[0]])]\n",
        "#     #delete textcolumnslist and column+'_NArows'\n",
        "#     df_train_fillfeatures = df_train_fillfeatures.drop(textcolumnslist, axis=1)\n",
        "#     df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "    \n",
        "\n",
        "  \n",
        "    \n",
        "    #create features df_test for rows needing infill\n",
        "    #create copy of df_test (note it already has NArows included)\n",
        "    df_test_fillfeatures = df_test.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns.get_values()[0]])]\n",
        "    #delete column and column+'_NArows'\n",
        "    df_test_fillfeatures = df_test_fillfeatures.drop(textcolumnslist, axis=1)\n",
        "    df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "    #delete NArows from df_test\n",
        "#     df_train = df_train.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "    df_test = df_test.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "  if category == 'date':\n",
        "\n",
        "    #create empty sets for now\n",
        "    #an extension of this method would be to implement a comparable method \\\n",
        "    #for the time category, based on the columns output from the preprocessing\n",
        "#     df_train_filltrain = pd.DataFrame({'foo' : []}) \n",
        "#     df_train_filllabel = pd.DataFrame({'foo' : []})\n",
        "#     df_train_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "    df_test_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "\n",
        "\n",
        "  return df_test_fillfeatures\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#predictpostinfill(category, model, df_test_fillfeatures, \\\n",
        "#textcolumnslist = []), function that takes as input \\\n",
        "#a category string, a model trained as part of automunge on the coresponding \\\n",
        "#column from the train set, the output of createpostMLinfillsets(.), a seed \\\n",
        "#for randomness, and a list of columns \\\n",
        "#produced by a text class preprocessor when applicable and returns \\\n",
        "#predicted infills for the test feature sets as df_testinfill based on \\\n",
        "#derivations using scikit-learn, with the lenth of \\\n",
        "#infill consistent with the number of True values from NArows\n",
        "\n",
        "\n",
        "#imports for numerical class training\n",
        "#from sklearn.linear_model import LinearRegression\n",
        "#from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "#from sklearn.linear_model import Ridge\n",
        "#from sklearn.linear_model import RidgeCV\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "#imports for binary and text class training\n",
        "from sklearn import preprocessing\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "def predictpostinfill(category, model, df_test_fillfeatures, \\\n",
        "                      textcolumnslist = []):\n",
        "  \n",
        "  \n",
        "  #a reasonable extension of this funciton would be to allow ML inference with \\\n",
        "  #other ML architectures such a SVM or something SGD based for instance\n",
        "  \n",
        "  #convert dataframes to numpy arrays\n",
        "#   np_train_filltrain = df_train_filltrain.values\n",
        "#   np_train_filllabel = df_train_filllabel.values\n",
        "#   np_train_fillfeatures = df_train_fillfeatures.values\n",
        "  np_test_fillfeatures = df_test_fillfeatures.values\n",
        "  \n",
        "  #ony run the following if we have any rows needing infill\n",
        "#   if df_train_fillfeatures.shape[0] > 0:\n",
        "  #since we don't have df_train_fillfeatures to work with we'll look at the \n",
        "  #model which will be set to False if there was no infill model trained\n",
        "  #if model[0] != False:\n",
        "  if model != False:\n",
        "\n",
        "    if category == 'nmbr':\n",
        "\n",
        "#       #train linear regression model using scikit-learn for numerical prediction\n",
        "#       #model = LinearRegression()\n",
        "#       #model = PassiveAggressiveRegressor(random_state = randomseed)\n",
        "#       #model = Ridge(random_state = randomseed)\n",
        "#       #model = RidgeCV()\n",
        "#       #note that SVR doesn't have an argument for random_state\n",
        "#       model = SVR()\n",
        "#       model.fit(np_train_filltrain, np_train_filllabel)    \n",
        "      \n",
        "      \n",
        "#       #predict infill values\n",
        "#       np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "#       df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "#       print('category is nmbr, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "  \n",
        "    if category == 'bnry':\n",
        "\n",
        "#       #train logistic regression model using scikit-learn for binary classifier\n",
        "#       #model = LogisticRegression()\n",
        "#       #model = LogisticRegression(random_state = randomseed)\n",
        "#       #model = SGDClassifier(random_state = randomseed)\n",
        "#       model = SVC(random_state = randomseed)\n",
        "      \n",
        "#       model.fit(np_train_filltrain, np_train_filllabel)\n",
        "\n",
        "#       #predict infill values\n",
        "#       np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "#       df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "#       print('category is bnry, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'text':\n",
        "\n",
        "#       #first convert the one-hot encoded set via argmax to a 1D array\n",
        "#       np_train_filllabel_argmax = np.argmax(np_train_filllabel, axis=1)\n",
        "\n",
        "#       #train logistic regression model using scikit-learn for binary classifier\n",
        "#       #with multi_class argument activated\n",
        "#       #model = LogisticRegression()\n",
        "#       #model = SGDClassifier(random_state = randomseed)\n",
        "#       model = SVC(random_state = randomseed)\n",
        "      \n",
        "#       model.fit(np_train_filltrain, np_train_filllabel_argmax)\n",
        "\n",
        "#       #predict infill values\n",
        "#       np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        #this needs to have same number of columns as text category\n",
        "        np_testinfill = np.zeros(shape=(1,len(textcolumnslist)))\n",
        "\n",
        "      #convert the 1D arrary back to one hot encoding\n",
        "#       labelbinarizertrain = preprocessing.LabelBinarizer()\n",
        "#       labelbinarizertrain.fit(np_traininfill)\n",
        "#       np_traininfill = labelbinarizertrain.transform(np_traininfill)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        labelbinarizertest = preprocessing.LabelBinarizer()\n",
        "        labelbinarizertest.fit(np_testinfill)\n",
        "        np_testinfill = labelbinarizertest.transform(np_testinfill)\n",
        "\n",
        "\n",
        "\n",
        "      #run function to ensure correct dimensions of re-encoded classifier array\n",
        "#       np_traininfill = labelbinarizercorrect(np_traininfill, textcolumnslist)\n",
        "      \n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = labelbinarizercorrect(np_testinfill, textcolumnslist)\n",
        "\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "#       df_traininfill = pd.DataFrame(np_traininfill, columns = [textcolumnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [textcolumnslist]) \n",
        "\n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'date':\n",
        "\n",
        "      #create empty sets for now\n",
        "      #an extension of this method would be to implement a comparable infill \\\n",
        "      #method for the time category, based on the columns output from the \\\n",
        "      #preprocessing\n",
        "#       df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "      \n",
        "#       model = False\n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "  \n",
        "  \n",
        "  #else if we didn't have any infill rows let's create some plug values\n",
        "  else:\n",
        "    \n",
        "    if category == 'text':\n",
        "#       np_traininfill = np.zeros(shape=(1,len(textcolumnslist)))\n",
        "      np_testinfill = np.zeros(shape=(1,len(textcolumnslist)))\n",
        "#       df_traininfill = pd.DataFrame(np_traininfill, columns = [textcolumnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [textcolumnslist]) \n",
        "    \n",
        "    else :\n",
        "#       df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "  \n",
        "#     model = False\n",
        "  \n",
        "  return df_testinfill\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6_t5epfWLXwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 8) insert postmunge function for subsequent train data"
      ]
    },
    {
      "metadata": {
        "id": "hNjRgoEuLf5G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#postmunge(df_test, testID_column, postprocess_dict) Function that when fed a \\\n",
        "#test data set coresponding to a previously processed train data set which was \\\n",
        "#processed using the automunge function automates the process \\\n",
        "#of evaluating each column for determination and applicaiton of appropriate \\\n",
        "#preprocessing. Takes as arguement pandas dataframes of test data \\\n",
        "#(mdf_test), a string identifying the ID column for test (testID_column), a \\\n",
        "#dictionary containing keys for the processing which had been generated by the \\\n",
        "#original processing of the coresponding train set using automunge function. \\\n",
        "#Returns following sets as numpy arrays: \n",
        "#test, testID, labelsencoding_dict, finalcolumns_test\n",
        "\n",
        "#Requires consistent column naming and order as original train set pre \\\n",
        "#application of automunge. Requires postprocess_dict from original applicaiton. \\\n",
        "#Currently assumes coinbsistent columns carved out from application of munging \\\n",
        "#from original automunge, a potential future extension is to allow for additional \\\n",
        "#columns to be excluded from processing.\n",
        "\n",
        "\n",
        "\n",
        "def postmunge(postprocess_dict, df_test, testID_column = False):\n",
        "  \n",
        "  \n",
        "  #my understanding is it is good practice to convert any None values into NaN \\\n",
        "  #so I'll just get that out of the way\n",
        "  df_test.fillna(value=float('nan'), inplace=True)\n",
        "  \n",
        "  #extract the ID columns from test set\n",
        "  if testID_column != False:\n",
        "    df_testID = pd.DataFrame(df_test[testID_column])\n",
        "    del df_test[testID_column]\n",
        "  \n",
        "  #confirm consistency of train an test sets\n",
        "  \n",
        "  #check number of columns is consistent\n",
        "  if len(postprocess_dict['origtraincolumns'])!= df_test.shape[1]:\n",
        "    print(\"error, different number of original columns in train and test sets\")\n",
        "    return\n",
        "  \n",
        "  #check column headers are consistent (this works independent of order)\n",
        "  columns_train_set = set(postprocess_dict['origtraincolumns'])\n",
        "  columns_test_set = set(list(df_test))\n",
        "  if columns_train_set != columns_test_set:\n",
        "    print(\"error, different column labels in the train and test set\")\n",
        "    return\n",
        "  \n",
        "  #check order of column headers are consistent\n",
        "  columns_train = postprocess_dict['origtraincolumns']\n",
        "  columns_test = list(df_test)\n",
        "  if columns_train != columns_test:\n",
        "    print(\"error, different order of column labels in the train and test set\")\n",
        "    return\n",
        "  \n",
        "  #create an empty dataframe to serve as a store for each column's NArows\n",
        "  #the column id's for this df will follow convention from NArows of \n",
        "  #column+'_NArows' for each column in columns_train\n",
        "  #these are used in the ML infill methods\n",
        "  #masterNArows_train = pd.DataFrame()\n",
        "  masterNArows_test = pd.DataFrame()\n",
        "\n",
        "    \n",
        "  #For each column, determine appropriate processing function\n",
        "  #processing function will be based on evaluation of train set\n",
        "  for column in columns_train:\n",
        "    \n",
        "    #we're only going to process columns that weren't in our excluded set\n",
        "    #note a foreseeable workflow would be for there to be additional\\\n",
        "    #columns desired for exclusion in post processing, consider adding\\\n",
        "    #asdditional excluded columns as future extensionl\n",
        "    if column not in postprocess_dict['excludetransformscolumns']:\n",
        "\n",
        "      \n",
        "      category = evalcategory(df_test, column)\n",
        "\n",
        "      \n",
        "      #ok postprocess_dict stores column data by the key of column names after\\\n",
        "      #they have gone through our pre-processing functions, which means the \\\n",
        "      #column will match for categories of 'nmbr' and 'bnry', but for the \\\n",
        "      #categories of 'date' and 'text' the act of processing will have \\\n",
        "      #created new columns and deleted the original column - so since we are \\\n",
        "      #currently walking through the original column names we'll need to \\\n",
        "      #pull a post-process column name for the associated columns to serve as \\\n",
        "      #a key for our postprocess_dict which we'll call columnkey. Also the  \\\n",
        "      #original category from train set (traincategory) will be accessed to \\\n",
        "      #serve as a check for consistency between train and test sets.\n",
        "      for postprocesscolumn in postprocess_dict['finalcolumns_train']:\n",
        "        \n",
        "        traincategory = False\n",
        "        \n",
        "        if postprocesscolumn in postprocess_dict['text_dict']:\n",
        "          if column == postprocess_dict[postprocesscolumn]['origcolumn']:\n",
        "            traincategory = 'text'\n",
        "            columnkey = postprocesscolumn\n",
        "            break\n",
        "\n",
        "        elif postprocesscolumn in postprocess_dict['date_dict']:\n",
        "          if column == postprocess_dict[postprocesscolumn]['origcolumn']:\n",
        "            traincategory = 'date'\n",
        "            columnkey = postprocesscolumn\n",
        "            break\n",
        "        \n",
        "        elif postprocesscolumn == column:\n",
        "          \n",
        "          if postprocess_dict[postprocesscolumn]['category'] == 'bnry':\n",
        "            traincategory = 'bnry'\n",
        "            columnkey = column\n",
        "            break\n",
        "        \n",
        "          elif postprocess_dict[postprocesscolumn]['category'] == 'nmbr':\n",
        "            traincategory = 'nmbr'\n",
        "            columnkey = column\n",
        "            break\n",
        "\n",
        "          elif traincategory == False:\n",
        "            traincategory = 'null'\n",
        "            break\n",
        "          \n",
        "      \n",
        "      #let's make sure the category is consistent between train and test sets\n",
        "      if category != traincategory:\n",
        "        print('error - different category between train and test sets for column ',\\\n",
        "              column)\n",
        "      \n",
        "      \n",
        "      #here we'll delete any columns that returned a 'null' category\n",
        "      if category == 'null':\n",
        "        df_test = df_test.drop([column], axis=1)\n",
        "        \n",
        "      #so if we didn't delete the column let's proceed\n",
        "      else:\n",
        "        \n",
        "        #create NArows (column of True/False where True coresponds to missing data)\n",
        "        testNArows = NArows(df_test, column, category)\n",
        "        \n",
        "        #now append that NArows onto a master NA rows df\n",
        "        masterNArows_test = pd.concat([masterNArows_test, testNArows], axis=1)\n",
        "\n",
        "        #(now normalize as would normally)\n",
        "\n",
        "        #for binary class use the train majority field for missing plug value\n",
        "        if category == 'bnry':\n",
        "          binary_missing_plug = postprocess_dict[columnkey]['missing']\n",
        "          \n",
        "        #apply appropriate processing function to this column based on the result\n",
        "        #original bnry processing function still works since only had one df input\n",
        "        if category == 'bnry':\n",
        "          df_test = process_binary_class(df_test, column, binary_missing_plug)\n",
        "          \n",
        "        #for nmbr category process test set with new function postprocess_numerical_class\n",
        "        if category == 'nmbr':\n",
        "          df_test = postprocess_numerical_class(df_test, column, \\\n",
        "                                                postprocess_dict[columnkey]['mean'], \\\n",
        "                                                postprocess_dict[columnkey]['std'])\n",
        "        \n",
        "        #for text category process test set with new function postprocess_text_class\n",
        "        if category == 'text':\n",
        "          df_test = postprocess_text_class(df_test, column, postprocess_dict[columnkey]['textcolumnsarray'])\n",
        "        \n",
        "        #for date category process test set with new function postprocess_date_class\n",
        "        if category == 'date':\n",
        "          df_test = postprocess_time_class(df_test, column, \\\n",
        "                                           postprocess_dict[columnkey]['datecolumnsarray'], \\\n",
        "                                           postprocess_dict[columnkey]['timenormalization_dict'])\n",
        "          \n",
        "        \n",
        "  #now that we've pre-processed all of the columns, let's run through them again\\\n",
        "  #using ML to derive plug values for the previously missing cells\n",
        "  \n",
        "  #if MLinfill == True\n",
        "  if postprocess_dict['MLinfill'] == True:\n",
        "    \n",
        "    \n",
        "    #now let's create a list of columns just like we did in automunge\n",
        "    columns_test_ML = list(df_test)\n",
        "    \n",
        "    iteration = 0\n",
        "    #while iteration < infilliterate:\n",
        "    while iteration < postprocess_dict['infilliterate']:\n",
        "      \n",
        "          \n",
        "      #since we're reusing the text_dict and date_dict from our original automunge\n",
        "      #we're going to need to re-initialize the infillcomplete markers\n",
        "      #actually come to this of it we need to go back to automunge and do this\n",
        "      #for the MLinfill iterations as well\n",
        "      \n",
        "      #re-initialize the infillcomplete marker in text_dict and date_dict\n",
        "      for key in postprocess_dict['text_dict']:\n",
        "        postprocess_dict['text_dict'][key]['infillcomplete'] = False\n",
        "      for key in postprocess_dict['date_dict']:\n",
        "        postprocess_dict['date_dict'][key]['infillcomplete'] = False\n",
        "      \n",
        "      \n",
        "      for column in columns_test_ML:\n",
        "        \n",
        "        #we're only going to process columns that weren't in our excluded set\n",
        "        #if column not in excludetransformscolumns:\n",
        "        if column not in postprocess_dict['excludetransformscolumns']:\n",
        "          \n",
        "          #If column id is found in the text_dict then will require different \\\n",
        "          #type of address since this category won't be found in our \\\n",
        "          #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "          #collective group of columns from the associated textcolumns array.\n",
        "          \n",
        "          #if column in text_dict:\n",
        "          if column in postprocess_dict['text_dict']:\n",
        "            \n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            if postprocess_dict['text_dict'][column]['infillcomplete'] == False:\n",
        "              \n",
        "              #pull this column's textcolumns array\n",
        "              textcolumns = \\\n",
        "              postprocess_dict['text_dict'][column]['textcolumnsarray']\n",
        "              \n",
        "              category = 'text'\n",
        "              \n",
        "              #now let's apply our functions for ML infill\n",
        "              \n",
        "              \n",
        "              df_test_fillfeatures = \\\n",
        "              createpostMLinfillsets(df_test, column, pd.DataFrame(masterNArows_test[postprocess_dict['text_dict'][column]['origcolumn']+'_NArows']), \\\n",
        "                                     category, textcolumnslist = postprocess_dict[column]['textcolumnsarray'])\n",
        "              \n",
        "              #predict infill values using defined function predictpostinfill(.)\n",
        "              df_testinfill = \\\n",
        "              predictpostinfill(category, postprocess_dict[column]['infillmodel'], \\\n",
        "                                df_test_fillfeatures, textcolumnslist = postprocess_dict[column]['textcolumnsarray'])\n",
        "              \n",
        "              #it's a quirk of the ML models that if we don't train the\n",
        "              #train set model on any features, that we won't be able to apply\n",
        "              #the model to predict the test set infill. \n",
        "              #For now we'll only use insertilnfill if we had\n",
        "              #some missing points in the train set, a future extension would be\n",
        "              #to update our createMLinfillsets and predictinfill to create \n",
        "              #some arbitrary features to train the infill on train set for\n",
        "              #cases where there are NaN values in test set but not in train\n",
        "              #such that we could insert infill for missing values in test set.              \n",
        "              \n",
        "              #if model != False:\n",
        "              if postprocess_dict[column]['infillmodel'] != False:\n",
        "                \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[postprocess_dict['text_dict'][column]['origcolumn']+'_NArows']), \\\n",
        "                                       textcolumnslist = postprocess_dict[column]['textcolumnsarray'])\n",
        "              \n",
        "                                                    \n",
        "                \n",
        "          #If column id is found in the date_dict then will require different \\\n",
        "          #type of address since this category won't be found in our \\\n",
        "          #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "          #collective group of columns from the associated datecolumns array. \\\n",
        "          #The development of this address for date columns is a future extension.\n",
        "          elif column in postprocess_dict['date_dict']:\n",
        "\n",
        "            #this section to be a future extension.\n",
        "            pass\n",
        "          \n",
        "          else:\n",
        "            #this is for columns that weren't found in the text_dict or date_dict\n",
        "            #For each column, determine appropriate processing function\n",
        "            #processing function will be based on evaluation of train set\n",
        "            \n",
        "            #pull category from dictionary\n",
        "            category = postprocess_dict['mastercategory_dict'][column+'cat']\n",
        "            \n",
        "            if category == 'bnry':\n",
        "              \n",
        "              #create MLinfill set using defined function createpostMLinfillsets(.)\n",
        "              df_test_fillfeatures = \\\n",
        "              createpostMLinfillsets(df_test, column, pd.DataFrame(masterNArows_test[column + '_NArows']), \\\n",
        "                                     category, textcolumnslist = [])\n",
        "              \n",
        "              #predict infill values using defined function predictpostinfill(.)\n",
        "              df_testinfill = \\\n",
        "              predictpostinfill(category, postprocess_dict[column]['infillmodel'], \\\n",
        "                                df_test_fillfeatures, textcolumnslist = [])\n",
        "              \n",
        "              #(only insert infill to test set if we had NA rows in train set)\n",
        "              #note comments above in text class for potential future expansion\n",
        "              if postprocess_dict[column]['infillmodel'] != False:\n",
        "                \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[column + '_NArows']), \\\n",
        "                                       textcolumnslist = [])\n",
        "              \n",
        "            if category == 'nmbr':\n",
        "              \n",
        "              #create MLinfill set using defined function createpostMLinfillsets(.)\n",
        "              df_test_fillfeatures = \\\n",
        "              createpostMLinfillsets(df_test, column, pd.DataFrame(masterNArows_test[column + '_NArows']), \\\n",
        "                                     category, textcolumnslist = [])\n",
        "              \n",
        "              #predict infill values using defined function predictpostinfill(.)\n",
        "              df_testinfill = \\\n",
        "              predictpostinfill(category, postprocess_dict[column]['infillmodel'], \\\n",
        "                                df_test_fillfeatures, textcolumnslist = [])\n",
        "              \n",
        "              #(only insert infill to test set if we had NA rows in train set)\n",
        "              #note comments above in text class for potential future expansion\n",
        "              if postprocess_dict[column]['infillmodel'] != False:\n",
        "                \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[column + '_NArows']), \\\n",
        "                                       textcolumnslist = [])\n",
        "          \n",
        "      iteration += 1          \n",
        "\n",
        "  #here's a list of final column names saving here since the translation to \\\n",
        "  #numpy arrays scrubs the column names\n",
        "  finalcolumns_test = list(df_test)\n",
        "  \n",
        "  #global processing to test set including conversion to numpy array\n",
        "  np_test = df_test.values\n",
        "  \n",
        "  if testID_column != False:\n",
        "    np_testID = df_testID.values\n",
        "    testID = np_testID\n",
        "  else:\n",
        "    testID = []\n",
        "    \n",
        "  test = np_test\n",
        "  \n",
        "  labelsencoding_dict = postprocess_dict['labelsencoding_dict']\n",
        "  \n",
        "  \n",
        "  return test, testID, labelsencoding_dict, finalcolumns_test\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyThHKNQZOar",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9) Test our functions"
      ]
    },
    {
      "metadata": {
        "id": "ReCBHoBAY9wr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#create sample test and train data for demonstration purposes\n",
        "\n",
        "#train data set from list of dictionaries\n",
        "#24 rows\n",
        "train = [{'ID' : 101, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 102, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 103, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 104, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 105, 'number': -1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 106, 'number': 'Q', 'Y-N': 'N', 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "         {'ID' : 107, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 108, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 109, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 110, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 111, 'number': -1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 112, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "         {'ID' : 113, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 114, 'number': 2, 'Y-N': None, 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 115, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 116, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 117, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 118, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'}]\n",
        "\n",
        "#convert train data to pandas dataframe\n",
        "df_train = pd.DataFrame(train)\n",
        "\n",
        "\n",
        "#test data set from list of dictionaries\n",
        "#21 rows\n",
        "test = [{'ID' : 1, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID': 2, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 3,'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 4, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 5, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 6, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 7, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 8, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 9, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 10, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 11, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 12, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 13, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 14, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 15, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 16, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 17, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 18, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 19, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 20, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 21, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'}]\n",
        "\n",
        "#convert test data to pandas dataframe\n",
        "df_test = pd.DataFrame(test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bq4i5jCrZbu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4129a135-80e6-48e2-f276-9e7bce43ec12"
      },
      "cell_type": "code",
      "source": [
        "#apply automunge\n",
        "#this application is primarily to serve as a quick check for bugs prior to more\\\n",
        "#computationally expensive applicaitons in cases of some update to automunge\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(df_train, df_test, labels_column = 'label', trainID_column = 'ID', \\\n",
        "         testID_column = 'ID', MLinfill = True, infilliterate=1, \\\n",
        "         randomseed = 42, excludetransformscolumns = [])\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  0.2844838080000045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "x0rxR8KOAe2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "a144ab1b-5364-4847-aeed-a251611191d2"
      },
      "cell_type": "code",
      "source": [
        "#validate the output\n",
        "print('train shape =            ', train.shape)\n",
        "print('trainID shape =          ', trainID.shape)\n",
        "print('labels shape =           ', labels.shape)\n",
        "print('validation shape =       ', validation.shape)\n",
        "print('validationID shape =     ', validationID.shape)\n",
        "print('validationlabels shape = ', validationlabels.shape)\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_train = ')\n",
        "print(finalcolumns_train)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape =             (14, 12)\n",
            "trainID shape =           (14, 1)\n",
            "labels shape =            (14, 1)\n",
            "validation shape =        (4, 12)\n",
            "validationID shape =      (4, 1)\n",
            "validationlabels shape =  (4, 1)\n",
            "test shape =              (21, 12)\n",
            "testID shape =            (21, 1)\n",
            "labelsencoding_dict = \n",
            "{'bnry': {0: 'cat', 1: 'dog'}}\n",
            "finalcolumns_train = \n",
            "['shape__missing', 'shape_circle', 'shape_oval', 'shape_square', 'Y-N', 'number', 'date_year', 'date_month', 'date_day', 'date_hour', 'date_minute', 'date_second']\n",
            "finalcolumns_test = \n",
            "['shape__missing', 'shape_circle', 'shape_oval', 'shape_square', 'Y-N', 'number', 'date_year', 'date_month', 'date_day', 'date_hour', 'date_minute', 'date_second']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HeW93eKCBAX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#demonstrate download of postprocess_dict\n",
        "\n",
        "#save postprocess_dict as pickle object\n",
        "with open('postprocess_dict.pickle', 'wb') as handle:\n",
        "    pickle.dump(postprocess_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "#download to local drive\n",
        "#(code specific to Colaboratory)\n",
        "files.download('postprocess_dict.pickle')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_fA6VIfKEVTz",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "cb2ce756-92e7-4a14-8453-e396a3d15050"
      },
      "cell_type": "code",
      "source": [
        "#demonstrate upload of postprocess_dict\n",
        "\n",
        "#upload postprocess_dict from local drive\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-51060f8a-2a44-4b82-9e8c-daf860b1b4ef\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-51060f8a-2a44-4b82-9e8c-daf860b1b4ef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving postprocess_dict.pickle to postprocess_dict (1).pickle\n",
            "User uploaded file \"postprocess_dict.pickle\" with length 6564 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KRtb8fWwEWAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#acces uploaded file with pickle\n",
        "#(code specific to Colaboratory)\n",
        "with open('postprocess_dict.pickle', 'rb') as handle:\n",
        "    postprocess_dict_upload = pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7bXlJzG-qsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test data set from list of dictionaries\n",
        "#21 rows\n",
        "test = [{'ID' : 1, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID': 2, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 3,'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 4, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 5, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 6, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 7, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 8, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 9, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 10, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 11, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 12, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 13, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 14, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 15, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 16, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 17, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 18, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 19, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 20, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 21, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'}]\n",
        "\n",
        "#convert test data to pandas dataframe\n",
        "df_test = pd.DataFrame(test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YOlPUvVr9O2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dd0d9da-a664-4ed6-b5ac-14b3ba0bb7ad"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "#this application is primarily to serve as a quick check for bugs prior to more\\\n",
        "#computationally expensive applicaitons in cases of some update to automunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, df_test, testID_column = 'ID')\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, df_test, testID_column = 'ID')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  0.1526696359997004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VXBtBy8qAk4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e94dce93-e260-4d06-e5ed-ca2159ed8705"
      },
      "cell_type": "code",
      "source": [
        "#validate the output\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test shape =              (21, 12)\n",
            "testID shape =            (21, 1)\n",
            "labelsencoding_dict = \n",
            "{'bnry': {0: 'cat', 1: 'dog'}}\n",
            "finalcolumns_test = \n",
            "['shape__missing', 'shape_circle', 'shape_oval', 'shape_square', 'Y-N', 'number', 'date_year', 'date_month', 'date_day', 'date_hour', 'date_minute', 'date_second']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sRK8M5saCYga",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "leMtlYjEro_2",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "34c3407a-6be7-4872-93ec-6be81e4fab67"
      },
      "cell_type": "code",
      "source": [
        "#Now let's try a larger dataset, the Titanic dataset from Kaggle\n",
        "#available here: https://www.kaggle.com/c/titanic/data\n",
        "#(which I will upload form my local hard drive)\n",
        "#for more on data imports in Colaboratory see my medium post \n",
        "#https://medium.com/@_NicT_/colaboratorys-free-gpu-72ebc9272933\n",
        "#Following is as presented in the Colaboratory tutorial notebook\n",
        "#Once run this will allow you to manually select the path on local drive for file you wish to upload\n",
        "\n",
        "#import titanic train data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4c2c34c8-d3de-4796-8129-0157b5f00050\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4c2c34c8-d3de-4796-8129-0157b5f00050\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.csv to train.csv\n",
            "User uploaded file \"train.csv\" with length 60302 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "frC9fYhFsPzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "titanic_train_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "#titanic_train_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "btRfHQ68seZv",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "ea07ce96-0523-4c86-9015-f8063cd1c91f"
      },
      "cell_type": "code",
      "source": [
        "#import titanic test data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9cc70991-475d-46f6-8fc1-a5c3e4410103\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9cc70991-475d-46f6-8fc1-a5c3e4410103\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n",
            "User uploaded file \"test.csv\" with length 28210 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wkXAzqELsiEw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "titanic_test_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "#titanic_test_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHurufIAsrS2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initialize data\n",
        "titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()\n",
        "\n",
        "\n",
        "#Now there are certain aspects of feature engineering that our automunge won't address\n",
        "#for example one could extract from the Mrs/Ms/Miss designation in the Name \\\n",
        "#column if a female is married. From Cabin field perhaps we could infer what \\\n",
        "#deck passenger was on or whether they even had a cabin. This type of evaluation \\\n",
        "#would need to be done prior to applicaiton of automunge. Because each column is \\\n",
        "#unique there won't be any learning for Cabin, Name, or Ticket I expect so we'll \\\n",
        "#go ahead and delete those rows for our demonstration. It is certainly \\\n",
        "#feasible that there is some feature buried in these columns that can be \\\n",
        "#extracted prior to applicaiton of automunge. PassengerId will serve as ID column.\n",
        "\n",
        "titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcWlP3Q7ucam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4df9e6bb-92cc-44d5-8ea0-a201beb55c33"
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did, first we'll try \\\n",
        "#without the MLinfill:\n",
        "\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(titanic_train_df, titanic_test_df, labels_column = 'Survived', \\\n",
        "          trainID_column = 'PassengerId', testID_column = 'PassengerId', \\\n",
        "          MLinfill = False)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  1.1411580809999577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VZk284HfDQj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d1dea772-e457-42a7-a034-abc039368a29"
      },
      "cell_type": "code",
      "source": [
        "print('train shape =            ', train.shape)\n",
        "print('trainID shape =          ', trainID.shape)\n",
        "print('labels shape =           ', labels.shape)\n",
        "print('validation shape =       ', validation.shape)\n",
        "print('validationID shape =     ', validationID.shape)\n",
        "print('validationlabels shape = ', validationlabels.shape)\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_train = ')\n",
        "print(finalcolumns_train)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape =             (712, 10)\n",
            "trainID shape =           (712, 1)\n",
            "labels shape =            (712, 1)\n",
            "validation shape =        (179, 10)\n",
            "validationID shape =      (179, 1)\n",
            "validationlabels shape =  (179, 1)\n",
            "test shape =              (418, 10)\n",
            "testID shape =            (418, 1)\n",
            "labelsencoding_dict = \n",
            "{'bnry': {0: 0, 1: 1}}\n",
            "finalcolumns_train = \n",
            "['Embarked_C', 'Embarked_Q', 'Embarked_S', 'Embarked__missing', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
            "finalcolumns_test = \n",
            "['Embarked_C', 'Embarked_Q', 'Embarked_S', 'Embarked__missing', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oEL_M8vgOB3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#re-initialize test data\n",
        "#titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()\n",
        "\n",
        "\n",
        "#titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JKlxPyitOL6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b30c9ea-e4d3-461b-db13-eb713297b7a6"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, titanic_test_df, testID_column = 'PassengerId')\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, titanic_test_df, testID_column = 'PassengerId')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  0.3643497919993024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E-XrUZs0WOn7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#re-initialize data\n",
        "titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()\n",
        "\n",
        "titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MurnZAzKK5E9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e3c9c99f-5607-4d18-8b8c-91215363be0a"
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did, now we'll try \\\n",
        "#with the MLinfill included:\n",
        "\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(titanic_train_df, titanic_test_df, labels_column = 'Survived', \\\n",
        "          trainID_column = 'PassengerId', testID_column = 'PassengerId', \\\n",
        "          MLinfill = True)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  1.4845602929999586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ux3y2EcsRAou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#re-initialize test data\n",
        "#titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()\n",
        "\n",
        "\n",
        "#titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mt61ihoARBCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10c4f876-9d74-4b57-e473-96cf4dbfe861"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, titanic_test_df, testID_column = 'PassengerId')\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, titanic_test_df, testID_column = 'PassengerId')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  0.44089024099957896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nd35fyC3cro2",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "8bd24d43-be78-4a7e-ab18-1d64566d07fe"
      },
      "cell_type": "code",
      "source": [
        "#ok let's try with a different set, how about the Kaggel house prices competition\n",
        "#available here: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
        "#(which I will upload form my local hard drive)\n",
        "#for more on data imports in Colaboratory see my medium post \n",
        "#https://medium.com/@_NicT_/colaboratorys-free-gpu-72ebc9272933\n",
        "#Following is as presented in the Colaboratory tutorial notebook\n",
        "#Once run this will allow you to manually select the path on local drive for file you wish to upload\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9d84c734-1220-4ac8-a69b-2322e2f2cca0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9d84c734-1220-4ac8-a69b-2322e2f2cca0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.csv to train (1).csv\n",
            "User uploaded file \"train.csv\" with length 460676 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WlbP-3Sfd4JQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "house_train_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "#house_train_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RgX6wD6eGJa",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c656e92b-b671-4dbc-b382-f98ad5f855bf"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-15bebed4-cea2-4832-b7c8-0322e78f1885\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-15bebed4-cea2-4832-b7c8-0322e78f1885\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test (1).csv\n",
            "User uploaded file \"test.csv\" with length 451405 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZEJgz_qheTYS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "house_test_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "#house_test_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FxPUF2F9gBBu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Ok some bug with the fireplace column so let's just delete that one from our sets\n",
        "#(I believe the evalcategory(.) function is determining different category \\\n",
        "#between train and test sets, fixing this bug is future extension.)\n",
        "house_train_df = house_train_dforig.copy()\n",
        "house_train_df = house_train_df.drop(['FireplaceQu'], axis=1)\n",
        "\n",
        "house_test_df = house_test_dforig.copy()\n",
        "house_test_df = house_test_df.drop(['FireplaceQu'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4OeVv1Efl6Au",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7924e3dc-7743-44e4-bdc5-815e118250ae"
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did, first we'll try \\\n",
        "#without the MLinfill:\n",
        "\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(house_train_df, house_test_df, labels_column = 'SalePrice', \\\n",
        "          trainID_column = 'Id', testID_column = 'Id', MLinfill = False)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  29.615665662999163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RLlA4N7cGhPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "60d7d193-38ad-4c0c-c9b1-43d8006cfdd9"
      },
      "cell_type": "code",
      "source": [
        "print('train shape =            ', train.shape)\n",
        "print('trainID shape =          ', trainID.shape)\n",
        "print('labels shape =           ', labels.shape)\n",
        "print('validation shape =       ', validation.shape)\n",
        "print('validationID shape =     ', validationID.shape)\n",
        "print('validationlabels shape = ', validationlabels.shape)\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_train = ')\n",
        "print(finalcolumns_train)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape =             (1168, 302)\n",
            "trainID shape =           (1168, 1)\n",
            "labels shape =            (1168, 1)\n",
            "validation shape =        (292, 302)\n",
            "validationID shape =      (292, 1)\n",
            "validationlabels shape =  (292, 1)\n",
            "test shape =              (1459, 302)\n",
            "testID shape =            (1459, 1)\n",
            "labelsencoding_dict = \n",
            "{'nmbr': {}}\n",
            "finalcolumns_train = \n",
            "['SaleCondition__missing', 'SaleCondition_Abnorml', 'SaleCondition_AdjLand', 'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal', 'SaleCondition_Partial', 'SaleType__missing', 'SaleType_COD', 'SaleType_CWD', 'SaleType_Con', 'SaleType_ConLD', 'SaleType_ConLI', 'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD', 'PavedDrive__missing', 'PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y', 'GarageCond_Ex', 'GarageCond_Fa', 'GarageCond_Gd', 'GarageCond_Po', 'GarageCond_TA', 'GarageCond__missing', 'GarageQual_Ex', 'GarageQual_Fa', 'GarageQual_Gd', 'GarageQual_Po', 'GarageQual_TA', 'GarageQual__missing', 'GarageFinish_Fin', 'GarageFinish_RFn', 'GarageFinish_Unf', 'GarageFinish__missing', 'GarageType_2Types', 'GarageType_Attchd', 'GarageType_Basment', 'GarageType_BuiltIn', 'GarageType_CarPort', 'GarageType_Detchd', 'GarageType__missing', 'Functional__missing', 'Functional_Maj1', 'Functional_Maj2', 'Functional_Min1', 'Functional_Min2', 'Functional_Mod', 'Functional_Sev', 'Functional_Typ', 'KitchenQual__missing', 'KitchenQual_Ex', 'KitchenQual_Fa', 'KitchenQual_Gd', 'KitchenQual_TA', 'Electrical_FuseA', 'Electrical_FuseF', 'Electrical_FuseP', 'Electrical_Mix', 'Electrical_SBrkr', 'Electrical__missing', 'HeatingQC__missing', 'HeatingQC_Ex', 'HeatingQC_Fa', 'HeatingQC_Gd', 'HeatingQC_Po', 'HeatingQC_TA', 'Heating__missing', 'Heating_Floor', 'Heating_GasA', 'Heating_GasW', 'Heating_Grav', 'Heating_OthW', 'Heating_Wall', 'BsmtFinType2_ALQ', 'BsmtFinType2_BLQ', 'BsmtFinType2_GLQ', 'BsmtFinType2_LwQ', 'BsmtFinType2_Rec', 'BsmtFinType2_Unf', 'BsmtFinType2__missing', 'BsmtFinType1_ALQ', 'BsmtFinType1_BLQ', 'BsmtFinType1_GLQ', 'BsmtFinType1_LwQ', 'BsmtFinType1_Rec', 'BsmtFinType1_Unf', 'BsmtFinType1__missing', 'BsmtExposure_Av', 'BsmtExposure_Gd', 'BsmtExposure_Mn', 'BsmtExposure_No', 'BsmtExposure__missing', 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_Po', 'BsmtCond_TA', 'BsmtCond__missing', 'BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_TA', 'BsmtQual__missing', 'Foundation__missing', 'Foundation_BrkTil', 'Foundation_CBlock', 'Foundation_PConc', 'Foundation_Slab', 'Foundation_Stone', 'Foundation_Wood', 'ExterCond__missing', 'ExterCond_Ex', 'ExterCond_Fa', 'ExterCond_Gd', 'ExterCond_Po', 'ExterCond_TA', 'ExterQual__missing', 'ExterQual_Ex', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA', 'MasVnrType_BrkCmn', 'MasVnrType_BrkFace', 'MasVnrType_None', 'MasVnrType_Stone', 'MasVnrType__missing', 'Exterior2nd__missing', 'Exterior2nd_AsbShng', 'Exterior2nd_AsphShn', 'Exterior2nd_Brk Cmn', 'Exterior2nd_BrkFace', 'Exterior2nd_CBlock', 'Exterior2nd_CmentBd', 'Exterior2nd_HdBoard', 'Exterior2nd_ImStucc', 'Exterior2nd_MetalSd', 'Exterior2nd_Other', 'Exterior2nd_Plywood', 'Exterior2nd_Stone', 'Exterior2nd_Stucco', 'Exterior2nd_VinylSd', 'Exterior2nd_Wd Sdng', 'Exterior2nd_Wd Shng', 'Exterior1st__missing', 'Exterior1st_AsbShng', 'Exterior1st_AsphShn', 'Exterior1st_BrkComm', 'Exterior1st_BrkFace', 'Exterior1st_CBlock', 'Exterior1st_CemntBd', 'Exterior1st_HdBoard', 'Exterior1st_ImStucc', 'Exterior1st_MetalSd', 'Exterior1st_Plywood', 'Exterior1st_Stone', 'Exterior1st_Stucco', 'Exterior1st_VinylSd', 'Exterior1st_Wd Sdng', 'Exterior1st_WdShing', 'RoofMatl__missing', 'RoofMatl_ClyTile', 'RoofMatl_CompShg', 'RoofMatl_Membran', 'RoofMatl_Metal', 'RoofMatl_Roll', 'RoofMatl_Tar&Grv', 'RoofMatl_WdShake', 'RoofMatl_WdShngl', 'RoofStyle__missing', 'RoofStyle_Flat', 'RoofStyle_Gable', 'RoofStyle_Gambrel', 'RoofStyle_Hip', 'RoofStyle_Mansard', 'RoofStyle_Shed', 'HouseStyle__missing', 'HouseStyle_1.5Fin', 'HouseStyle_1.5Unf', 'HouseStyle_1Story', 'HouseStyle_2.5Fin', 'HouseStyle_2.5Unf', 'HouseStyle_2Story', 'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'BldgType__missing', 'BldgType_1Fam', 'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE', 'Condition2__missing', 'Condition2_Artery', 'Condition2_Feedr', 'Condition2_Norm', 'Condition2_PosA', 'Condition2_PosN', 'Condition2_RRAe', 'Condition2_RRAn', 'Condition2_RRNn', 'Condition1__missing', 'Condition1_Artery', 'Condition1_Feedr', 'Condition1_Norm', 'Condition1_PosA', 'Condition1_PosN', 'Condition1_RRAe', 'Condition1_RRAn', 'Condition1_RRNe', 'Condition1_RRNn', 'Neighborhood__missing', 'Neighborhood_Blmngtn', 'Neighborhood_Blueste', 'Neighborhood_BrDale', 'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_CollgCr', 'Neighborhood_Crawfor', 'Neighborhood_Edwards', 'Neighborhood_Gilbert', 'Neighborhood_IDOTRR', 'Neighborhood_MeadowV', 'Neighborhood_Mitchel', 'Neighborhood_NAmes', 'Neighborhood_NPkVill', 'Neighborhood_NWAmes', 'Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_SWISU', 'Neighborhood_Sawyer', 'Neighborhood_SawyerW', 'Neighborhood_Somerst', 'Neighborhood_StoneBr', 'Neighborhood_Timber', 'Neighborhood_Veenker', 'LandSlope__missing', 'LandSlope_Gtl', 'LandSlope_Mod', 'LandSlope_Sev', 'LotConfig__missing', 'LotConfig_Corner', 'LotConfig_CulDSac', 'LotConfig_FR2', 'LotConfig_FR3', 'LotConfig_Inside', 'LandContour__missing', 'LandContour_Bnk', 'LandContour_HLS', 'LandContour_Low', 'LandContour_Lvl', 'LotShape__missing', 'LotShape_IR1', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg', 'MSZoning__missing', 'MSZoning_C (all)', 'MSZoning_FV', 'MSZoning_RH', 'MSZoning_RL', 'MSZoning_RM', 'MSSubClass', 'LotFrontage', 'LotArea', 'Street', 'Utilities', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'CentralAir', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n",
            "finalcolumns_test = \n",
            "['SaleCondition__missing', 'SaleCondition_Abnorml', 'SaleCondition_AdjLand', 'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal', 'SaleCondition_Partial', 'SaleType__missing', 'SaleType_COD', 'SaleType_CWD', 'SaleType_Con', 'SaleType_ConLD', 'SaleType_ConLI', 'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD', 'PavedDrive__missing', 'PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y', 'GarageCond_Ex', 'GarageCond_Fa', 'GarageCond_Gd', 'GarageCond_Po', 'GarageCond_TA', 'GarageCond__missing', 'GarageQual_Ex', 'GarageQual_Fa', 'GarageQual_Gd', 'GarageQual_Po', 'GarageQual_TA', 'GarageQual__missing', 'GarageFinish_Fin', 'GarageFinish_RFn', 'GarageFinish_Unf', 'GarageFinish__missing', 'GarageType_2Types', 'GarageType_Attchd', 'GarageType_Basment', 'GarageType_BuiltIn', 'GarageType_CarPort', 'GarageType_Detchd', 'GarageType__missing', 'Functional__missing', 'Functional_Maj1', 'Functional_Maj2', 'Functional_Min1', 'Functional_Min2', 'Functional_Mod', 'Functional_Sev', 'Functional_Typ', 'KitchenQual__missing', 'KitchenQual_Ex', 'KitchenQual_Fa', 'KitchenQual_Gd', 'KitchenQual_TA', 'Electrical_FuseA', 'Electrical_FuseF', 'Electrical_FuseP', 'Electrical_Mix', 'Electrical_SBrkr', 'Electrical__missing', 'HeatingQC__missing', 'HeatingQC_Ex', 'HeatingQC_Fa', 'HeatingQC_Gd', 'HeatingQC_Po', 'HeatingQC_TA', 'Heating__missing', 'Heating_Floor', 'Heating_GasA', 'Heating_GasW', 'Heating_Grav', 'Heating_OthW', 'Heating_Wall', 'BsmtFinType2_ALQ', 'BsmtFinType2_BLQ', 'BsmtFinType2_GLQ', 'BsmtFinType2_LwQ', 'BsmtFinType2_Rec', 'BsmtFinType2_Unf', 'BsmtFinType2__missing', 'BsmtFinType1_ALQ', 'BsmtFinType1_BLQ', 'BsmtFinType1_GLQ', 'BsmtFinType1_LwQ', 'BsmtFinType1_Rec', 'BsmtFinType1_Unf', 'BsmtFinType1__missing', 'BsmtExposure_Av', 'BsmtExposure_Gd', 'BsmtExposure_Mn', 'BsmtExposure_No', 'BsmtExposure__missing', 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_Po', 'BsmtCond_TA', 'BsmtCond__missing', 'BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_TA', 'BsmtQual__missing', 'Foundation__missing', 'Foundation_BrkTil', 'Foundation_CBlock', 'Foundation_PConc', 'Foundation_Slab', 'Foundation_Stone', 'Foundation_Wood', 'ExterCond__missing', 'ExterCond_Ex', 'ExterCond_Fa', 'ExterCond_Gd', 'ExterCond_Po', 'ExterCond_TA', 'ExterQual__missing', 'ExterQual_Ex', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA', 'MasVnrType_BrkCmn', 'MasVnrType_BrkFace', 'MasVnrType_None', 'MasVnrType_Stone', 'MasVnrType__missing', 'Exterior2nd__missing', 'Exterior2nd_AsbShng', 'Exterior2nd_AsphShn', 'Exterior2nd_Brk Cmn', 'Exterior2nd_BrkFace', 'Exterior2nd_CBlock', 'Exterior2nd_CmentBd', 'Exterior2nd_HdBoard', 'Exterior2nd_ImStucc', 'Exterior2nd_MetalSd', 'Exterior2nd_Other', 'Exterior2nd_Plywood', 'Exterior2nd_Stone', 'Exterior2nd_Stucco', 'Exterior2nd_VinylSd', 'Exterior2nd_Wd Sdng', 'Exterior2nd_Wd Shng', 'Exterior1st__missing', 'Exterior1st_AsbShng', 'Exterior1st_AsphShn', 'Exterior1st_BrkComm', 'Exterior1st_BrkFace', 'Exterior1st_CBlock', 'Exterior1st_CemntBd', 'Exterior1st_HdBoard', 'Exterior1st_ImStucc', 'Exterior1st_MetalSd', 'Exterior1st_Plywood', 'Exterior1st_Stone', 'Exterior1st_Stucco', 'Exterior1st_VinylSd', 'Exterior1st_Wd Sdng', 'Exterior1st_WdShing', 'RoofMatl__missing', 'RoofMatl_ClyTile', 'RoofMatl_CompShg', 'RoofMatl_Membran', 'RoofMatl_Metal', 'RoofMatl_Roll', 'RoofMatl_Tar&Grv', 'RoofMatl_WdShake', 'RoofMatl_WdShngl', 'RoofStyle__missing', 'RoofStyle_Flat', 'RoofStyle_Gable', 'RoofStyle_Gambrel', 'RoofStyle_Hip', 'RoofStyle_Mansard', 'RoofStyle_Shed', 'HouseStyle__missing', 'HouseStyle_1.5Fin', 'HouseStyle_1.5Unf', 'HouseStyle_1Story', 'HouseStyle_2.5Fin', 'HouseStyle_2.5Unf', 'HouseStyle_2Story', 'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'BldgType__missing', 'BldgType_1Fam', 'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE', 'Condition2__missing', 'Condition2_Artery', 'Condition2_Feedr', 'Condition2_Norm', 'Condition2_PosA', 'Condition2_PosN', 'Condition2_RRAe', 'Condition2_RRAn', 'Condition2_RRNn', 'Condition1__missing', 'Condition1_Artery', 'Condition1_Feedr', 'Condition1_Norm', 'Condition1_PosA', 'Condition1_PosN', 'Condition1_RRAe', 'Condition1_RRAn', 'Condition1_RRNe', 'Condition1_RRNn', 'Neighborhood__missing', 'Neighborhood_Blmngtn', 'Neighborhood_Blueste', 'Neighborhood_BrDale', 'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_CollgCr', 'Neighborhood_Crawfor', 'Neighborhood_Edwards', 'Neighborhood_Gilbert', 'Neighborhood_IDOTRR', 'Neighborhood_MeadowV', 'Neighborhood_Mitchel', 'Neighborhood_NAmes', 'Neighborhood_NPkVill', 'Neighborhood_NWAmes', 'Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_SWISU', 'Neighborhood_Sawyer', 'Neighborhood_SawyerW', 'Neighborhood_Somerst', 'Neighborhood_StoneBr', 'Neighborhood_Timber', 'Neighborhood_Veenker', 'LandSlope__missing', 'LandSlope_Gtl', 'LandSlope_Mod', 'LandSlope_Sev', 'LotConfig__missing', 'LotConfig_Corner', 'LotConfig_CulDSac', 'LotConfig_FR2', 'LotConfig_FR3', 'LotConfig_Inside', 'LandContour__missing', 'LandContour_Bnk', 'LandContour_HLS', 'LandContour_Low', 'LandContour_Lvl', 'LotShape__missing', 'LotShape_IR1', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg', 'MSZoning__missing', 'MSZoning_C (all)', 'MSZoning_FV', 'MSZoning_RH', 'MSZoning_RL', 'MSZoning_RM', 'MSSubClass', 'LotFrontage', 'LotArea', 'Street', 'Utilities', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'CentralAir', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TUZCSQxNTChT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#re-initialize the test set\n",
        "# house_train_df = house_train_dforig.copy()\n",
        "# house_train_df = house_train_df.drop(['FireplaceQu'], axis=1)\n",
        "\n",
        "house_test_df = house_test_dforig.copy()\n",
        "house_test_df = house_test_df.drop(['FireplaceQu'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hii-Rfc6TMqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b2886bba-90eb-41ae-9546-068425f5f0cb"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, house_test_df, testID_column = 'Id')\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, house_test_df, testID_column = 'Id')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error - different category between train and test sets for column  Alley\n",
            "error - different category between train and test sets for column  PoolQC\n",
            "error - different category between train and test sets for column  Fence\n",
            "error - different category between train and test sets for column  MiscFeature\n",
            "seconds elapsed =  14.715861022001263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EbuRzVqbT-xy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#go ahead and ignore these error messages, these are the columns that returned '\n",
        "#a category of 'null' from applicaiton of evalcagteghory and will have been dropped\\\n",
        "#I'll have to put some thought in to how to better compare the train and test \\\n",
        "#column categories. (to confirm see look at the validation array shapes below)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QsL6w7kPUdE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "58c4fff1-9d42-4b07-c6cb-d84f3f068c65"
      },
      "cell_type": "code",
      "source": [
        "#validate the output\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test shape =              (1459, 302)\n",
            "testID shape =            (1459, 1)\n",
            "labelsencoding_dict = \n",
            "{'nmbr': {}}\n",
            "finalcolumns_test = \n",
            "['SaleCondition__missing', 'SaleCondition_Abnorml', 'SaleCondition_AdjLand', 'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal', 'SaleCondition_Partial', 'SaleType__missing', 'SaleType_COD', 'SaleType_CWD', 'SaleType_Con', 'SaleType_ConLD', 'SaleType_ConLI', 'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD', 'PavedDrive__missing', 'PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y', 'GarageCond_Ex', 'GarageCond_Fa', 'GarageCond_Gd', 'GarageCond_Po', 'GarageCond_TA', 'GarageCond__missing', 'GarageQual_Ex', 'GarageQual_Fa', 'GarageQual_Gd', 'GarageQual_Po', 'GarageQual_TA', 'GarageQual__missing', 'GarageFinish_Fin', 'GarageFinish_RFn', 'GarageFinish_Unf', 'GarageFinish__missing', 'GarageType_2Types', 'GarageType_Attchd', 'GarageType_Basment', 'GarageType_BuiltIn', 'GarageType_CarPort', 'GarageType_Detchd', 'GarageType__missing', 'Functional__missing', 'Functional_Maj1', 'Functional_Maj2', 'Functional_Min1', 'Functional_Min2', 'Functional_Mod', 'Functional_Sev', 'Functional_Typ', 'KitchenQual__missing', 'KitchenQual_Ex', 'KitchenQual_Fa', 'KitchenQual_Gd', 'KitchenQual_TA', 'Electrical_FuseA', 'Electrical_FuseF', 'Electrical_FuseP', 'Electrical_Mix', 'Electrical_SBrkr', 'Electrical__missing', 'HeatingQC__missing', 'HeatingQC_Ex', 'HeatingQC_Fa', 'HeatingQC_Gd', 'HeatingQC_Po', 'HeatingQC_TA', 'Heating__missing', 'Heating_Floor', 'Heating_GasA', 'Heating_GasW', 'Heating_Grav', 'Heating_OthW', 'Heating_Wall', 'BsmtFinType2_ALQ', 'BsmtFinType2_BLQ', 'BsmtFinType2_GLQ', 'BsmtFinType2_LwQ', 'BsmtFinType2_Rec', 'BsmtFinType2_Unf', 'BsmtFinType2__missing', 'BsmtFinType1_ALQ', 'BsmtFinType1_BLQ', 'BsmtFinType1_GLQ', 'BsmtFinType1_LwQ', 'BsmtFinType1_Rec', 'BsmtFinType1_Unf', 'BsmtFinType1__missing', 'BsmtExposure_Av', 'BsmtExposure_Gd', 'BsmtExposure_Mn', 'BsmtExposure_No', 'BsmtExposure__missing', 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_Po', 'BsmtCond_TA', 'BsmtCond__missing', 'BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_TA', 'BsmtQual__missing', 'Foundation__missing', 'Foundation_BrkTil', 'Foundation_CBlock', 'Foundation_PConc', 'Foundation_Slab', 'Foundation_Stone', 'Foundation_Wood', 'ExterCond__missing', 'ExterCond_Ex', 'ExterCond_Fa', 'ExterCond_Gd', 'ExterCond_Po', 'ExterCond_TA', 'ExterQual__missing', 'ExterQual_Ex', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA', 'MasVnrType_BrkCmn', 'MasVnrType_BrkFace', 'MasVnrType_None', 'MasVnrType_Stone', 'MasVnrType__missing', 'Exterior2nd__missing', 'Exterior2nd_AsbShng', 'Exterior2nd_AsphShn', 'Exterior2nd_Brk Cmn', 'Exterior2nd_BrkFace', 'Exterior2nd_CBlock', 'Exterior2nd_CmentBd', 'Exterior2nd_HdBoard', 'Exterior2nd_ImStucc', 'Exterior2nd_MetalSd', 'Exterior2nd_Other', 'Exterior2nd_Plywood', 'Exterior2nd_Stone', 'Exterior2nd_Stucco', 'Exterior2nd_VinylSd', 'Exterior2nd_Wd Sdng', 'Exterior2nd_Wd Shng', 'Exterior1st__missing', 'Exterior1st_AsbShng', 'Exterior1st_AsphShn', 'Exterior1st_BrkComm', 'Exterior1st_BrkFace', 'Exterior1st_CBlock', 'Exterior1st_CemntBd', 'Exterior1st_HdBoard', 'Exterior1st_ImStucc', 'Exterior1st_MetalSd', 'Exterior1st_Plywood', 'Exterior1st_Stone', 'Exterior1st_Stucco', 'Exterior1st_VinylSd', 'Exterior1st_Wd Sdng', 'Exterior1st_WdShing', 'RoofMatl__missing', 'RoofMatl_ClyTile', 'RoofMatl_CompShg', 'RoofMatl_Membran', 'RoofMatl_Metal', 'RoofMatl_Roll', 'RoofMatl_Tar&Grv', 'RoofMatl_WdShake', 'RoofMatl_WdShngl', 'RoofStyle__missing', 'RoofStyle_Flat', 'RoofStyle_Gable', 'RoofStyle_Gambrel', 'RoofStyle_Hip', 'RoofStyle_Mansard', 'RoofStyle_Shed', 'HouseStyle__missing', 'HouseStyle_1.5Fin', 'HouseStyle_1.5Unf', 'HouseStyle_1Story', 'HouseStyle_2.5Fin', 'HouseStyle_2.5Unf', 'HouseStyle_2Story', 'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'BldgType__missing', 'BldgType_1Fam', 'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE', 'Condition2__missing', 'Condition2_Artery', 'Condition2_Feedr', 'Condition2_Norm', 'Condition2_PosA', 'Condition2_PosN', 'Condition2_RRAe', 'Condition2_RRAn', 'Condition2_RRNn', 'Condition1__missing', 'Condition1_Artery', 'Condition1_Feedr', 'Condition1_Norm', 'Condition1_PosA', 'Condition1_PosN', 'Condition1_RRAe', 'Condition1_RRAn', 'Condition1_RRNe', 'Condition1_RRNn', 'Neighborhood__missing', 'Neighborhood_Blmngtn', 'Neighborhood_Blueste', 'Neighborhood_BrDale', 'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_CollgCr', 'Neighborhood_Crawfor', 'Neighborhood_Edwards', 'Neighborhood_Gilbert', 'Neighborhood_IDOTRR', 'Neighborhood_MeadowV', 'Neighborhood_Mitchel', 'Neighborhood_NAmes', 'Neighborhood_NPkVill', 'Neighborhood_NWAmes', 'Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_SWISU', 'Neighborhood_Sawyer', 'Neighborhood_SawyerW', 'Neighborhood_Somerst', 'Neighborhood_StoneBr', 'Neighborhood_Timber', 'Neighborhood_Veenker', 'LandSlope__missing', 'LandSlope_Gtl', 'LandSlope_Mod', 'LandSlope_Sev', 'LotConfig__missing', 'LotConfig_Corner', 'LotConfig_CulDSac', 'LotConfig_FR2', 'LotConfig_FR3', 'LotConfig_Inside', 'LandContour__missing', 'LandContour_Bnk', 'LandContour_HLS', 'LandContour_Low', 'LandContour_Lvl', 'LotShape__missing', 'LotShape_IR1', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg', 'MSZoning__missing', 'MSZoning_C (all)', 'MSZoning_FV', 'MSZoning_RH', 'MSZoning_RL', 'MSZoning_RM', 'MSSubClass', 'LotFrontage', 'LotArea', 'Street', 'Utilities', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'CentralAir', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1WBJ4qoPc86Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now let's try again with the MLinfill and compare results. I would expect to \\\n",
        "#see hopefully a more pronounced result than the delta from titanic set due \\\n",
        "#to higher frequency of missing data in the set."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Q21WOBgtMq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#reinitialize the data\n",
        "house_train_df = house_train_dforig.copy()\n",
        "house_train_df = house_train_df.drop(['FireplaceQu'], axis=1)\n",
        "\n",
        "house_test_df = house_test_dforig.copy()\n",
        "house_test_df = house_test_df.drop(['FireplaceQu'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_-TqGqJeE5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3d28c2da-8a1f-422b-f4cc-c1107a92b802"
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did with the MLinfill:\n",
        "\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(house_train_df, house_test_df, labels_column = 'SalePrice', \\\n",
        "          trainID_column = 'Id', testID_column = 'Id', MLinfill = True)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:2530: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
            "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  45.67986768699848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3cbnxgUl4HkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#reinitialize the test data\n",
        "# house_train_df = house_train_dforig.copy()\n",
        "# house_train_df = house_train_df.drop(['FireplaceQu'], axis=1)\n",
        "\n",
        "house_test_df = house_test_dforig.copy()\n",
        "house_test_df = house_test_df.drop(['FireplaceQu'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIUXqP_dZ9w0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cfdb96cb-e4f7-40f7-a173-a04827a43f6c"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, house_test_df, testID_column = 'Id')\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, house_test_df, testID_column = 'Id')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error - different category between train and test sets for column  Alley\n",
            "error - different category between train and test sets for column  PoolQC\n",
            "error - different category between train and test sets for column  Fence\n",
            "error - different category between train and test sets for column  MiscFeature\n",
            "seconds elapsed =  20.813620159000493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k_CGXW5sQ9pn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
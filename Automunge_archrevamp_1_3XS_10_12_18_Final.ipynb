{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automunge_archrevamp_1.3XS_10-12-18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "5Jh-t4wgNizK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "list of updates:\n",
        "\n",
        "1.   incorporated ravel(.) into predictinfill(.) to deal with error message\n",
        "2.   updated predictinfill(.) scikit architectures to RandomForest instead of support vector (10/8 7:44pm)\n",
        "3. change all nmbr_dict to bxcx_dict for boxcox process (10/8 8:06pm)\n",
        "4. add features to column_dict for bnry and nmbr (10/8 8:15pm)\n",
        "5. initialize nmbr and bnry dict's, add entries to postprocess_dict (10/8 8:30pm)\n",
        "6. update process functions to output columnslist and update column_dict where this list was #'d out', then update automunge to allow for multicolumn transformations in preprocessing stage (10/8 9:25pm)\n",
        "7. incorporate infillcomplete process for multicolumn transformations to automunge ML infill (10/8 9:48pm)\n",
        "8. incorporate NArows into processing functions for bxcx, nmbr, bnry, date (one at a time -> test ML infill each time, I think there might be some complexity in the create infill sets function - update complexity resolved by using different label for NArw column) (10/9 2:20pm)\n",
        "9. convert binary integers to 8 bit (10/9 2:42pm)\n",
        "10.  initialize multicolumntransform_dict prior to processing (10/11, 10:19 am)\n",
        "11. have process functions output an additional list categorylist (for now blank lists except for text class) (10/11 10:32am)\n",
        "12. update process steps for nmbr and bnry to include for column in columnslist address of column_dict and nmbr_dict, note this leads to bug in createinfillsets need to troubleshoot (10-11 7:26pm)\n",
        "13. update MLinfill functions creatMLinfillsets() and insertinfill()  to generalize between categories to categorical and single column (10-11 3:18pm) (10-11 7:26pm)\n",
        "14. audit column_dict and category dicts and postprocess_dict -> update to generalized format between columns (major architecture update) (10-11 10:30pm)\n",
        "15. combine category dict's into single MLinfill address for this generalization (10-12 10:30am)\n",
        "16. update postprocess functions and postinfill functions (10/12 2:24pm) (10/12 8:30pm)\n",
        "17. update postmunge (10/12 4:11pm) (10/12 8:30pm)\n",
        "\n",
        "complete pending troubleshoot:\n",
        "\n",
        "remaining to address:\n",
        "\n",
        "- labels processing at end of automunge\n",
        "- numerically encoded categorical data\n",
        "- carve out ML infill application from automunge to seperate functions\n",
        "- generalize MLinfill to columns with multiple transforms of different types (bxcx  is good testing ground)\n",
        "- double check createMLinfill sets has no leakage\n",
        "- double check  correct category applied for the application of ML infill\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hQJGsdymZDsD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#for measuring time duration of operations\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "#for file downloads\n",
        "import pickle\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmjR5Ze0U-JC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1) insert preprocess functions from prior notebook. "
      ]
    },
    {
      "metadata": {
        "id": "psx9E_ckUsPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#process_numerical_class(mdf_train, mdf_test, column)\n",
        "#function to normalize data to mean of 0 and standard deviation of 1 from training distribution\n",
        "#takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\\\n",
        "#and the name of the column string ('column') \n",
        "#replaces missing or improperly formatted data with mean of remaining values\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#expect this approach works better when the numerical distribution is thin tailed\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "#imports\n",
        "from pandas import Series\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def process_numerical_class(mdf_train, mdf_test, column):\n",
        "     \n",
        "  \n",
        "  #add a second column with boolean expression indicating a missing cell\n",
        "  #(using NArows(.) function defined below, column name will be column+'_NArows')\n",
        "  NArows_nmbr_train = NArows(mdf_train, column, 'nmbr')\n",
        "  NArows_nmbr_test = NArows(mdf_test, column, 'nmbr')\n",
        "  mdf_train[column + '_NArw'] = NArows_nmbr_train.copy()\n",
        "  mdf_test[column + '_NArw'] = NArows_nmbr_test.copy()\n",
        "  del NArows_nmbr_train\n",
        "  del NArows_nmbr_test\n",
        "  \n",
        "  #change NArows data type to 8-bit (1 byte) integers for memory savings\n",
        "  mdf_train[column + '_NArw'] = mdf_train[column + '_NArw'].astype(np.int8)\n",
        "  mdf_test[column + '_NArw'] = mdf_test[column + '_NArw'].astype(np.int8)\n",
        "  \n",
        "  #convert all values to either numeric or NaN\n",
        "  mdf_train[column] = pd.to_numeric(mdf_train[column], errors='coerce')\n",
        "  mdf_test[column] = pd.to_numeric(mdf_test[column], errors='coerce')\n",
        "\n",
        "  #get mean of training data\n",
        "  mean = mdf_train[column].mean()    \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_train[column] = mdf_train[column].fillna(mean)\n",
        "  mdf_test[column] = mdf_test[column].fillna(mean)\n",
        "\n",
        "  #subtract mean from column for both train and test\n",
        "  mdf_train[column] = mdf_train[column] - mean\n",
        "  mdf_test[column] = mdf_test[column] - mean\n",
        "\n",
        "  #get standard deviation of training data\n",
        "  std = mdf_train[column].std()\n",
        "\n",
        "  #divide column values by std for both training and test data\n",
        "  mdf_train[column] = mdf_train[column] / std\n",
        "  mdf_test[column] = mdf_test[column] / std\n",
        "  \n",
        "  #change column name to column + '_nmbr'\n",
        "  mdf_train[column + '_nmbr'] = mdf_train[column].copy()\n",
        "  mdf_test[column + '_nmbr'] = mdf_test[column].copy()\n",
        "  del mdf_train[column]\n",
        "  del mdf_test[column]\n",
        "  \n",
        "  #create list of columns\n",
        "  nmbrcolumns = [column + '_nmbr', column + '_NArw']\n",
        "  \n",
        "  #create list of columns associated with categorical transform (blank for now)\n",
        "  categorylist = []\n",
        "\n",
        "\n",
        "  return mdf_train, mdf_test, mean, std, nmbrcolumns, categorylist\n",
        "  \n",
        "\n",
        "  \n",
        "#process_binary_class(mdf, column, missing)\n",
        "#converts binary classification values to 0 or 1\n",
        "#takes as arguement a pandas dataframe (mdf), \\\n",
        "#the name of the column string ('column') \\\n",
        "#and the string classification to assign to missing data ('missing')\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#missing category must be identical to one of the two existing categories\n",
        "#returns error message if more than two categories remain\n",
        "\n",
        "\n",
        "def process_binary_class(mdf, column, missing):\n",
        "    \n",
        "  #add a second column with boolean expression indicating a missing cell\n",
        "  #(using NArows(.) function defined below, column name will be column+'_NArows')\n",
        "  NArows_bnry = NArows(mdf, column, 'bnry')\n",
        "  mdf[column + '_NArw'] = NArows_bnry.copy()\n",
        "  del NArows_bnry\n",
        "  \n",
        "  #replace missing data with specified classification\n",
        "  mdf[column] = mdf[column].fillna(missing)\n",
        "\n",
        "  #if more than two remaining classifications, return error message    \n",
        "  if len(mdf[column].unique()) > 2:\n",
        "      print('ERROR: number of categories in column for process_binary_class() call >2')\n",
        "      return mdf\n",
        "\n",
        "  #convert column to binary 0/1 classification\n",
        "  lb = preprocessing.LabelBinarizer()\n",
        "  mdf[column] = lb.fit_transform(mdf[column])\n",
        "\n",
        "  #change column name to column + '_bnry'\n",
        "  mdf[column + '_bnry'] = mdf[column].copy()\n",
        "  del mdf[column]\n",
        "  \n",
        "  #create list of columns\n",
        "  bnrycolumns = [column + '_bnry', column + '_NArw']\n",
        "  \n",
        "  #change data types to 8-bit (1 byte) integers for memory savings\n",
        "  for bnrycolumn in bnrycolumns:\n",
        "    mdf[bnrycolumn] = mdf[bnrycolumn].astype(np.int8)\n",
        "    \n",
        "  #create list of columns associated with categorical transform (blank for now)\n",
        "  categorylist = []\n",
        "  \n",
        "  return mdf, bnrycolumns, categorylist\n",
        "\n",
        "  \n",
        "#process_text_class(mdf_train, mdf_test, column)\n",
        "#preprocess column with text classifications\n",
        "#takes as arguement two pandas dataframe containing training and test data respectively \n",
        "#(mdf_train, mdf_test), and the name of the column string ('column')\n",
        "\n",
        "#note this trains both training and test data simultaneously due to unique treatment if any category\n",
        "#missing from training set but not from test set to ensure consistent formatting \n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with onehot encodings\n",
        "#with columns named after column_ + text classifications\n",
        "#missing data replaced with category label 'missing'+column\n",
        "#any categories missing from the training set removed from test set\n",
        "#any category present in training but missing from test set given a column of zeros for consistent formatting\n",
        "#ensures order of all new columns consistent between both sets\n",
        "#returns two transformed dataframe (mdf_train, mdf_test) \\\n",
        "#and a list of the new column names (textcolumns)\n",
        "\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "#note it is kind of a hack here to create a column for missing values with \\\n",
        "#two underscores (__) in the column name to ensure appropriate order for cases\\\n",
        "#where NaN present in test data but not train data, if a category starts with|\n",
        "#an underscore such that it preceeds '__missing' alphabetically in this scenario\\\n",
        "#this might create error due to different order of columns, address of this \\\n",
        "#potential issue will be a future extension\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def process_text_class(mdf_train, mdf_test, column):\n",
        "\n",
        "  \n",
        "  \n",
        "  #replace NA with a dummy variable\n",
        "  mdf_train[column] = mdf_train[column].fillna('NArw')\n",
        "  mdf_test[column] = mdf_test[column].fillna('NArw')\n",
        "\n",
        "\n",
        "  #extract categories for column labels\n",
        "  #note that .unique() extracts the labels as a numpy array\n",
        "  labels_train = mdf_train[column].unique()\n",
        "  labels_train.sort(axis=0)\n",
        "  labels_test = mdf_test[column].unique()\n",
        "  labels_test.sort(axis=0)\n",
        "\n",
        "  #transform text classifications to numerical id\n",
        "  encoder = LabelEncoder()\n",
        "  cat_train = mdf_train[column]\n",
        "  cat_train_encoded = encoder.fit_transform(cat_train)\n",
        "\n",
        "  cat_test = mdf_test[column]\n",
        "  cat_test_encoded = encoder.fit_transform(cat_test)\n",
        "\n",
        "\n",
        "  #apply onehotencoding\n",
        "  onehotencoder = OneHotEncoder()\n",
        "  cat_train_1hot = onehotencoder.fit_transform(cat_train_encoded.reshape(-1,1))\n",
        "  cat_test_1hot = onehotencoder.fit_transform(cat_test_encoded.reshape(-1,1))\n",
        "\n",
        "  #append column header name to each category listing\n",
        "  #note the iteration is over a numpy array hence the [...] approach  \n",
        "  labels_train[...] = column + '_' + labels_train[...]\n",
        "  labels_test[...] = column + '_' + labels_test[...]\n",
        "\n",
        "\n",
        "  #convert sparse array to pandas dataframe with column labels\n",
        "  df_train_cat = pd.DataFrame(cat_train_1hot.toarray(), columns=labels_train)\n",
        "  df_test_cat = pd.DataFrame(cat_test_1hot.toarray(), columns=labels_test)\n",
        "  \n",
        "  #add a missing column to train if it's not present\n",
        "  if column + '_NArw' not in df_train_cat.columns:\n",
        "    missingcolumn = pd.DataFrame(0, index=np.arange(df_train_cat.shape[0]), columns=[column+'_NArw'])\n",
        "    df_train_cat = pd.concat([missingcolumn, df_train_cat], axis=1)\n",
        "\n",
        "\n",
        "  #Get missing columns in test set that are present in training set\n",
        "  missing_cols = set( df_train_cat.columns ) - set( df_test_cat.columns )\n",
        "  \n",
        "  #Add a missing column in test set with default value equal to 0\n",
        "  for c in missing_cols:\n",
        "      df_test_cat[c] = 0\n",
        "  #Ensure the order of column in the test set is in the same order than in train set\n",
        "  #Note this also removes categories in test set that aren't present in training set\n",
        "  df_test_cat = df_test_cat[df_train_cat.columns]\n",
        "\n",
        "\n",
        "  #concatinate the sparse set with the rest of our training data\n",
        "  mdf_train = pd.concat([df_train_cat, mdf_train], axis=1)\n",
        "  mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)\n",
        "\n",
        "\n",
        "  #delete original column from training data\n",
        "  del mdf_train[column]    \n",
        "  del mdf_test[column]\n",
        "  \n",
        "  #create output of a list of the created column names\n",
        "  labels_train = list(df_train_cat)\n",
        "  textcolumns = labels_train\n",
        "  \n",
        "  #change data types to 8-bit (1 byte) integers for memory savings\n",
        "  for textcolumn in textcolumns:\n",
        "    mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)\n",
        "    mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)\n",
        "  \n",
        "  #create list of columns associated with categorical transform\n",
        "  categorylist = textcolumns\n",
        "\n",
        "  return mdf_train, mdf_test, textcolumns, categorylist\n",
        "\n",
        "\n",
        "\n",
        "#process_time_class(mdf_train, mdf_test, column)\n",
        "#preprocess column with time classifications\n",
        "#takes as arguement two pandas dataframe containing training and test data respectively \n",
        "#(mdf_train, mdf_test), and the name of the column string ('column')\n",
        "\n",
        "#note this trains both training and test data simultaneously due to unique treatment if any category\n",
        "#missing from training set but not from test set to ensure consistent formatting \n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with distinct columns for year, month, day, hour, minute, second\n",
        "#each normalized to the mean and std, with missing values plugged with the mean\n",
        "#with columns named after column_ + time category\n",
        "#returns two transformed dataframe (mdf_train, mdf_test)\n",
        "\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "def process_time_class(mdf_train, mdf_test, column):\n",
        "  \n",
        "  #add a second column with boolean expression indicating a missing cell\n",
        "  #(using NArows(.) function defined below, column name will be column+'_NArows')\n",
        "  NArows_nmbr_train = NArows(mdf_train, column, 'nmbr')\n",
        "  NArows_nmbr_test = NArows(mdf_test, column, 'nmbr')\n",
        "  mdf_train[column + '_NArw'] = NArows_nmbr_train.copy()\n",
        "  mdf_test[column + '_NArw'] = NArows_nmbr_test.copy()\n",
        "  del NArows_nmbr_train\n",
        "  del NArows_nmbr_test\n",
        "  \n",
        "  #change NArows data type to 8-bit (1 byte) integers for memory savings\n",
        "  mdf_train[column + '_NArw'] = mdf_train[column + '_NArw'].astype(np.int8)\n",
        "  mdf_test[column + '_NArw'] = mdf_test[column + '_NArw'].astype(np.int8)\n",
        "  \n",
        "  #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data\n",
        "  mdf_train[column] = pd.to_datetime(mdf_train[column], errors = 'coerce')\n",
        "  mdf_test[column] = pd.to_datetime(mdf_test[column], errors = 'coerce')\n",
        "  \n",
        "  #mdf_train[column].replace(-np.Inf, np.nan)\n",
        "  #mdf_test[column].replace(-np.Inf, np.nan)\n",
        "  \n",
        "  #get mean of various categories of datetime objects to use to plug in missing cells\n",
        "  meanyear = mdf_train[column].dt.year.mean()    \n",
        "  meanmonth = mdf_train[column].dt.month.mean()\n",
        "  meanday = mdf_train[column].dt.day.mean()\n",
        "  meanhour = mdf_train[column].dt.hour.mean()\n",
        "  meanminute = mdf_train[column].dt.minute.mean()\n",
        "  meansecond = mdf_train[column].dt.second.mean()\n",
        "  \n",
        "  #get standard deviation of training data\n",
        "  stdyear = mdf_train[column].dt.year.std()  \n",
        "  stdmonth = mdf_train[column].dt.month.std()\n",
        "  stdday = mdf_train[column].dt.day.std()\n",
        "  stdhour = mdf_train[column].dt.hour.std()\n",
        "  stdminute = mdf_train[column].dt.minute.std()\n",
        "  stdsecond = mdf_train[column].dt.second.std()\n",
        "  \n",
        "  \n",
        "  #create new columns for each category in train set\n",
        "  mdf_train[column + '_year'] = mdf_train[column].dt.year\n",
        "  mdf_train[column + '_month'] = mdf_train[column].dt.month\n",
        "  mdf_train[column + '_day'] = mdf_train[column].dt.day\n",
        "  mdf_train[column + '_hour'] = mdf_train[column].dt.hour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column].dt.minute\n",
        "  mdf_train[column + '_second'] = mdf_train[column].dt.second\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column].dt.year\n",
        "  mdf_test[column + '_month'] = mdf_test[column].dt.month\n",
        "  mdf_test[column + '_day'] = mdf_test[column].dt.day\n",
        "  mdf_test[column + '_hour'] = mdf_test[column].dt.hour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column].dt.minute \n",
        "  mdf_test[column + '_second'] = mdf_test[column].dt.second\n",
        "  \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(meanyear)\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(meanmonth)\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(meanday)\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(meanhour)\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(meanminute)\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(meanyear)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(meanmonth)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(meanday)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(meanhour)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(meanminute)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #subtract mean from column for both train and test\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'] - meanyear\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'] - meanmonth\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'] - meanday\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'] - meanhour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'] - meanminute\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'] - meansecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] - meanyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] - meanmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] - meanday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] - meanhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] - meanminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] - meansecond\n",
        "  \n",
        "  \n",
        "  #divide column values by std for both training and test data\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'] / stdyear\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'] / stdmonth\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'] / stdday\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'] / stdhour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'] / stdminute\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'] / stdsecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] / stdyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] / stdmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] / stdday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] / stdhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] / stdminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] / stdsecond\n",
        "  \n",
        "  \n",
        "  #now replace NaN with 0\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(0)\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(0)\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(0)\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(0)\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(0)\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(0)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(0)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(0)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(0)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(0)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(0)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(0)\n",
        "  \n",
        "  #output of a list of the created column names\n",
        "  datecolumns = [column + '_year', column + '_month', column + '_day', \\\n",
        "                column + '_hour', column + '_minute', column + '_second', \\\n",
        "                column + '_NArw']\n",
        "  \n",
        "  #this is to address an issue I found when parsing columns with only time no date\n",
        "  #which returned -inf vlaues, so if an issue will just delete the associated \n",
        "  #column along with the entry in datecolumns\n",
        "  checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])\n",
        "  if checkyear:\n",
        "    del mdf_train[column + '_year']\n",
        "    datecolumns.remove(column + '_year')\n",
        "    if column + '_year' in mdf_test.columns:\n",
        "      del mdf_test[column + '_year']\n",
        "\n",
        "  checkmonth = np.isinf(mdf_train.iloc[0][column + '_month'])\n",
        "  if checkmonth:\n",
        "    del mdf_train[column + '_month']\n",
        "    datecolumns.remove(column + '_month')\n",
        "    if column + '_month' in mdf_test.columns:\n",
        "      del mdf_test[column + '_month']\n",
        "\n",
        "  checkday = np.isinf(mdf_train.iloc[0][column + '_day'])\n",
        "  if checkday:\n",
        "    del mdf_train[column + '_day']\n",
        "    datecolumns.remove(column + '_day')\n",
        "    if column + '_day' in mdf_test.columns:\n",
        "      del mdf_test[column + '_day']\n",
        "  \n",
        "  \n",
        "  #delete original column from training data\n",
        "  del mdf_train[column]    \n",
        "  if column in mdf_test.columns:\n",
        "    del mdf_test[column]  \n",
        "  \n",
        "\n",
        "  \n",
        "  #output a dictionary of the associated column mean and std\n",
        "  \n",
        "  timenormalization_dict = {'meanyear' : meanyear, 'meanmonth' : meanmonth, \\\n",
        "                            'meanday' : meanday, 'meanhour' : meanhour, \\\n",
        "                            'meanminute' : meanminute, 'meansecond' : meansecond,\\\n",
        "                            'stdyear' : stdyear, 'stdmonth' : stdmonth, \\\n",
        "                            'stdday' : stdday, 'stdhour' : stdhour, \\\n",
        "                            'stdminute' : stdminute, 'stdsecond' : stdsecond}\n",
        "  \n",
        "  #create list of columns associated with categorical transform (blank for now)\n",
        "  categorylist = []\n",
        "  \n",
        "  return mdf_train, mdf_test, datecolumns, timenormalization_dict, categorylist\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_0JxikCHgUM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2) Create new preprocess function for Box-Cox transformation"
      ]
    },
    {
      "metadata": {
        "id": "P7VLpD3fHpIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#process_bxcx_class(df, column, bxcx_lmbda = None, trnsfrm_mean = None, trnsfrm_std = None)\n",
        "#function that takes as input a dataframe with numnerical column for purposes\n",
        "#of applying a box-cox transformation. If lmbda = None it will infer a suitable\n",
        "#lambda value by minimizing log likelihood using SciPy's stats boxcox call. If\n",
        "#we pass a mean or std value it will apply the mean for the initial infill and \n",
        "#use the values to apply postprocess_numerical_class function. \n",
        "#Returns transformed dataframe, a list nmbrcolumns of the associated columns,\n",
        "#and a normalization dictionary nmbrnormalization_dict which we'll use for our\n",
        "#postprocess_dict, and the parameter lmbda that was used\n",
        "\n",
        "#expect this approach works better than our prior numerical address when the \n",
        "#distribution is less thin tailed\n",
        "\n",
        "\n",
        "#imports\n",
        "from scipy import stats\n",
        "\n",
        "def process_bxcx_class(df, column, bxcx_lmbda = None, trnsfrm_mean = None, \\\n",
        "                       trnsfrm_std = None):\n",
        "  \n",
        "  #add a second column with boolean expression indicating a missing cell\n",
        "  #(using NArows(.) function defined below, column name will be column+'_NArows')\n",
        "  NArows_bxcx = NArows(df, column, 'nmbr')\n",
        "  df[column + '_NArw'] = NArows_bxcx.copy()\n",
        "  del NArows_bxcx\n",
        "  \n",
        "    \n",
        "  #change NArows data type to 8-bit (1 byte) integers for memory savings\n",
        "  df[column + '_NArw'] = df[column + '_NArw'].astype(np.int8)\n",
        "  \n",
        "  #convert all values to either numeric or NaN\n",
        "  df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "  \n",
        "  #get the mean value to apply to infill\n",
        "  if trnsfrm_mean == None:\n",
        "    #get mean of training data\n",
        "    mean = df[column].mean()  \n",
        "    \n",
        "  else:\n",
        "    mean = trnsfrm_mean\n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  df[column] = df[column].fillna(mean)\n",
        "  \n",
        "  #apply box-cox transformation to generate a new column\n",
        "  #note the returns are different based on whether we passed a lmbda value\n",
        "  \n",
        "  if bxcx_lmbda == None:\n",
        "    \n",
        "    df[column + '_bxcx'], bxcx_lmbda = stats.boxcox(df[column])\n",
        "    \n",
        "  else:\n",
        "    \n",
        "    df[column + '_bxcx'] = stats.boxcox(df[column], lmbda = bxcx_lmbda)\n",
        "  \n",
        "  \n",
        "  #apply process_numerical_class to other column and change name\n",
        "  #although if we were given a mean and std value then apply postprocess function\n",
        "  \n",
        "  #if std is none\n",
        "  if trnsfrm_std == None:\n",
        "    \n",
        "    #get std from train set\n",
        "    std = df[column].std()\n",
        "    \n",
        "    #apply numerical transforms\n",
        "    df[column + '_nmbr'] = df[column] - mean\n",
        "    df[column + '_nmbr'] = df[column + '_nmbr'] / std\n",
        "    \n",
        "  else:\n",
        "    \n",
        "    std = trnsfrm_std\n",
        "    \n",
        "    #apply numerical transforms\n",
        "    df[column + '_nmbr'] = df[column] - mean\n",
        "    df[column + '_nmbr'] = df[column + '_nmbr'] / std \n",
        "\n",
        "  #delete the original column\n",
        "  del df[column]  \n",
        "  \n",
        "  \n",
        "  #output of a list of the created column names\n",
        "  nmbrcolumns = [column + '_nmbr', column + '_bxcx', column + '_NArw']\n",
        "  \n",
        "  #output a dictionary of the associated column mean and std\n",
        "  \n",
        "  nmbrnormalization_dict = {'trnsfrm_mean' : mean, 'trnsfrm_std' : std, \\\n",
        "                            'bxcx_lmbda' : bxcx_lmbda}\n",
        "  \n",
        "  #create list of columns associated with categorical transform (blank for now)\n",
        "  categorylist = []\n",
        "  \n",
        "  return df, nmbrcolumns, nmbrnormalization_dict, categorylist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Nd6_y_jVcO3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3) Update evalcategory function for new bxcx category"
      ]
    },
    {
      "metadata": {
        "id": "Sks-OLMoGQEi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#evalcategory(df, column)\n",
        "#Function that dakes as input a dataframe and associated column id \\\n",
        "#evaluates the contents of cells and classifies the column into one of four categories\n",
        "#category 1, 'bnry', is for columns with only two categorys of text or integer\n",
        "#category 2, 'nmbr', is for columns with numerical integer or float values\n",
        "#category 3: 'bxcx', is for nmbr category with all positive values\n",
        "#category 4, 'text', is for columns with multiple categories appropriate for one-hot\n",
        "#category 5, 'date', is for columns with Timestamp data\n",
        "#category 6, 'null', is for columns with >85% null values (arbitrary figure)\n",
        "#returns category id as a string\n",
        "\n",
        "import collections\n",
        "import datetime as dt\n",
        "\n",
        "def evalcategory(df, column):\n",
        "\n",
        "  \n",
        "  #I couldn't find a good pandas tool for evaluating data class, \\\n",
        "  #So will produce an array containing data types of each cell and \\\n",
        "  #evaluate for most common variable using the collections library\n",
        "  \n",
        "  type1_df = df[column].apply(lambda x: type(x)).values\n",
        "  \n",
        "  c = collections.Counter(type1_df)\n",
        "  mc = c.most_common(1)\n",
        "  mc2 = c.most_common(2)\n",
        "  \n",
        "  #free memory (dtypes are memory hogs)\n",
        "  type1_df = None\n",
        "\n",
        "  \n",
        "  #additional array needed to check for time series\n",
        "  \n",
        "  #df['typecolumn2'] = df[column].apply(lambda x: type(pd.to_datetime(x, errors = 'coerce')))\n",
        "  type2_df = df[column].apply(lambda x: type(pd.to_datetime(x, errors = 'coerce'))).values\n",
        "  \n",
        "  datec = collections.Counter(type2_df)\n",
        "  datemc = datec.most_common(1)\n",
        "  datemc2 = datec.most_common(2)\n",
        "  \n",
        "  #free memory (dtypes are memory hogs)\n",
        "  type2_df = None\n",
        "  \n",
        "  \n",
        "  #an extension of this approach could be for those columns that produce a text\\\n",
        "  #category to implement an additional text to determine the number of \\\n",
        "  #common groupings / or the amount of uniquity. For example if every row has\\\n",
        "  #a unique value then one-hot-encoding would not be appropriate. It would \\\n",
        "  #probably be apopropraite to either return an error message if this is found \\\n",
        "  #or alternatively find a furhter way to automate this processing such as \\\n",
        "  #look for contextual clues to groupings that can be inferred.\n",
        "    \n",
        "  #This is kind of hack to evaluate class by comparing these with output of mc\n",
        "  checkint = 1\n",
        "  checkfloat = 1.1\n",
        "  checkstring = 'string'\n",
        "  checkNAN = None\n",
        "\n",
        "  #there's probably easier way to do this, here will create a check for date\n",
        "  df_checkdate = pd.DataFrame([{'checkdate' : '7/4/2018'}])\n",
        "  df_checkdate['checkdate'] = pd.to_datetime(df_checkdate['checkdate'], errors = 'coerce')\n",
        "  \n",
        "\n",
        "  #create dummy variable to store determined class (default is text class)\n",
        "  category = 'text'\n",
        "\n",
        "\n",
        "  #if most common in column is string and > two values, set category to text\n",
        "  if isinstance(checkstring, mc[0][0]) and df[column].nunique() > 2:\n",
        "    category = 'text'\n",
        "  \n",
        "  #if most common is date, set category to date\n",
        "  if isinstance(df_checkdate['checkdate'][0], datemc[0][0]):\n",
        "    category = 'date'\n",
        "  \n",
        "  #if most common in column is integer and > two values, set category to number of bxcx\n",
        "  if isinstance(checkint, mc[0][0]) and df[column].nunique() > 2:\n",
        "    \n",
        "    #if all postiive set category to bxcx\n",
        "    \n",
        "    #if (pd.to_numeric(df[column], errors = 'coerce').notnull() >= 0).all():\n",
        "    \n",
        "    #note we'll only allow bxcx category if all values greater than a clip value\n",
        "    #>0 (currently set at 0.001)since there is an asymptote for box-cox at 0\n",
        "    if (df[pd.to_numeric(df[column], errors='coerce').notnull()][column] >= 0.001).all():\n",
        "      category = 'bxcx'\n",
        "    \n",
        "    else:\n",
        "      category = 'nmbr'\n",
        "    \n",
        "  #if most common in column is float, set category to number or bxcx\n",
        "  if isinstance(checkfloat, mc[0][0]):\n",
        "    \n",
        "    #if all postiive set category to bxcx\n",
        "    \n",
        "    #note we'll only allow bxcx category if all values greater than a clip value\n",
        "    #>0 (currently set at 0.001) since there is an asymptote for box-cox at 0\n",
        "    if (df[pd.to_numeric(df[column], errors='coerce').notnull()][column] >= 0.001).all():\n",
        "      category = 'bxcx'\n",
        "    \n",
        "    else:\n",
        "      category = 'nmbr'\n",
        "  \n",
        "  #if most common in column is integer and <= two values, set category to binary\n",
        "  if isinstance(checkint, mc[0][0]) and df[column].nunique() <= 2:\n",
        "    category = 'bnry'\n",
        "  \n",
        "  #if most common in column is string and <= two values, set category to binary\n",
        "  if isinstance(checkstring, mc[0][0]) and df[column].nunique() <= 2:\n",
        "    category = 'bnry'\n",
        "    \n",
        "      \n",
        "  #if > 80% (ARBITRARY FIGURE) are NaN we'll just delete the column\n",
        "  if df[column].isna().sum() >= df.shape[0] * 0.80:\n",
        "    category = 'null'\n",
        "  \n",
        "  #else if most common in column is NaN, re-evaluate using the second most common type\n",
        "  #(I suspect the below might have a bug somewhere but is working on my current \n",
        "  #tests so will leave be for now)\n",
        "  elif df[column].isna().sum() >= df.shape[0] / 2:\n",
        "    \n",
        "    #if 2nd most common in column is string and > two values, set category to text\n",
        "    if isinstance(checkstring, mc2[1][0]) and df[column].nunique() > 2:\n",
        "      category = 'text'\n",
        "  \n",
        "    #if 2nd most common is date, set category to date   \n",
        "    if isinstance(df_checkdate['checkdate'][0], datemc2[0][0]):\n",
        "      category = 'date'\n",
        "  \n",
        "    #if 2nd most common in column is integer and > two values, set category to number\n",
        "    if isinstance(checkint, mc2[1][0]) and df[column].nunique() > 2:\n",
        "      category = 'nmbr'\n",
        "    \n",
        "    #if 2nd most common in column is float, set category to number\n",
        "    if isinstance(checkfloat, mc2[1][0]):\n",
        "      category = 'nmbr'\n",
        "  \n",
        "    #if 2nd most common in column is integer and <= two values, set category to binary\n",
        "    if isinstance(checkint, mc2[1][0]) and df[column].nunique() <= 2:\n",
        "      category = 'bnry'\n",
        "  \n",
        "    #if 2nd most common in column is string and <= two values, set category to binary\n",
        "    if isinstance(checkstring, mc2[1][0]) and df[column].nunique() <= 2:\n",
        "      category = 'bnry'\n",
        "\n",
        "  \n",
        "  return category"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CSWHrHUvYrYo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4) update MLinfill support functions to address new category"
      ]
    },
    {
      "metadata": {
        "id": "VCm4Vo6zVlv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#NArows(df, column), function that when fed a dataframe, \\\n",
        "#column id, and category label outputs a single column dataframe composed of \\\n",
        "#True and False with the same number of rows as the input and the True's \\\n",
        "#coresponding to those rows of the input that had missing or NaN data. This \\\n",
        "#output can later be used to identify which rows for a column to infill with ML\\\n",
        "# derived plug data\n",
        "\n",
        "\n",
        "def NArows(df, column, category):\n",
        "  \n",
        "\n",
        "  \n",
        "  if category == 'text':\n",
        "  \n",
        "    #returns dataframe of True and False, where True coresponds to the NaN's\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.isna(df[column])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "  \n",
        "  if category == 'bnry':\n",
        "    \n",
        "    #returns dataframe of True and False, where True coresponds to the NaN's\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.isna(df[column])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "    \n",
        "  if category == 'nmbr' or category == 'bxcx':\n",
        "  \n",
        "    #convert all values to either numeric or NaN\n",
        "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "    \n",
        "    #returns dataframe of True and False, where True coresponds to the NaN's\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.isna(df[column])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "\n",
        "  \n",
        "  if category == 'date':\n",
        "    \n",
        "    #returns dataframe column of all False\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.DataFrame(False, index=np.arange(df.shape[0]), columns=[column+'NA'])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "  \n",
        "\n",
        "  return NArows\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#labelbinarizercorrect(npinput, columnslist), function that takes as input the output\\\n",
        "#array from scikit learn's LabelBinarizer() and ensures that the re-encoding is\\\n",
        "#consistent with the original array prior to performing the argmax. This is \\\n",
        "#needed because LabelBinarizer automatically takes two class sets to a binary\\\n",
        "#setting and doesn't account for columns above index of active values based on\\\n",
        "#my understanding. For a large enough dataset this probably won't be an issue \\\n",
        "#but just trying to be thorough. Outputs a one-hot encoded array comparable to \\\n",
        "#the format of our input to argmax.\n",
        "\n",
        "def labelbinarizercorrect(npinput, columnslist):\n",
        "  \n",
        "  \n",
        "  #if our array post application of LabelBinarizer has few coloumns than our \\\n",
        "  #column list then run through these loops\n",
        "  if npinput.shape[1] < len(columnslist):\n",
        "    \n",
        "    #if only one column in our array means LabelEncoder must have binarized \\\n",
        "    #since we already established that there are more columns\n",
        "    if npinput.shape[1] == 1:\n",
        "      \n",
        "      #this transfers from the binary encoding to two columns of one hot\n",
        "      npinput = np.hstack((1 - npinput, npinput))\n",
        "      \n",
        "      np_corrected = npinput\n",
        "      \n",
        "    #if we still have fewer columns than the column list, means we'll need to \\\n",
        "    #pad out with columns containing zeros\n",
        "    if npinput.shape[1] < len(columnslist):\n",
        "      missingcols = len(columnslist) - npinput.shape[1]\n",
        "      append = np.zeros((npinput.shape[0], missingcols))\n",
        "      np_corrected = np.concatenate((npinput, append), axis=1)\n",
        "  \n",
        "  else:\n",
        "    #otherwise just return the input array because it is in good shape\n",
        "    np_corrected = npinput\n",
        "\n",
        "  \n",
        "  return np_corrected\n",
        "\n",
        "\n",
        "\n",
        "#predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "#df_train_fillfeatures, df_test_fillfeatures, randomseed, columnslist), \\\n",
        "#function that takes as input \\\n",
        "#a category string, the output of createMLinfillsets(.), a seed for randomness \\\n",
        "#and a list of columns produced by a text class preprocessor when applicable and \n",
        "#returns predicted infills for the train and test feature sets as df_traininfill, \\\n",
        "#df_testinfill based on derivations using scikit-learn, with the lenth of \\\n",
        "#infill consistent with the number of True values from NArows, and the trained \\\n",
        "#model\n",
        "\n",
        "\n",
        "#imports for numerical class training\n",
        "#from sklearn.linear_model import LinearRegression\n",
        "#from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "#from sklearn.linear_model import Ridge\n",
        "#from sklearn.linear_model import RidgeCV\n",
        "#from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#imports for binary and text class training\n",
        "from sklearn import preprocessing\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.linear_model import SGDClassifier\n",
        "#from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "def predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                  df_train_fillfeatures, df_test_fillfeatures, randomseed, \\\n",
        "                  columnslist = []):\n",
        "\n",
        "  #a reasonable extension of this funciton would be to allow ML inference with \\\n",
        "  #other ML architectures such a SVM or something SGD based for instance\n",
        "  \n",
        "  #convert dataframes to numpy arrays\n",
        "  np_train_filltrain = df_train_filltrain.values\n",
        "  np_train_filllabel = df_train_filllabel.values\n",
        "  np_train_fillfeatures = df_train_fillfeatures.values\n",
        "  np_test_fillfeatures = df_test_fillfeatures.values\n",
        "  \n",
        "  #ony run the following if we have any rows needing infill\n",
        "  if df_train_fillfeatures.shape[0] > 0:\n",
        "\n",
        "    if category == 'nmbr':\n",
        "      \n",
        "      #this is to address a weird error message suggesting I reshape the y with ravel()\n",
        "      np_train_filllabel = np.ravel(np_train_filllabel)\n",
        "      \n",
        "      #train linear regression model using scikit-learn for numerical prediction\n",
        "      #model = LinearRegression()\n",
        "      #model = PassiveAggressiveRegressor(random_state = randomseed)\n",
        "      #model = Ridge(random_state = randomseed)\n",
        "      #model = RidgeCV()\n",
        "      #note that SVR doesn't have an argument for random_state\n",
        "      #model = SVR()\n",
        "      model = RandomForestRegressor(random_state = randomseed)\n",
        "      \n",
        "      model.fit(np_train_filltrain, np_train_filllabel)    \n",
        "      \n",
        "      \n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "    \n",
        "    if category == 'bxcx':\n",
        "      \n",
        "      #this is to address a weird error message suggesting I reshape the y with ravel()\n",
        "      np_train_filllabel = np.ravel(np_train_filllabel)\n",
        "      \n",
        "      #model = SVR()\n",
        "      model = RandomForestRegressor(random_state = randomseed)\n",
        "      \n",
        "      model.fit(np_train_filltrain, np_train_filllabel)   \n",
        "      \n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])     \n",
        "      \n",
        "    \n",
        "\n",
        "    if category == 'bnry':\n",
        "      \n",
        "      #this is to address a weird error message suggesting I reshape the y with ravel()\n",
        "      np_train_filllabel = np.ravel(np_train_filllabel)\n",
        "      \n",
        "      #train logistic regression model using scikit-learn for binary classifier\n",
        "      #model = LogisticRegression()\n",
        "      #model = LogisticRegression(random_state = randomseed)\n",
        "      #model = SGDClassifier(random_state = randomseed)\n",
        "      #model = SVC(random_state = randomseed)\n",
        "      model = RandomForestClassifier(random_state = randomseed)\n",
        "      \n",
        "      model.fit(np_train_filltrain, np_train_filllabel)\n",
        "\n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "#       print('category is bnry, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'text':\n",
        "\n",
        "      #first convert the one-hot encoded set via argmax to a 1D array\n",
        "      np_train_filllabel_argmax = np.argmax(np_train_filllabel, axis=1)\n",
        "\n",
        "      #train logistic regression model using scikit-learn for binary classifier\n",
        "      #with multi_class argument activated\n",
        "      #model = LogisticRegression()\n",
        "      #model = SGDClassifier(random_state = randomseed)\n",
        "      #model = SVC(random_state = randomseed)\n",
        "      model = RandomForestClassifier(random_state = randomseed)\n",
        "      \n",
        "      model.fit(np_train_filltrain, np_train_filllabel_argmax)\n",
        "\n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        #this needs to have same number of columns as text category\n",
        "        np_testinfill = np.zeros(shape=(1,len(columnslist)))\n",
        "\n",
        "      #convert the 1D arrary back to one hot encoding\n",
        "      labelbinarizertrain = preprocessing.LabelBinarizer()\n",
        "      labelbinarizertrain.fit(np_traininfill)\n",
        "      np_traininfill = labelbinarizertrain.transform(np_traininfill)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        labelbinarizertest = preprocessing.LabelBinarizer()\n",
        "        labelbinarizertest.fit(np_testinfill)\n",
        "        np_testinfill = labelbinarizertest.transform(np_testinfill)\n",
        "\n",
        "\n",
        "\n",
        "      #run function to ensure correct dimensions of re-encoded classifier array\n",
        "      np_traininfill = labelbinarizercorrect(np_traininfill, columnslist)\n",
        "      \n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = labelbinarizercorrect(np_testinfill, columnslist)\n",
        "\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = [columnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [columnslist]) \n",
        "      \n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'date':\n",
        "\n",
        "      #create empty sets for now\n",
        "      #an extension of this method would be to implement a comparable infill \\\n",
        "      #method for the time category, based on the columns output from the \\\n",
        "      #preprocessing\n",
        "      df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "      \n",
        "      model = False\n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "  \n",
        "  \n",
        "  #else if we didn't have any infill rows let's create some plug values\n",
        "  else:\n",
        "    \n",
        "    if category == 'text':\n",
        "      np_traininfill = np.zeros(shape=(1,len(columnslist)))\n",
        "      np_testinfill = np.zeros(shape=(1,len(columnslist)))\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = [columnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [columnslist]) \n",
        "    \n",
        "    else :\n",
        "      df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "    \n",
        "    #set model to False, this will be be needed for this eventiality in \n",
        "    #test set post-processing\n",
        "    model = False\n",
        "  \n",
        "  return df_traininfill, df_testinfill, model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BRfkTMdZp5GJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Update some of the ML infill functions"
      ]
    },
    {
      "metadata": {
        "id": "4HEHmpT4Kw7B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#update createMLinfillsets as follows:\n",
        "#instead of diferientiation by category, do a test for whether categorylist = []\n",
        "#if so do a single column transform excluding those other columns from columnslist\n",
        "#in the sets comparable to , otherwise do a transform comparable to text category\n",
        "\n",
        "#createMLinfillsets(df_train, df_test, column, trainNArows, testNArows, \\\n",
        "#category, columnslist = []) function that when fed dataframes of train and\\\n",
        "#test sets, column id, df of True/False corresponding to rows from original \\\n",
        "#sets with missing values, a string category of 'text', 'date', 'nmbr', or \\\n",
        "#'bnry', and a list of column id's for the text category if applicable. The \\\n",
        "#function returns a seris of dataframes which can be applied to training a \\\n",
        "#machine learning model to predict apppropriate infill values for those points \\\n",
        "#that had missing values from the original sets, indlucing returns of \\\n",
        "#df_train_filltrain, df_train_filllabel, df_train_fillfeatures, \\\n",
        "#and df_test_fillfeatures\n",
        "\n",
        "\n",
        "def createMLinfillsets(df_train, df_test, column, trainNArows, testNArows, \\\n",
        "                       category, columnslist = [], categorylist = []):\n",
        "\n",
        "  \n",
        "  \n",
        "  #create 3 new dataframes for each train column - the train and labels \\\n",
        "  #for rows not needing infill, and the features for rows needing infill \\\n",
        "  #also create a test features column \n",
        "\n",
        "  #reminder:\n",
        "    #for numerical there won't be a new column\n",
        "    #for binary there won't be a new column\n",
        "    #for text the new column has a defined name as column+'_missing'\n",
        "\n",
        "  #note that for text class the labels will be a little more complicated \\\n",
        "  #since will be multi-column\n",
        "\n",
        "  if category in ['nmbr', 'bxcx', 'bnry', 'text']:\n",
        "    \n",
        "    #if this is a single column set (not categorical)\n",
        "    if categorylist == []:\n",
        "      \n",
        "      #first concatinate the NArows True/False designations to df_train & df_test\n",
        "      df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "      df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "\n",
        "      #create copy of df_train to serve as training set for fill\n",
        "      df_train_filltrain = df_train.copy()\n",
        "      #now delete rows coresponding to True\n",
        "      df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns.get_values()[0]] == False]\n",
        "\n",
        "      #now delete columns = columnslist and the NA labels (orig column+'_NArows') from this df\n",
        "      df_train_filltrain = df_train_filltrain.drop(columnslist, axis=1)\n",
        "      df_train_filltrain = df_train_filltrain.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "      \n",
        "      \n",
        "      \n",
        "      #create a copy of df_train[column] for fill train labels\n",
        "      df_train_filllabel = pd.DataFrame(df_train[column].copy())\n",
        "      #concatinate with the NArows\n",
        "      df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "      #drop rows corresponding to True\n",
        "      df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns.get_values()[0]] == False]\n",
        "\n",
        "      #delete the NArows column\n",
        "      df_train_filllabel = df_train_filllabel.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "      #create features df_train for rows needing infill\n",
        "      #create copy of df_train (note it already has NArows included)\n",
        "      df_train_fillfeatures = df_train.copy()\n",
        "      #delete rows coresponding to False\n",
        "      df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns.get_values()[0]])]\n",
        "      #delete columnslist and column+'_NArows'\n",
        "      df_train_fillfeatures = df_train_fillfeatures.drop(columnslist, axis=1)\n",
        "      df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "    \n",
        "      \n",
        "      #create features df_test for rows needing infill\n",
        "      #create copy of df_test (note it already has NArows included)\n",
        "      df_test_fillfeatures = df_test.copy()\n",
        "      #delete rows coresponding to False\n",
        "      df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns.get_values()[0]])]\n",
        "      #delete column and column+'_NArows'\n",
        "      df_test_fillfeatures = df_test_fillfeatures.drop(columnslist, axis=1)\n",
        "      df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "      #delete NArows from df_train, df_test\n",
        "      df_train = df_train.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "      df_test = df_test.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "      \n",
        "    #else if categorylist wasn't empty\n",
        "    else:\n",
        "      \n",
        "      #create a list of columns representing columnslist exlucding elements from\n",
        "      #categorylist\n",
        "      noncategorylist = columnslist[:]\n",
        "      #this removes categorylist elements from noncategorylist\n",
        "      noncategorylist = list(set(noncategorylist).difference(set(categorylist)))\n",
        "      \n",
        "      \n",
        "      #first concatinate the NArows True/False designations to df_train & df_test\n",
        "      df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "      df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "\n",
        "      #create copy of df_train to serve as training set for fill\n",
        "      df_train_filltrain = df_train.copy()\n",
        "      #now delete rows coresponding to True\n",
        "      df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns.get_values()[0]] == False]\n",
        "\n",
        "      #now delete columns = columnslist and the NA labels (orig column+'_NArows') from this df\n",
        "      df_train_filltrain = df_train_filltrain.drop(columnslist, axis=1)\n",
        "      df_train_filltrain = df_train_filltrain.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "      #create a copy of df_train[columnslist] for fill train labels\n",
        "      df_train_filllabel = df_train[columnslist].copy()\n",
        "      #concatinate with the NArows\n",
        "      df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "      #drop rows corresponding to True\n",
        "      df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns.get_values()[0]] == False]\n",
        "      \n",
        "      #now delete columns = noncategorylist from this df\n",
        "      df_train_filltrain = df_train_filltrain.drop(noncategorylist, axis=1)\n",
        "      \n",
        "      #delete the NArows column\n",
        "      df_train_filllabel = df_train_filllabel.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "      \n",
        "      #create features df_train for rows needing infill\n",
        "      #create copy of df_train (note it already has NArows included)\n",
        "      df_train_fillfeatures = df_train.copy()\n",
        "      #delete rows coresponding to False\n",
        "      df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns.get_values()[0]])]\n",
        "      #delete columnslist and column+'_NArows'\n",
        "      df_train_fillfeatures = df_train_fillfeatures.drop(columnslist, axis=1)\n",
        "      df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "          \n",
        "      #create features df_test for rows needing infill\n",
        "      #create copy of df_test (note it already has NArows included)\n",
        "      df_test_fillfeatures = df_test.copy()\n",
        "      #delete rows coresponding to False\n",
        "      df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns.get_values()[0]])]\n",
        "      #delete column and column+'_NArows'\n",
        "      df_test_fillfeatures = df_test_fillfeatures.drop(columnslist, axis=1)\n",
        "      df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "      #delete NArows from df_train, df_test\n",
        "      df_train = df_train.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "      df_test = df_test.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "  if category == 'date':\n",
        "\n",
        "    #create empty sets for now\n",
        "    #an extension of this method would be to implement a comparable method \\\n",
        "    #for the time category, based on the columns output from the preprocessing\n",
        "    df_train_filltrain = pd.DataFrame({'foo' : []}) \n",
        "    df_train_filllabel = pd.DataFrame({'foo' : []})\n",
        "    df_train_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "    df_test_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "\n",
        "\n",
        "  return df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vMvIUXv3c1jD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#insertinfill(df, column, infill, category, NArows, columnslist = [])\n",
        "#function that takes as input a dataframe, column id, category string of either\\\n",
        "#'nmbr'/'text'/'bnry'/'date', a df column of True/False identifiying row id of\\\n",
        "#rows that will recieve infill, and and a list of columns produced by a text \\\n",
        "#class preprocessor when applicable. Replaces the column cells in rows \\\n",
        "#coresponding to the NArows True values with the values from infill, returns\\\n",
        "#the associated transformed dataframe.\n",
        "\n",
        "\n",
        "def insertinfill(df, column, infill, category, NArows, columnslist = [], categorylist = []):\n",
        "\n",
        "  #this is kind of a hack\n",
        "  #NArows column name uses original column name + _NArows as key\n",
        "  #current column has original column name + _nmbr or _bxcx at end\n",
        "  #so we'll drop final 5 characters from column string\n",
        "  origcolumnname = column[:-5]\n",
        "  \n",
        "  if category in ['nmbr', 'bxcx', 'bnry', 'text']:\n",
        "\n",
        "    #if this is a single column set (not categorical)\n",
        "    if categorylist == []:\n",
        "      \n",
        "      #create new dataframe for infills wherein the infill values are placed in \\\n",
        "      #rows coresponding to NArows True values and rows coresponding to NArows \\\n",
        "      #False values are filled with a 0    \n",
        "\n",
        "      \n",
        "      #assign index values to a column\n",
        "      df['tempindex1'] = df.index\n",
        "\n",
        "      #concatinate our df with NArows\n",
        "      df = pd.concat([df, NArows], axis=1)\n",
        "\n",
        "      #create list of index numbers coresponding to the NArows True values\n",
        "      infillindex = df.loc[df[origcolumnname+'_NArows']]['tempindex1']\n",
        "      \n",
        "      #create a dictionary for use to insert infill using df's index as the key\n",
        "      infill_dict = dict(zip(infillindex, infill.values))\n",
        "\n",
        "      #replace 'tempindex1' column with infill in rows where NArows is True\n",
        "      df['tempindex1'] = np.where(df[origcolumnname+'_NArows'], df['tempindex1'].replace(infill_dict), 'fill')\n",
        "\n",
        "      #now carry that infill over to the target column for rows where NArows is True\n",
        "      df[column] = np.where(df[origcolumnname+'_NArows'], df['tempindex1'], df[column])\n",
        "\n",
        "      #remove the temporary columns from df\n",
        "      df = df.drop(['tempindex1'], axis=1)\n",
        "      df = df.drop([origcolumnname+'_NArows'], axis=1)\n",
        "      \n",
        "      \n",
        "    #else if categorylist wasn't empty\n",
        "    else:\n",
        "      \n",
        "      #create new dataframe for infills wherein the infill values are placed in \\\n",
        "      #rows coresponding to NArows True values and rows coresponding to NArows \\\n",
        "      #False values are filled with a 0\n",
        "\n",
        "      #text infill contains multiple columns for each predicted calssification\n",
        "      #which were derived from one-hot encoding the original column in preprocessing\n",
        "      for textcolumnname in categorylist:\n",
        "\n",
        "        #create newcolumn which will serve as the NArows specific to textcolumnname\n",
        "        df['textNArows'] = NArows\n",
        "\n",
        "        df['textNArows'] = df['textNArows'].replace(0, False)\n",
        "        df['textNArows'] = df['textNArows'].replace(1, True)\n",
        "\n",
        "        #assign index values to a column\n",
        "        df['tempindex1'] = df.index\n",
        "\n",
        "\n",
        "        #create list of index numbers coresponding to the NArows True values\n",
        "        textinfillindex = pd.DataFrame(df.loc[df['textNArows']]['tempindex1'])\n",
        "        #reset the index\n",
        "        textinfillindex = textinfillindex.reset_index()\n",
        "\n",
        "        #now before we create our infill dicitonaries, we're going to need to\n",
        "        #create a seperate textinfillindex for each category\n",
        "\n",
        "        infill['tempindex1'] = textinfillindex['tempindex1']\n",
        "\n",
        "        #first let's create a copy of this textcolumn's infill column replacing \n",
        "        #0/1 with True False (this works because we are one hot encoding)\n",
        "        infill[textcolumnname + '_bool'] = infill[textcolumnname].astype('bool')\n",
        "\n",
        "        #we'll use the mask feature to create infillindex which only contains \\\n",
        "        #rows coresponding to the True value in the column we just created\n",
        "\n",
        "        mask = (infill[[textcolumnname + '_bool']]==True).all(1)\n",
        "        infillindex = infill[mask]['tempindex1']\n",
        "\n",
        "\n",
        "\n",
        "        #we're only going to insert the infill to column textcolumnname if we \\\n",
        "        #have infill to insert\n",
        "\n",
        "        if len(infillindex.values) > 0:\n",
        "\n",
        "          df.loc[infillindex.values[0], textcolumnname] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #now we'll delete temporary support columns associated with textcolumnname\n",
        "        infill = infill.drop([textcolumnname + '_bool'], axis=1)\n",
        "        infill = infill.drop(['tempindex1'], axis=1)\n",
        "        df = df.drop(['textNArows'], axis=1)\n",
        "        df = df.drop(['tempindex1'], axis=1)\n",
        "      \n",
        "      \n",
        "\n",
        "  \n",
        "  if category == 'date':\n",
        "    #this spot reserved for future update to incorporate address of datetime\\\n",
        "    #category data\n",
        "    df = df\n",
        "  \n",
        "  \n",
        "  return df "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MS4ulMW843U0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5) update automunge(.) function for new bxcx category"
      ]
    },
    {
      "metadata": {
        "id": "6gLKZbliVumS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#automunge(df_train, df_test, labels_column, valpercent=0.20, powertransform = True, \\\n",
        "#MLinfill = True, infilliterate=1, randomseed = 42, excludetransformscolumns = []) \\\n",
        "#Function that when fed a train and test data set automates the process \\\n",
        "#of evaluating each column for determination and applicaiton of appropriate \\\n",
        "#preprocessing. Takes as arguement pandas dataframes of training and test data \\\n",
        "#(mdf_train), (mdf_test), the name of the column from train set containing \\\n",
        "#labels, a string identifying the ID column for train and test, a value for \\\n",
        "#percent of training data to be applied to a validation set, a True'False selector \\\n",
        "#to determine if a power law transoffrmation will be applied to numerical sets, a \\\n",
        "#True/False selector to determine if MLinfill methods will be applied to any missing \\\n",
        "#points, an integer indication how many iterations of infill predfictions to \\\n",
        "#run, a random seed integer, and a list of any stroing column names that are to\\\n",
        "#be excluded from processing. (If MLinfill = False, missing points are addressed \\\n",
        "#with mean for numerical, most common value for binary, new column for one-hot \\\n",
        "#encoding, and mean for datetime). Note that the ML method for datetime data is \\\n",
        "#future extension. Based on an evaluation of columns selectively applies one of \\\n",
        "#four preprocessing functions to each. Shuffles the data and splits the training \\\n",
        "#set into train and validation sets. \n",
        "#returns train, trainID, labels, validation, validationID, validationlabels, \\\n",
        "#test, testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test,  \\\n",
        "#postprocess_dict\n",
        "\n",
        "#Note that this approach assumes that the test data is available at time of training\n",
        "#For subsequent processing of test data the postmung function can be applied\n",
        "#with as input the postprocess_dict returned by automunge's address of the train set\n",
        "\n",
        "#The thinking with the infilliterate approach is that for particularly messy \\\n",
        "#sets the predictinfill method will be influenced by the initial plug value \\\n",
        "#for missing cells, and so multiple iterations of the predictinfill should \\\n",
        "#trend towards better predictions. Initial tests of this iteration did not \\\n",
        "#demonstrate much effect so this probably is not neccesary for common use.\n",
        "\n",
        "#a word of caution: if you are excluding any columns from processing via \\\n",
        "#excludetransformscolumns list make sure they are already in a suitable state \\\n",
        "#for application of ML (e.g. numerical) otherwise the MLinfill technique will \\\n",
        "#return errors\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def automunge(df_train, df_test, labels_column, trainID_column = False, \\\n",
        "              testID_column = False, valpercent=0.20, powertransform = True, \\\n",
        "              MLinfill = True, infilliterate=1, randomseed = 42, \\\n",
        "              excludetransformscolumns = []):\n",
        "  \n",
        "  #An extension could be to test the input data here for non-dataframe format \\\n",
        "  #(such as csv) to convert it to pandas within the function. \n",
        "  \n",
        "  #my understanding is it is good practice to convert any None values into NaN \\\n",
        "  #so I'll just get that out of the way\n",
        "  df_train.fillna(value=float('nan'), inplace=True)\n",
        "  df_test.fillna(value=float('nan'), inplace=True)\n",
        "  \n",
        "  #we'll delete any rows from training set missing values in the labels column\n",
        "  df_train = df_train.dropna(subset=[labels_column])\n",
        "  \n",
        "  #extract the ID columns from train and test set\n",
        "  if trainID_column != False:\n",
        "    df_trainID = pd.DataFrame(df_train[trainID_column])\n",
        "    del df_train[trainID_column]\n",
        "    \n",
        "  if testID_column != False:\n",
        "    df_testID = pd.DataFrame(df_test[testID_column])\n",
        "    del df_test[testID_column]\n",
        "  \n",
        "  #extract labels from train set\n",
        "  #an extension to this function could be to delete the training set rows\\\n",
        "  #where the labels are missing or improperly formatted prior to performing\\\n",
        "  #this step\n",
        "  df_labels = pd.DataFrame(df_train[labels_column])\n",
        "  \n",
        "  #create copy of labels to support the translation dictionary for use after \\\n",
        "  #prediction to convert encoded predictions back to the original label\n",
        "  df_labels2 = pd.DataFrame(df_labels.copy())\n",
        "  \n",
        "  del df_train[labels_column]\n",
        "  \n",
        "  \n",
        "  #confirm consistency of train an test sets\n",
        "  \n",
        "  #check number of columns is consistent\n",
        "  if df_train.shape[1] != df_test.shape[1]:\n",
        "    print(\"error, different number of columns in train and test sets\")\n",
        "    print(\"(This assesment excludes labels and ID columns.)\")\n",
        "    return\n",
        "  \n",
        "  #check column headers are consistent (this works independent of order)\n",
        "  columns_train = set(list(df_train))\n",
        "  columns_test = set(list(df_test))\n",
        "  if columns_train != columns_test:\n",
        "    print(\"error, different column labels in the train and test set\")\n",
        "    print(\"(This assesment excludes labels and ID columns.)\")\n",
        "    return\n",
        "\n",
        "  columns_train = list(df_train)\n",
        "  columns_test = list(df_test)\n",
        "  if columns_train != columns_test:\n",
        "    print(\"error, different order of column labels in the train and test set\")\n",
        "    print(\"(This assesment excludes labels and ID columns.)\")\n",
        "    return\n",
        "  \n",
        "  #extract column lists again but this time as a list\n",
        "  columns_train = list(df_train)\n",
        "  columns_test = list(df_test)\n",
        "\n",
        "  \n",
        "  #create an empty dataframe to serve as a store for each column's NArows\n",
        "  #the column id's for this df will follow convention from NArows of \n",
        "  #column+'_NArows' for each column in columns_train\n",
        "  #these are used in the ML infill methods\n",
        "  masterNArows_train = pd.DataFrame()\n",
        "  masterNArows_test = pd.DataFrame()\n",
        "  \n",
        "  \n",
        "  #create an empty dictionary to serve as store for categorical transforms lists\n",
        "  #of associated columns\n",
        "  multicolumntransform_dict = {}\n",
        "  \n",
        "  #create an empty dictionary to serve as a store of processing variables from \\\n",
        "  #processing that were specific to the train dataset. These can be used for \\\n",
        "  #future processing of a later test set without the need to reprocess the \\\n",
        "  #original train. The dictionary will be populated with an entry for each \\\n",
        "  #column post processing, and will contain a column specific and category \\\n",
        "  #specific (i.e. nmbr, bnry, text, date) set of variable.\n",
        "  postprocess_dict = {'column_dict' : {}}\n",
        "  \n",
        "  \n",
        "  #For each column, determine appropriate processing function\n",
        "  #processing function will be based on evaluation of train set\n",
        "  for column in columns_train:\n",
        "    \n",
        "    #re-initialize the column specific dictionary for later insertion into\n",
        "    #our postprocess_dict\n",
        "    column_dict = {}\n",
        "    \n",
        "    #we're only going to process columns that weren't in our excluded set\n",
        "    if column not in excludetransformscolumns:\n",
        "      \n",
        "      category = evalcategory(df_train, column)\n",
        "      \n",
        "      #let's make sure the category is consistent between train and test sets\n",
        "      category_test = evalcategory(df_test, column)\n",
        "      \n",
        "      #for the special case of train category = bxcx and test category = nmbr\n",
        "      #(meaning there were no negative values in train but there were in test)\n",
        "      #we'll resolve by reseting the train category to nmbr\n",
        "      if category == 'bxcx' and category_test == 'nmbr':\n",
        "        category = 'nmbr'\n",
        "        \n",
        "      #one more bxcx special case: if user elects not to apply boxcox transform\n",
        "      #default to 'nmbr' category instead of 'bxcx'\n",
        "      if category == 'bxcx' and powertransform == False:\n",
        "        category = 'nmbr'\n",
        "        category_test = 'nmbr'\n",
        "      \n",
        "      #otherwise if train category != test category return error\n",
        "      if category != category_test:\n",
        "        print('error - different category between train and test sets for column ',\\\n",
        "             column)\n",
        "        \n",
        "      #here we'll delete any columns that returned a 'null' category\n",
        "      if category == 'null':\n",
        "        df_train = df_train.drop([column], axis=1)\n",
        "        df_test = df_test.drop([column], axis=1)\n",
        "        \n",
        "        column_dict = { column + '_null' : {'category' : 'null', \\\n",
        "                                            'origcategory' : 'null', \\\n",
        "                                            'normalization_dict' : {}, \\\n",
        "                                            'origcolumn' : column, \\\n",
        "                                            'columnslist' : [column], \\\n",
        "                                            'categorylist' : [], \\\n",
        "                                            'infillmodel' : False, \\\n",
        "                                            'infillcomplete' : False }}\n",
        "        \n",
        "        #now append column_dict onto postprocess_dict\n",
        "        postprocess_dict['column_dict'].update(column_dict)\n",
        "\n",
        "      \n",
        "      #so if we didn't delete the column let's proceed\n",
        "      else:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #create NArows (column of True/False where True coresponds to missing data)\n",
        "        trainNArows = NArows(df_train, column, category)\n",
        "        testNArows = NArows(df_test, column, category)\n",
        "\n",
        "        #now append that NArows onto a master NA rows df\n",
        "        masterNArows_train = pd.concat([masterNArows_train, trainNArows], axis=1)\n",
        "        masterNArows_test = pd.concat([masterNArows_test, testNArows], axis=1)\n",
        "\n",
        "\n",
        "        #(now normalize as would normally)\n",
        "\n",
        "\n",
        "\n",
        "        #for binary class use the majority field for missing plug value\n",
        "        if category == 'bnry':\n",
        "          binary_missing_plug = df_train[column].value_counts().index.tolist()[0]\n",
        "\n",
        "\n",
        "          #apply appropriate processing function to this column based on the result\n",
        "          df_train, bnrycolumns, categorylist = process_binary_class(df_train, column, binary_missing_plug)\n",
        "          df_test, _1, _2 = process_binary_class(df_test, column, binary_missing_plug)\n",
        "          \n",
        "          bnrynormalization_dict = {'missing' : binary_missing_plug}\n",
        "          \n",
        "          #store some values in the column_dict{} for use later in ML infill methods\n",
        "          for bc in bnrycolumns:\n",
        "\n",
        "            \n",
        "            column_dict = { bc : {'category' : 'bnry', \\\n",
        "                                 'origcategory' : 'bnry', \\\n",
        "                                 'normalization_dict' : bnrynormalization_dict, \\\n",
        "                                 'origcolumn' : column, \\\n",
        "                                 'columnslist' : bnrycolumns, \\\n",
        "                                 'categorylist' : categorylist, \\\n",
        "                                 'infillmodel' : False, \\\n",
        "                                 'infillcomplete' : False}}\n",
        "\n",
        "\n",
        "            #now append column_dict onto postprocess_dict\n",
        "            postprocess_dict['column_dict'].update(column_dict)\n",
        "          \n",
        "          \n",
        "        if category == 'nmbr':\n",
        "          df_train, df_test, mean, std, nmbrcolumns, categorylist = \\\n",
        "          process_numerical_class(df_train, df_test, column)\n",
        "          \n",
        "          nmbrnormalization_dict = {'mean' : mean, 'std' : std}\n",
        "          \n",
        "          #store some values in the nmbr_dict{} for use later in ML infill methods\n",
        "          for nc in nmbrcolumns:\n",
        "\n",
        "\n",
        "            column_dict = { nc : {'category' : 'nmbr' , \\\n",
        "                                  'origcategory' : 'nmbr', \\\n",
        "                                  'normalization_dict' : nmbrnormalization_dict, \\\n",
        "                                  'origcolumn' : column, \\\n",
        "                                  'columnslist' : nmbrcolumns, \\\n",
        "                                  'categorylist' : categorylist, \\\n",
        "                                  'infillmodel' : False, \\\n",
        "                                  'infillcomplete' : False}}\n",
        "\n",
        "            \n",
        "            #now append column_dict onto postprocess_dict\n",
        "            postprocess_dict['column_dict'].update(column_dict)\n",
        "          \n",
        "\n",
        "        if category == 'bxcx':\n",
        "          \n",
        "          df_train, nmbrcolumns, nmbrnormalization_dict, categorylist = \\\n",
        "          process_bxcx_class(df_train, column, bxcx_lmbda = None, \\\n",
        "                             trnsfrm_mean = None, trnsfrm_std = None)\n",
        "          \n",
        "          df_test, nmbrcolumns, _1, _2 = \\\n",
        "          process_bxcx_class(df_test, column, bxcx_lmbda = \\\n",
        "                             nmbrnormalization_dict['bxcx_lmbda'], \\\n",
        "                             trnsfrm_mean = nmbrnormalization_dict['trnsfrm_mean'], \\\n",
        "                             trnsfrm_std = nmbrnormalization_dict['trnsfrm_std'])\n",
        "          \n",
        "          \n",
        "          #store some values in the bxcx_dict{} for use later in ML infill methods\n",
        "          for nc in nmbrcolumns:\n",
        "\n",
        "          \n",
        "            #now create our column_dict for bxcx category \n",
        "            column_dict = {nc : {'category' : 'bxcx', \\\n",
        "                                 'origcategory' : 'bxcx', \\\n",
        "                                 'normalization_dict' : nmbrnormalization_dict, \\\n",
        "                                 'origcolumn' : column, \\\n",
        "                                 'columnslist' : nmbrcolumns, \\\n",
        "                                 'categorylist' : categorylist, \\\n",
        "                                 'infillmodel' : False, \\\n",
        "                                 'infillcomplete' : False }}\n",
        "          \n",
        "            \n",
        "            #now append column_dict onto postprocess_dict\n",
        "            postprocess_dict['column_dict'].update(column_dict)\n",
        "        \n",
        "        \n",
        "        if category == 'text':\n",
        "          df_train, df_test, textcolumns, categorylist = \\\n",
        "          process_text_class(df_train, df_test, column)\n",
        "          \n",
        "          #store some values in the text_dict{} for use later in ML infill methods\n",
        "          for tc in textcolumns:\n",
        "\n",
        "          \n",
        "            column_dict = {tc : {'category' : 'text', \\\n",
        "                                 'origcategory' : 'text', \\\n",
        "                                 'normalization_dict' : {}, \\\n",
        "                                 'origcolumn' : column, \\\n",
        "                                 'columnslist' : textcolumns, \\\n",
        "                                 'categorylist' : categorylist, \\\n",
        "                                 'infillmodel' : False, \\\n",
        "                                 'infillcomplete' : False }}\n",
        "\n",
        "    \n",
        "    \n",
        "            #now append column_dict onto postprocess_dict\n",
        "            postprocess_dict['column_dict'].update(column_dict)\n",
        "          \n",
        "\n",
        "        if category == 'date':\n",
        "          df_train, df_test, datecolumns, timenormalization_dict, categorylist = \\\n",
        "          process_time_class(df_train, df_test, column)\n",
        "\n",
        "          #store some values in the date_dict{} for use later in ML infill methods\n",
        "\n",
        "          for dc in datecolumns:\n",
        "\n",
        "            \n",
        "            column_dict = {dc :  {'category' : 'date', \\\n",
        "                                  'origcategory' : 'date', \\\n",
        "                                  'normalization_dict' : timenormalization_dict, \\\n",
        "                                  'origcolumn' : column, \\\n",
        "                                  'columnslist' : datecolumns, \\\n",
        "                                  'categorylist' : categorylist, \\\n",
        "                                  'infillmodel' : False, \\\n",
        "                                  'infillcomplete' : False }}\n",
        "                                  \n",
        "            \n",
        "\n",
        "            #now append column_dict onto postprocess_dict\n",
        "            postprocess_dict['column_dict'].update(column_dict)\n",
        "\n",
        "  \n",
        "  #now that we've pre-processed all of the columns, let's run through them again\\\n",
        "  #using ML to derive plug values for the previously missing cells\n",
        "\n",
        "  \n",
        "  if MLinfill == True:\n",
        "    \n",
        "    \n",
        "    columns_train_ML = list(df_train)\n",
        "    columns_test_ML = list(df_test)\n",
        "    \n",
        "    \n",
        "    iteration = 0\n",
        "    \n",
        "    while iteration < infilliterate:\n",
        "\n",
        "        \n",
        "      for key in postprocess_dict['column_dict']:\n",
        "        postprocess_dict['column_dict'][key]['infillcomplete'] = False\n",
        "      \n",
        "      \n",
        "      for column in columns_train_ML:\n",
        "\n",
        "\n",
        "        #we're only going to process columns that weren't in our excluded set\n",
        "        #or aren't identifiers for NA rows\n",
        "        if column not in excludetransformscolumns \\\n",
        "        and column[-5:] != '_NArw':\n",
        "\n",
        "\n",
        "          \n",
        "          #If column id is found in the text_dict then will require different \\\n",
        "          #type of address since this category won't be found in our \\\n",
        "          #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "          #collective group of columns from the associated textcolumns array.\n",
        "\n",
        "          #if column in text_dict:\n",
        "          if column[-5:] == '_text':\n",
        "          \n",
        "\n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            if postprocess_dict['column_dict'][column]['infillcomplete'] == False:\n",
        "\n",
        "              #pull this column's textcolumns list\n",
        "              textcolumns = postprocess_dict['column_dict'][column]['columnslist']\n",
        "              textcategorylist = postprocess_dict['column_dict'][column]['categorylist']\n",
        "\n",
        "              category = 'text'\n",
        "\n",
        "              #now let's apply our functions for ML infill\n",
        "\n",
        "              #createMLinfillsets\n",
        "              df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures = \\\n",
        "              createMLinfillsets(df_train, \\\n",
        "                                 df_test, column, \\\n",
        "                                 pd.DataFrame(masterNArows_train[postprocess_dict['column_dict'][column]['origcolumn']+'_NArows']), \\\n",
        "                                 pd.DataFrame(masterNArows_test[postprocess_dict['column_dict'][column]['origcolumn']+'_NArows']), \\\n",
        "                                 category, \\\n",
        "                                 columnslist = textcolumns, \\\n",
        "                                 categorylist = textcategorylist)          \n",
        "\n",
        "\n",
        "              #predict infill values using defined function predictinfill(.)\n",
        "              df_traininfill, df_testinfill, model = \\\n",
        "              predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                            df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                            randomseed, columnslist = textcolumns)\n",
        "              \n",
        "              #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "              #to df's associated column\n",
        "              \n",
        "              df_train = insertinfill(df_train, column, df_traininfill, category, \\\n",
        "                                      pd.DataFrame(masterNArows_train[postprocess_dict['column_dict'][column]['origcolumn']+'_NArows']), \\\n",
        "                                      columnslist = textcolumns, categorylist = textcategorylist)\n",
        "              \n",
        "              #now we'll add our trained text model to the postprocess_dict\n",
        "              postprocess_dict['column_dict'][column]['infillmodel'] \\\n",
        "              = model\n",
        "              \n",
        "              #troubleshooting note: it occurs to me that we're only saving our\n",
        "              #trained model in the postprocess_dict for one of the text columns\n",
        "              #not all, however since this will be the first column to be \n",
        "              #addressed here and also in the postmunge function (they're in \n",
        "              #same order) my expectation is that this will not be an issue and \\\n",
        "              #accidental bonus since we're only saving once results in reduced\n",
        "              #file size\n",
        "              \n",
        "              \n",
        "              #it's a quirk of the ML models that if we don't train the\n",
        "              #train set model on any features, that we won't be able to apply\n",
        "              #the model to predict the test set infill. \n",
        "              #For now we'll only use insertilnfill if we had\n",
        "              #some missing points in the train set, a future extension would be\n",
        "              #to update our createMLinfillsets and predictinfill to create \n",
        "              #some arbitrary features to train the infill on train set for\n",
        "              #cases where there are NaN values in test set but not in train\n",
        "              #such that we could insert infill for missing values in test set. \n",
        "              if any(x == True for x in masterNArows_train[postprocess_dict['column_dict'][column]['origcolumn']+'_NArows']):\n",
        "              \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[postprocess_dict['column_dict'][column]['origcolumn']+'_NArows']), \\\n",
        "                                       columnslist = textcolumns, categorylist = textcategorylist)\n",
        "\n",
        "              #now change the infillcomplete marker in the text_dict for each \\\n",
        "              #associated text column\n",
        "              for textcolumnname in textcolumns:\n",
        "                postprocess_dict['column_dict'][textcolumnname]['infillcomplete'] = True\n",
        "          \n",
        "          \n",
        "          #If column id found in the bxcx_dict then we'll address with the\n",
        "          #box-cox methods\n",
        "          #elif column in bxcx_dict:\n",
        "          elif column[-5:] == '_bxcx':\n",
        "            \n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            if postprocess_dict['column_dict'][column]['infillcomplete'] == False:\n",
        "              \n",
        "              #pull this column's textcolumns list\n",
        "              #for the record I know this is a list not an array\n",
        "              nmbrcolumns = postprocess_dict['column_dict'][column]['columnslist']\n",
        "              nmbrcategorylist = postprocess_dict['column_dict'][column]['categorylist']\n",
        "              \n",
        "              #grab the original column name prior to preprocessing\n",
        "              nmbrorigcolumn = postprocess_dict['column_dict'][column]['origcolumn']\n",
        "              \n",
        "              #now get the transformed column name for the two categories of nmbr and bxcx\n",
        "              column_bxcx = nmbrorigcolumn + '_bxcx'\n",
        "              category_bxcx = 'bxcx'\n",
        "              column_nmbr = nmbrorigcolumn + '_nmbr'\n",
        "              category_nmbr = 'nmbr'\n",
        "              \n",
        "              #now let's apply our functions for ML infill\n",
        "              \n",
        "              #for the record I'm sure that the conversion of the single column\n",
        "              #series to a dataframe is counter to the intent of pandas\n",
        "              #it's probably less memory efficient but it's the current basis of\n",
        "              #the functions so we're going to maintain that approach for now\n",
        "              #the revision of these functions to accept pandas series is a\n",
        "              #possible future extension\n",
        "              \n",
        "              #createMLinfillsets\n",
        "              \n",
        "              df_train_filltrain, df_train_filllabel, df_train_fillfeatures, \\\n",
        "              df_test_fillfeatures = \\\n",
        "              createMLinfillsets(df_train, df_test, column_bxcx, \\\n",
        "                                 pd.DataFrame(masterNArows_train[nmbrorigcolumn+'_NArows']), \\\n",
        "                                 pd.DataFrame(masterNArows_test[nmbrorigcolumn+'_NArows']), \\\n",
        "                                 category_bxcx, columnslist = nmbrcolumns, \\\n",
        "                                 categorylist = nmbrcategorylist)\n",
        "              \n",
        "              #predict infill values using defined function predictinfill(.)\n",
        "              #first we'll address the bxcx transformed column\n",
        "              df_traininfill_bxcx, df_testinfill_bxcx, model_bxcx = \\\n",
        "                            predictinfill(category_bxcx, df_train_filltrain, \\\n",
        "                            df_train_filllabel, \\\n",
        "                            df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                            randomseed, columnslist = nmbrcolumns)\n",
        "\n",
        "              \n",
        "              #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "              #to df's associated column\n",
        "              #first we'll address the bxcx transformed column\n",
        "              df_train = insertinfill(df_train, column_bxcx, df_traininfill_bxcx, \\\n",
        "                                      category_bxcx, \\\n",
        "                                      pd.DataFrame(masterNArows_train[nmbrorigcolumn+'_NArows']), \\\n",
        "                                      columnslist = nmbrcolumns, categorylist = nmbrcategorylist)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "              #now we'll address the nmbr column\n",
        "              \n",
        "              df_traininfill_nmbr, df_testinfill_nmbr, model_nmbr = \\\n",
        "              predictinfill(category_nmbr, df_train_filltrain, \\\n",
        "                            df_train_filllabel, \\\n",
        "                            df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                            randomseed, columnslist = nmbrcolumns)\n",
        "\n",
        "\n",
        "              \n",
        "              #then we'll address the nmbr transformed column\n",
        "              df_train = insertinfill(df_train, column_nmbr, df_traininfill_nmbr, \\\n",
        "                                      category_bxcx, \\\n",
        "                                      pd.DataFrame(masterNArows_train[nmbrorigcolumn+'_NArows']), \\\n",
        "                                      columnslist = nmbrcolumns, categorylist = nmbrcategorylist)\n",
        "              \n",
        "              \n",
        "              #now we'll add our trained text model to the postprocess_dict\n",
        "              postprocess_dict['column_dict'][column[:-5] + '_bxcx']['infillmodel'] \\\n",
        "              = model_bxcx\n",
        "              \n",
        "              postprocess_dict['column_dict'][column[:-5] + '_nmbr']['infillmodel'] \\\n",
        "              = model_nmbr\n",
        "              \n",
        "              \n",
        "              #only use insertilnfill if we had some missing points in the train set\n",
        "              if any(x == True for x in masterNArows_train[nmbrorigcolumn + '_NArows']):\n",
        "              \n",
        "                df_test = insertinfill(df_test, column_bxcx, df_testinfill_bxcx, \\\n",
        "                                       category_bxcx, \\\n",
        "                                       pd.DataFrame(masterNArows_test[nmbrorigcolumn+'_NArows']), \\\n",
        "                                       columnslist = nmbrcolumns, categorylist = nmbrcategorylist)\n",
        "                \n",
        "                df_test = insertinfill(df_test, column_nmbr, df_testinfill_nmbr, \\\n",
        "                                       category_bxcx, \\\n",
        "                                       pd.DataFrame(masterNArows_test[nmbrorigcolumn+'_NArows']), \\\n",
        "                                       columnslist = nmbrcolumns, categorylist = nmbrcategorylist)\n",
        "                \n",
        "\n",
        "              #now change the infillcomplete marker in the text_dict for each \\\n",
        "              #associated text column\n",
        "              for nmbrcolumnname in nmbrcolumns:\n",
        "                postprocess_dict['column_dict'][nmbrcolumnname]['infillcomplete'] = True\n",
        "              \n",
        "              \n",
        "\n",
        "          #If column id is found in the date_dict then will require different \\\n",
        "          #type of address since this category won't be found in our \\\n",
        "          #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "          #collective group of columns from the associated datecolumns array. \\\n",
        "          #The development of this address for date columns is a future extension.\n",
        "          #elif column in date_dict:\n",
        "          elif column[-5:] == '_date':\n",
        "\n",
        "            #this section to be a future extension.\n",
        "            pass\n",
        "\n",
        "\n",
        "          #elif column in bnry_dict:\n",
        "          elif column[-5:] == '_bnry':\n",
        "            \n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            if postprocess_dict['column_dict'][column]['infillcomplete'] == False:\n",
        "              \n",
        "              #pull this column's bnrycolumns list\n",
        "              bnrycolumns = postprocess_dict['column_dict'][column]['columnslist']\n",
        "              bnrycategorylist = postprocess_dict['column_dict'][column]['categorylist']\n",
        "\n",
        "              category = 'bnry'\n",
        "              \n",
        "              #grab the original column name prior to preprocessing\n",
        "              bnryorigcolumn = postprocess_dict['column_dict'][column]['origcolumn']\n",
        "              \n",
        "              #now get the transformed column name for the row needing infill\n",
        "              bnryinfillcolumn = bnryorigcolumn+'_bnry'\n",
        "              \n",
        "              #now let's apply our functions for ML infill\n",
        "              #createMLinfillsets\n",
        "              \n",
        "              df_train_filltrain, df_train_filllabel, df_train_fillfeatures, \\\n",
        "              df_test_fillfeatures = \\\n",
        "              createMLinfillsets(df_train, df_test, bnryinfillcolumn, \\\n",
        "                                 pd.DataFrame(masterNArows_train[bnryorigcolumn + '_NArows']), \\\n",
        "                                 pd.DataFrame(masterNArows_test[bnryorigcolumn + '_NArows']), category, \\\n",
        "                                 columnslist = bnrycolumns, categorylist = bnrycategorylist)\n",
        "              \n",
        "              #predict infill values using defined function predictinfill(.)\n",
        "              df_traininfill, df_testinfill, model = \\\n",
        "              predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                            df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                            randomseed, columnslist = bnrycolumns)\n",
        "              \n",
        "              #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "              #to df's associated column\n",
        "              df_train = insertinfill(df_train, bnryinfillcolumn, df_traininfill, category, \\\n",
        "                                      pd.DataFrame(masterNArows_train[bnryorigcolumn + '_NArows']), \\\n",
        "                                      columnslist = bnrycolumns, categorylist = bnrycategorylist)\n",
        "              \n",
        "              #(only insert infill to test set if we had NA rows in train set)\n",
        "              #note comments above in text class for potential future expansion\n",
        "              if any(x == True for x in masterNArows_train[bnryorigcolumn+'_NArows']):\n",
        "                \n",
        "                df_test = insertinfill(df_test, bnryinfillcolumn, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[bnryorigcolumn + '_NArows']), \\\n",
        "                                       columnslist = bnrycolumns, categorylist = bnrycategorylist)\n",
        "              \n",
        "              #now we'll add our trained bnry model to the postprocess_dict\n",
        "              postprocess_dict['column_dict'][bnryinfillcolumn]['infillmodel'] = model\n",
        "          \n",
        "          #elif column in nmbr_dict:\n",
        "          elif column[-5:] == '_nmbr':\n",
        "            \n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            if postprocess_dict['column_dict'][column]['infillcomplete'] == False:\n",
        "              \n",
        "              #pull this column's nmbrcolumns list\n",
        "              nmbrcolumns = postprocess_dict['column_dict'][column]['columnslist']\n",
        "              nmbrcategorylist = postprocess_dict['column_dict'][column]['categorylist']\n",
        "              \n",
        "              category = 'nmbr'\n",
        "              \n",
        "              #grab the original column name prior to preprocessing\n",
        "              nmbrorigcolumn = postprocess_dict['column_dict'][column]['origcolumn']\n",
        "\n",
        "              #now get the transformed column name for the row needing infill\n",
        "              nmbrinfillcolumn = nmbrorigcolumn+'_nmbr'\n",
        "              \n",
        "              #create MLinfill sets using defined function createMLinfillsets(.)\n",
        "              df_train_filltrain, df_train_filllabel, df_train_fillfeatures, \\\n",
        "              df_test_fillfeatures = \\\n",
        "              createMLinfillsets(df_train, df_test, nmbrinfillcolumn, \\\n",
        "                                 pd.DataFrame(masterNArows_train[nmbrorigcolumn+'_NArows']), \\\n",
        "                                 pd.DataFrame(masterNArows_test[nmbrorigcolumn+'_NArows']), \\\n",
        "                                 category, columnslist = nmbrcolumns, categorylist = nmbrcategorylist)\n",
        "              \n",
        "              #predict infill values using defined function predictinfill(.)\n",
        "              df_traininfill, df_testinfill, model = \\\n",
        "              predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                            df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                            randomseed, columnslist = nmbrcolumns)\n",
        "              \n",
        "              #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "              #to df's associated column\n",
        "              df_train = insertinfill(df_train, nmbrinfillcolumn, df_traininfill, category, \\\n",
        "                                      pd.DataFrame(masterNArows_train[nmbrorigcolumn+'_NArows']), \\\n",
        "                                      columnslist = nmbrcolumns, categorylist = nmbrcategorylist)\n",
        "                            \n",
        "              #(only insert infill to test set if we had NA rows in train set)\n",
        "              #note comments above in text class for potential future expansion\n",
        "              if any(x == True for x in masterNArows_train[nmbrorigcolumn+'_NArows']):\n",
        "                \n",
        "                df_test = insertinfill(df_test, nmbrinfillcolumn, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[nmbrorigcolumn+'_NArows']), \\\n",
        "                                       columnslist = nmbrcolumns, categorylist = nmbrcategorylist)\n",
        "              \n",
        "              #now we'll add our trained nmbr model to the postprocess_dict\n",
        "              postprocess_dict['column_dict'][nmbrinfillcolumn]['infillmodel'] = model\n",
        "          \n",
        "          \n",
        "\n",
        "            \n",
        "      iteration += 1\n",
        "\n",
        "  \n",
        "  #determine labels category and apply appropriate function\n",
        "  labelscategory = evalcategory(df_labels, labels_column)\n",
        "  \n",
        "  #empty dummy labels \"test\" df for our preprocessing functions\n",
        "  labelsdummy = pd.DataFrame()\n",
        "  \n",
        "  #initialize a dictionary to serve as the store between labels and their \\\n",
        "  #associated encoding\n",
        "  labelsencoding_dict = {labelscategory:{}}\n",
        "  \n",
        "  #apply appropriate processing function to this column based on the result\n",
        "  if labelscategory == 'bnry':\n",
        "    labels_binary_missing_plug = df_labels[labels_column].value_counts().index.tolist()[0]\n",
        "    df_labels, _1, _2 = process_binary_class(df_labels, labels_column, labels_binary_missing_plug)\n",
        "    \n",
        "    #here we'll populate the dictionery pairing values from the encoded labels \\\n",
        "    #column with the original value for transformation post prediciton\n",
        "    \n",
        "    \n",
        "    i = 0\n",
        "    \n",
        "    for row in df_labels.iterrows():\n",
        "      if row[1][0] in labelsencoding_dict[labelscategory].keys():\n",
        "          i += 1\n",
        "      else:\n",
        "          labelsencoding_dict[labelscategory].update({row[1][0] : df_labels2.iloc[i][0]})\n",
        "          i += 1\n",
        "\n",
        "      \n",
        "  if labelscategory == 'nmbr':\n",
        "    \n",
        "    #if labels category is 'nmbr' we won't apply any further processing to the \\\n",
        "    #column as my experience with linear regression methods is that this is not\\\n",
        "    #required. Further processing of numerical labels would need to be addressed\\\n",
        "    #by returning mean and std from the process_numerical_class method so as to\\\n",
        "    #potentially store in our labelsencoding_dict\n",
        "    pass\n",
        "    \n",
        "    \n",
        "  #it occurs to me there might be an argument for preferring a single numerical \\\n",
        "  #classifier for labels to keep this to a single column, if so scikitlearn's \\\n",
        "  #LabelEcncoder could be used here, will assume that onehot encoding is acceptable\n",
        "  if labelscategory == 'text':\n",
        "    \n",
        "    df_labels, labelsdummy, _1, _2 = \\\n",
        "    process_text_class(df_labels, labelsdummy, labels_column)\n",
        "  \n",
        "    i = 0\n",
        "    \n",
        "    for row in df_labels2.iterrows():\n",
        "      if row[1][0] in labelsencoding_dict[labelscategory].keys():\n",
        "          i += 1\n",
        "      else:\n",
        "          labelsencoding_dict[labelscategory].\\\n",
        "          update({row[1][0] : labels_column+'_'+row[1][0]})\n",
        "          i += 1\n",
        "    \n",
        "  \n",
        "  #great the data is processed now let's do a few moore global training preps\n",
        "  \n",
        "  \n",
        "  #here's a list of final column names saving here since the translation to \\\n",
        "  #numpy arrays scrubs the column names\n",
        "  finalcolumns_train = list(df_train)\n",
        "  finalcolumns_test = list(df_test)\n",
        "  \n",
        "  \n",
        "  #convert all of our dataframes to numpy arrays (train, test, labels, and ID)\n",
        "  #    df_trainID, df_testID\n",
        "  np_train = df_train.values\n",
        "  np_test = df_test.values\n",
        "  np_labels = df_labels.values\n",
        "  \n",
        "  if trainID_column != False:\n",
        "    np_trainID = df_trainID.values\n",
        "  if testID_column != False:\n",
        "    np_testID = df_testID.values\n",
        "  \n",
        "  \n",
        "  #set randomness seed number\n",
        "  answer = randomseed\n",
        "  #a reasonable extension would be to tie this in with randomness seed for \\\n",
        "  #ML infill methods calls to scikit learn\n",
        "  \n",
        "  #shuffle training set and labels\n",
        "  np_train = shuffle(np_train, random_state = answer)\n",
        "  np_labels = shuffle(np_labels, random_state = answer)\n",
        "  \n",
        "  if trainID_column != False:\n",
        "    np_trainID = shuffle(np_trainID, random_state = answer)\n",
        "  \n",
        "  \n",
        "  #split validation sets from training and labels\n",
        "  train, validation, labels, validationlabels = \\\n",
        "  train_test_split(np_train, np_labels, test_size=valpercent, shuffle = False)\n",
        "  \n",
        "  if trainID_column != False:\n",
        "    trainID, validationID = \\\n",
        "    train_test_split(np_trainID, test_size=valpercent, shuffle = False)\n",
        "  else:\n",
        "    trainID = []\n",
        "    validationID = []\n",
        "  if testID_column != False:\n",
        "    testID = np_testID\n",
        "  else:\n",
        "    testID = []\n",
        "  \n",
        "  test = np_test\n",
        "  \n",
        "\n",
        "  \n",
        "  postprocess_dict.update({'origtraincolumns' : columns_train, \\\n",
        "                           'finalcolumns_train' : finalcolumns_train, \\\n",
        "                           'testID_column' : testID_column, \\\n",
        "                           'MLinfill' : MLinfill, \\\n",
        "                           'infilliterate' : infilliterate, \\\n",
        "                           'randomseed' : randomseed, \\\n",
        "                           'excludetransformscolumns' : excludetransformscolumns,\\\n",
        "                           'labelsencoding_dict' : labelsencoding_dict, \\\n",
        "                           'automungeversion' : '1.1' })\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  #a reasonable extension would be to perform some validation functions on the\\\n",
        "  #sets here (or also prior to transofrm to numpuy arrays) and confirm things \\\n",
        "  #like consistency between format of columns and data between our train and \\\n",
        "  #test sets and if any issues return a coresponding error message to alert user\n",
        "  \n",
        "  \n",
        "  return train, trainID, labels, validation, validationID, validationlabels, \\\n",
        "  test, testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test,  \\\n",
        "  postprocess_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JEOu259JTF66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that a future extension will be to create some standardized functions recreatelabels(.) for post prediction transforms of predicitons from numerical to string values if applicable."
      ]
    },
    {
      "metadata": {
        "id": "bmsGjmPt0_xC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6) update postprocess functions for test set"
      ]
    },
    {
      "metadata": {
        "id": "_V4UL-91jA5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #Here is a summary of the postprocess_dict structure from automunge:\n",
        "\n",
        "\n",
        "\n",
        "# postprocess_dict.update({'origtraincolumns' : columns_train, \\\n",
        "#                          'finalcolumns_train' : finalcolumns_train, \\\n",
        "#                          'testID_column' : testID_column, \\\n",
        "#                          'MLinfill' : MLinfill, \\\n",
        "#                          'infilliterate' : infilliterate, \\\n",
        "#                          'randomseed' : randomseed, \\\n",
        "#                          'excludetransformscolumns' : excludetransformscolumns,\\\n",
        "#                          'labelsencoding_dict' : labelsencoding_dict, \\\n",
        "#                          'automungeversion' : '1.1', \n",
        "#                          'column_dict' : {}})\n",
        "\n",
        "# (example of bnry)\n",
        "# column_dict = { bc : {'category' : 'bnry', \\\n",
        "#                      'origcategory' : 'bnry', \\\n",
        "#                      'normalization_dict' : bnrynormalization_dict, \\\n",
        "#                      'origcolumn' : column, \\\n",
        "#                      'columnslist' : bnrycolumns, \\\n",
        "#                      'categorylist' : categorylist, \\\n",
        "#                      'infillmodel' : False, \\\n",
        "#                      'infillcomplete' : False}}\n",
        "\n",
        "# nmbr\n",
        "# nmbrnormalization_dict = {'mean' : mean, 'std' : std}\n",
        "\n",
        "# bxcx\n",
        "# nmbrnormalization_dict = {'trnsfrm_mean' : mean, 'trnsfrm_std' : std, \\\n",
        "#                           'bxcx_lmbda' : bxcx_lmbda}\n",
        "\n",
        "\n",
        "\n",
        "# bnrynormalization_dict = {'missing' : binary_missing_plug}\n",
        "\n",
        "\n",
        "\n",
        "# timenormalization_dict = {'meanyear' : meanyear, 'meanmonth' : meanmonth, \\\n",
        "#                           'meanday' : meanday, 'meanhour' : meanhour, \\\n",
        "#                           'meanminute' : meanminute, 'meansecond' : meansecond,\\\n",
        "#                           'stdyear' : stdyear, 'stdmonth' : stdmonth, \\\n",
        "#                           'stdday' : stdday, 'stdhour' : stdhour, \\\n",
        "#                           'stdminute' : stdminute, 'stdsecond' : stdsecond}\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HKPfQyjI4-FM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#postprocess_numerical_class(mdf_test, column, mean, std)\n",
        "#function to normalize data to mean of 0 and standard deviation of 1 from training distribution\n",
        "#takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\\\n",
        "#and the name of the column string ('column'), and the mean and std from the train set \\\n",
        "#replaces missing or improperly formatted data with mean of remaining values\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#expect this approach works better when the numerical distribution is thin tailed\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "#imports\n",
        "from pandas import Series\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def postprocess_numerical_class(mdf_test, column, mean, std):\n",
        "     \n",
        "  \n",
        "  #add a second column with boolean expression indicating a missing cell\n",
        "  #(using NArows(.) function defined below, column name will be column+'_NArows')\n",
        "  NArows_nmbr_test = NArows(mdf_test, column, 'nmbr')\n",
        "  mdf_test[column + '_NArw'] = NArows_nmbr_test.copy()\n",
        "  del NArows_nmbr_test\n",
        "  \n",
        "  #change NArows data type to 8-bit (1 byte) integers for memory savings\n",
        "  mdf_test[column + '_NArw'] = mdf_test[column + '_NArw'].astype(np.int8)\n",
        "  \n",
        "  #convert all values to either numeric or NaN\n",
        "  mdf_test[column] = pd.to_numeric(mdf_test[column], errors='coerce')\n",
        "\n",
        "  #get mean of training data\n",
        "  mean = mean  \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_test[column] = mdf_test[column].fillna(mean)\n",
        "\n",
        "  #subtract mean from column\n",
        "  mdf_test[column] = mdf_test[column] - mean\n",
        "\n",
        "  #get standard deviation of training data\n",
        "  std = std\n",
        "\n",
        "  #divide column values by std\n",
        "  mdf_test[column] = mdf_test[column] / std\n",
        "  \n",
        "  #change column name to column + '_nmbr'\n",
        "  mdf_test[column + '_nmbr'] = mdf_test[column].copy()\n",
        "  del mdf_test[column]\n",
        "\n",
        "\n",
        "  return mdf_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#postprocess_text_class(mdf_test, column, textcolumns)\n",
        "#process column with text classifications\n",
        "#takes as arguement pandas dataframe containing test data  \n",
        "#()mdf_test), and the name of the column string ('column'), and an array of\n",
        "#the associated transformed column s from the train set (textcolumns)\n",
        "\n",
        "#note this aligns formatting of transformed columns to the original train set\n",
        "#fromt he original treatment with automunge\n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with onehot encodings\n",
        "#with columns named after column_ + text classifications\n",
        "#missing data replaced with category label 'missing'+column\n",
        "#any categories missing from the training set removed from test set\n",
        "#any category present in training but missing from test set given a column of zeros for consistent formatting\n",
        "#ensures order of all new columns consistent between both sets\n",
        "#returns two transformed dataframe (mdf_train, mdf_test) \\\n",
        "#and a list of the new column names (textcolumns)\n",
        "\n",
        "#note it is kind of a hack here to create a column for missing values with \\\n",
        "#two underscores (__) in the column name to ensure appropriate order for cases\\\n",
        "#where NaN present in test data but not train data, if a category starts with|\n",
        "#an underscore such that it preceeds '__missing' alphabetically in this scenario\\\n",
        "#this might create error due to different order of columns, address of this \\\n",
        "#potential issue will be a future extension\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def postprocess_text_class(mdf_test, column, textcolumns):\n",
        "\n",
        "  #replace NA with a dummy variable\n",
        "  mdf_test[column] = mdf_test[column].fillna('_missing')\n",
        "\n",
        "\n",
        "  #extract categories for column labels\n",
        "  #note that .unique() extracts the labels as a numpy array\n",
        "  \n",
        "  #we'll get the category names from the textcolumns array by stripping the \\\n",
        "  #prefixes of column name + '_'\n",
        "  prefixlength = len(column)+1\n",
        "  labels_train = textcolumns[:]\n",
        "  for textcolumn in labels_train:\n",
        "    textcolumn = textcolumn[prefixlength :]\n",
        "  #labels_train.sort(axis=0)\n",
        "  labels_train.sort()\n",
        "  labels_test = mdf_test[column].unique()\n",
        "  labels_test.sort(axis=0)\n",
        "\n",
        "  #transform text classifications to numerical id\n",
        "  encoder = LabelEncoder()\n",
        "#   cat_train = mdf_train[column]\n",
        "#   cat_train_encoded = encoder.fit_transform(cat_train)\n",
        "\n",
        "  cat_test = mdf_test[column]\n",
        "  cat_test_encoded = encoder.fit_transform(cat_test)\n",
        "\n",
        "\n",
        "  #apply onehotencoding\n",
        "  onehotencoder = OneHotEncoder()\n",
        "#   cat_train_1hot = onehotencoder.fit_transform(cat_train_encoded.reshape(-1,1))\n",
        "  cat_test_1hot = onehotencoder.fit_transform(cat_test_encoded.reshape(-1,1))\n",
        "\n",
        "  #append column header name to each category listing\n",
        "  #note the iteration is over a numpy array hence the [...] approach  \n",
        "#   labels_train[...] = column + '_' + labels_train[...]\n",
        "  labels_test[...] = column + '_' + labels_test[...]\n",
        "\n",
        "\n",
        "  #convert sparse array to pandas dataframe with column labels\n",
        "#   df_train_cat = pd.DataFrame(cat_train_1hot.toarray(), columns=labels_train)\n",
        "  df_test_cat = pd.DataFrame(cat_test_1hot.toarray(), columns=labels_test)\n",
        "  \n",
        "#   #add a missing column to train if it's not present\n",
        "#   if column + '__missing' not in df_train_cat.columns:\n",
        "#     missingcolumn = pd.DataFrame(0, index=np.arange(df_train_cat.shape[0]), columns=[column+'__missing'])\n",
        "#     df_train_cat = pd.concat([missingcolumn, df_train_cat], axis=1)\n",
        "\n",
        "\n",
        "  #Get missing columns in test set that are present in training set\n",
        "  missing_cols = set( textcolumns ) - set( df_test_cat.columns )\n",
        "  \n",
        "  #Add a missing column in test set with default value equal to 0\n",
        "  for c in missing_cols:\n",
        "      df_test_cat[c] = 0\n",
        "  \n",
        "  #Ensure the order of column in the test set is in the same order than in train set\n",
        "  #Note this also removes categories in test set that aren't present in training set\n",
        "  df_test_cat = df_test_cat[textcolumns]\n",
        "\n",
        "\n",
        "  #concatinate the sparse set with the rest of our training data\n",
        "#   mdf_train = pd.concat([df_train_cat, mdf_train], axis=1)\n",
        "  mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)\n",
        "\n",
        "\n",
        "  #delete original column from training data\n",
        "#   del mdf_train[column]    \n",
        "  del mdf_test[column]\n",
        "  \n",
        "#   #create output of a list of the created column names\n",
        "#   labels_train = list(df_train_cat)\n",
        "#   textcolumns = labels_train\n",
        "  \n",
        "  #change data types to 8-bit (1 byte) integers for memory savings\n",
        "  for textcolumn in textcolumns:\n",
        "    mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)\n",
        "  \n",
        "\n",
        "  return mdf_test\n",
        "\n",
        "\n",
        "\n",
        "#postprocess_time_class(mdf_test, column, datecolumns, timenormalization_dict)\n",
        "#postprocess test column with of date category\n",
        "#takes as arguement pandas dataframe containing test data \n",
        "#(mdf_test), the name of the column string ('column'), and the timenormalization_dict \n",
        "#from the original application of automunge to the associated date column from train set\n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with distinct columns for year, month, day, hour, minute, second\n",
        "#each normalized to the mean and std from original train set, \n",
        "#with missing values plugged with the mean\n",
        "#with columns named after column_ + time category\n",
        "#returns two transformed dataframe (mdf_train, mdf_test)\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "def postprocess_time_class(mdf_test, column, datecolumns, timenormalization_dict):\n",
        "  \n",
        "  #add a second column with boolean expression indicating a missing cell\n",
        "  #(using NArows(.) function defined below, column name will be column+'_NArows')\n",
        "  NArows_nmbr_test = NArows(mdf_test, column, 'nmbr')\n",
        "  mdf_test[column + '_NArw'] = NArows_nmbr_test.copy()\n",
        "  del NArows_nmbr_test\n",
        "  \n",
        "  #change NArows data type to 8-bit (1 byte) integers for memory savings\n",
        "  mdf_test[column + '_NArw'] = mdf_test[column + '_NArw'].astype(np.int8)\n",
        "  \n",
        "  \n",
        "  #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data\n",
        "#   mdf_train[column] = pd.to_datetime(mdf_train[column], errors = 'coerce')\n",
        "  mdf_test[column] = pd.to_datetime(mdf_test[column], errors = 'coerce')\n",
        "  \n",
        "  #mdf_train[column].replace(-np.Inf, np.nan)\n",
        "  #mdf_test[column].replace(-np.Inf, np.nan)\n",
        "  \n",
        "  #get mean of various categories of datetime objects to use to plug in missing cells\n",
        "#   meanyear = mdf_train[column].dt.year.mean()    \n",
        "#   meanmonth = mdf_train[column].dt.month.mean()\n",
        "#   meanday = mdf_train[column].dt.day.mean()\n",
        "#   meanhour = mdf_train[column].dt.hour.mean()\n",
        "#   meanminute = mdf_train[column].dt.minute.mean()\n",
        "#   meansecond = mdf_train[column].dt.second.mean()\n",
        "  \n",
        "  meanyear = timenormalization_dict['meanyear']\n",
        "  meanmonth = timenormalization_dict['meanmonth']\n",
        "  meanday = timenormalization_dict['meanday']\n",
        "  meanhour = timenormalization_dict['meanhour']\n",
        "  meanminute = timenormalization_dict['meanminute']\n",
        "  meansecond = timenormalization_dict['meansecond']\n",
        "  \n",
        "\n",
        "  #get standard deviation of training data\n",
        "#   stdyear = mdf_train[column].dt.year.std()  \n",
        "#   stdmonth = mdf_train[column].dt.month.std()\n",
        "#   stdday = mdf_train[column].dt.day.std()\n",
        "#   stdhour = mdf_train[column].dt.hour.std()\n",
        "#   stdminute = mdf_train[column].dt.minute.std()\n",
        "#   stdsecond = mdf_train[column].dt.second.std()\n",
        "  \n",
        "  stdyear = timenormalization_dict['stdyear']\n",
        "  stdmonth = timenormalization_dict['stdmonth']\n",
        "  stdday = timenormalization_dict['stdday']\n",
        "  stdhour = timenormalization_dict['stdhour']\n",
        "  stdminute = timenormalization_dict['stdminute']\n",
        "  stdsecond = timenormalization_dict['stdsecond']\n",
        "  \n",
        "  \n",
        "#   #create new columns for each category in train set\n",
        "#   mdf_train[column + '_year'] = mdf_train[column].dt.year\n",
        "#   mdf_train[column + '_month'] = mdf_train[column].dt.month\n",
        "#   mdf_train[column + '_day'] = mdf_train[column].dt.day\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column].dt.hour\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column].dt.minute\n",
        "#   mdf_train[column + '_second'] = mdf_train[column].dt.second\n",
        "  \n",
        "  #create new columns for each category in test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column].dt.year\n",
        "  mdf_test[column + '_month'] = mdf_test[column].dt.month\n",
        "  mdf_test[column + '_day'] = mdf_test[column].dt.day\n",
        "  mdf_test[column + '_hour'] = mdf_test[column].dt.hour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column].dt.minute \n",
        "  mdf_test[column + '_second'] = mdf_test[column].dt.second\n",
        "  \n",
        "\n",
        "#   #replace missing data with training set mean\n",
        "#   mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(meanyear)\n",
        "#   mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(meanmonth)\n",
        "#   mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(meanday)\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(meanhour)\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(meanminute)\n",
        "#   mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #do same for test set (replace missing data with training set mean)\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(meanyear)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(meanmonth)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(meanday)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(meanhour)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(meanminute)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #subtract mean from column for both train and test\n",
        "#   mdf_train[column + '_year'] = mdf_train[column + '_year'] - meanyear\n",
        "#   mdf_train[column + '_month'] = mdf_train[column + '_month'] - meanmonth\n",
        "#   mdf_train[column + '_day'] = mdf_train[column + '_day'] - meanday\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column + '_hour'] - meanhour\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column + '_minute'] - meanminute\n",
        "#   mdf_train[column + '_second'] = mdf_train[column + '_second'] - meansecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] - meanyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] - meanmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] - meanday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] - meanhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] - meanminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] - meansecond\n",
        "  \n",
        "  \n",
        "  #divide column values by std for both training and test data\n",
        "#   mdf_train[column + '_year'] = mdf_train[column + '_year'] / stdyear\n",
        "#   mdf_train[column + '_month'] = mdf_train[column + '_month'] / stdmonth\n",
        "#   mdf_train[column + '_day'] = mdf_train[column + '_day'] / stdday\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column + '_hour'] / stdhour\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column + '_minute'] / stdminute\n",
        "#   mdf_train[column + '_second'] = mdf_train[column + '_second'] / stdsecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] / stdyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] / stdmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] / stdday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] / stdhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] / stdminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] / stdsecond\n",
        "  \n",
        "  \n",
        "  #now replace NaN with 0\n",
        "#   mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(0)\n",
        "#   mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(0)\n",
        "#   mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(0)\n",
        "#   mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(0)\n",
        "#   mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(0)\n",
        "#   mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(0)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(0)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(0)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(0)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(0)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(0)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(0)\n",
        "  \n",
        "    \n",
        "#   #output of a list of the created column names\n",
        "#   datecolumns = [column + '_year', column + '_month', column + '_day', \\\n",
        "#                 column + '_hour', column + '_minute', column + '_second']\n",
        "  \n",
        "  #this is to address an issue I found when parsing columns with only time no date\n",
        "  #which returned -inf vlaues, so if an issue will just delete the associated \n",
        "  #column along with the entry in datecolumns\n",
        "#   checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])\n",
        "#   if checkyear:\n",
        "#     del mdf_train[column + '_year']\n",
        "#     if column + '_year' in mdf_test.columns:\n",
        "#       del mdf_test[column + '_year']\n",
        "\n",
        "#   checkmonth = np.isinf(mdf_train.iloc[0][column + '_month'])\n",
        "#   if checkmonth:\n",
        "#     del mdf_train[column + '_month']\n",
        "#     if column + '_month' in mdf_test.columns:\n",
        "#       del mdf_test[column + '_month']\n",
        "\n",
        "#   checkday = np.isinf(mdf_train.iloc[0][column + '_day'])\n",
        "#   if checkmonth:\n",
        "#     del mdf_train[column + '_day']\n",
        "#     if column + '_day' in mdf_test.columns:\n",
        "#       del mdf_test[column + '_day']\n",
        "\n",
        "  #instead we'll just delete a column from test set if not found in train set\n",
        "  if column + '_year' not in datecolumns:\n",
        "    del mdf_test[column + '_year']\n",
        "#     datecolumns.remove(column + '_year')\n",
        "  if column + '_month' not in datecolumns:\n",
        "    del mdf_test[column + '_month'] \n",
        "#     datecolumns.remove(column + '_month')\n",
        "  if column + '_day' not in datecolumns:\n",
        "    del mdf_test[column + '_day']  \n",
        "#     datecolumns.remove(column + '_day')\n",
        "  if column + '_hour' not in datecolumns:\n",
        "    del mdf_test[column + '_hour']\n",
        "#     datecolumns.remove(column + '_hour')\n",
        "  if column + '_minute' not in datecolumns:\n",
        "    del mdf_test[column + '_minute'] \n",
        "#     datecolumns.remove(column + '_minute')\n",
        "  if column + '_second' not in datecolumns:\n",
        "    del mdf_test[column + '_second'] \n",
        "#     datecolumns.remove(column + '_second')\n",
        "  \n",
        "  \n",
        "  #delete original column from training data\n",
        "  if column in mdf_test.columns:\n",
        "    del mdf_test[column]  \n",
        "\n",
        "  \n",
        "#   #output a dictionary of the associated column mean and std\n",
        "  \n",
        "#   timenormalization_dict = {'meanyear' : meanyear, 'meanmonth' : meanmonth, \\\n",
        "#                             'meanday' : meanday, 'meanhour' : meanhour, \\\n",
        "#                             'meanminute' : meanminute, 'meansecond' : meansecond,\\\n",
        "#                             'stdyear' : stdyear, 'stdmonth' : stdmonth, \\\n",
        "#                             'stdday' : stdday, 'stdhour' : stdhour, \\\n",
        "#                             'stdminute' : stdminute, 'stdsecond' : stdsecond}\n",
        "  \n",
        "  \n",
        "  return mdf_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lv1J5tknpULt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 7) update postprocess ML infill functions for test set"
      ]
    },
    {
      "metadata": {
        "id": "BqJWyALDkxLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#createpostMLinfillsets(df_test, column, testNArows, category, \\\n",
        "#columnslist = []) function that when fed dataframe of\n",
        "#test set, column id, df of True/False corresponding to rows from original \\\n",
        "#sets with missing values, a string category of 'text', 'date', 'nmbr', or \\\n",
        "#'bnry', and a list of column id's for the text category if applicable. The \\\n",
        "#function returns a series of dataframes which can be applied to apply a \\\n",
        "#machine learning model previously trained on our train set as part of the \n",
        "#original automunge application to predict apppropriate infill values for those\\\n",
        "#points that had missing values from the original sets, returning the dataframe\\\n",
        "#df_test_fillfeatures\n",
        "\n",
        "def createpostMLinfillsets(df_test, column, testNArows, category, \\\n",
        "                           columnslist = [], categorylist = []):\n",
        "\n",
        "  if category in ['nmbr', 'bxcx', 'bnry', 'text']:\n",
        "    \n",
        "    #if this is a single column set (not categorical)\n",
        "    if categorylist == []:\n",
        "      \n",
        "      #first concatinate the NArows True/False designations to df_train & df_test\n",
        "#       df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "      df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "\n",
        "#       #create copy of df_train to serve as training set for fill\n",
        "#       df_train_filltrain = df_train.copy()\n",
        "#       #now delete rows coresponding to True\n",
        "#       df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns.get_values()[0]] == False]\n",
        "\n",
        "#       #now delete columns = columnslist and the NA labels (orig column+'_NArows') from this df\n",
        "#       df_train_filltrain = df_train_filltrain.drop(columnslist, axis=1)\n",
        "#       df_train_filltrain = df_train_filltrain.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "      \n",
        "      \n",
        "#       #create a copy of df_train[column] for fill train labels\n",
        "#       df_train_filllabel = pd.DataFrame(df_train[column].copy())\n",
        "#       #concatinate with the NArows\n",
        "#       df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "#       #drop rows corresponding to True\n",
        "#       df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns.get_values()[0]] == False]\n",
        "\n",
        "#       #delete the NArows column\n",
        "#       df_train_filllabel = df_train_filllabel.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "#       #create features df_train for rows needing infill\n",
        "#       #create copy of df_train (note it already has NArows included)\n",
        "#       df_train_fillfeatures = df_train.copy()\n",
        "#       #delete rows coresponding to False\n",
        "#       df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns.get_values()[0]])]\n",
        "#       #delete columnslist and column+'_NArows'\n",
        "#       df_train_fillfeatures = df_train_fillfeatures.drop(columnslist, axis=1)\n",
        "#       df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "  \n",
        "  \n",
        "      #create features df_test for rows needing infill\n",
        "      #create copy of df_test (note it already has NArows included)\n",
        "      df_test_fillfeatures = df_test.copy()\n",
        "      #delete rows coresponding to False\n",
        "      df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns.get_values()[0]])]\n",
        "      #delete column and column+'_NArows'\n",
        "      df_test_fillfeatures = df_test_fillfeatures.drop(columnslist, axis=1)\n",
        "      df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "      #delete NArows from df_train, df_test\n",
        "#       df_train = df_train.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "      df_test = df_test.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "  \n",
        "    #else if categorylist wasn't empty\n",
        "    else:\n",
        "      \n",
        "      #create a list of columns representing columnslist exlucding elements from\n",
        "      #categorylist\n",
        "      noncategorylist = columnslist[:]\n",
        "      #this removes categorylist elements from noncategorylist\n",
        "      noncategorylist = list(set(noncategorylist).difference(set(categorylist)))\n",
        "      \n",
        "      \n",
        "      #first concatinate the NArows True/False designations to df_train & df_test\n",
        "#       df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "      df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "\n",
        "#       #create copy of df_train to serve as training set for fill\n",
        "#       df_train_filltrain = df_train.copy()\n",
        "#       #now delete rows coresponding to True\n",
        "#       df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns.get_values()[0]] == False]\n",
        "\n",
        "#       #now delete columns = columnslist and the NA labels (orig column+'_NArows') from this df\n",
        "#       df_train_filltrain = df_train_filltrain.drop(columnslist, axis=1)\n",
        "#       df_train_filltrain = df_train_filltrain.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "      \n",
        "      \n",
        "#       #create a copy of df_train[columnslist] for fill train labels\n",
        "#       df_train_filllabel = df_train[columnslist].copy()\n",
        "#       #concatinate with the NArows\n",
        "#       df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "#       #drop rows corresponding to True\n",
        "#       df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns.get_values()[0]] == False]\n",
        "      \n",
        "#       #now delete columns = noncategorylist from this df\n",
        "#       df_train_filltrain = df_train_filltrain.drop(noncategorylist, axis=1)\n",
        "      \n",
        "#       #delete the NArows column\n",
        "#       df_train_filllabel = df_train_filllabel.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "      \n",
        "      \n",
        "#       #create features df_train for rows needing infill\n",
        "#       #create copy of df_train (note it already has NArows included)\n",
        "#       df_train_fillfeatures = df_train.copy()\n",
        "#       #delete rows coresponding to False\n",
        "#       df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns.get_values()[0]])]\n",
        "#       #delete columnslist and column+'_NArows'\n",
        "#       df_train_fillfeatures = df_train_fillfeatures.drop(columnslist, axis=1)\n",
        "#       df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "      \n",
        "      \n",
        "      #create features df_test for rows needing infill\n",
        "      #create copy of df_test (note it already has NArows included)\n",
        "      df_test_fillfeatures = df_test.copy()\n",
        "      #delete rows coresponding to False\n",
        "      df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns.get_values()[0]])]\n",
        "      #delete column and column+'_NArows'\n",
        "      df_test_fillfeatures = df_test_fillfeatures.drop(columnslist, axis=1)\n",
        "      df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "      #delete NArows from df_train, df_test\n",
        "#       df_train = df_train.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "      df_test = df_test.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "      \n",
        "  if category == 'date':\n",
        "    \n",
        "    #create empty sets for now\n",
        "    #an extension of this method would be to implement a comparable method \\\n",
        "    #for the time category, based on the columns output from the preprocessing\n",
        "#     df_train_filltrain = pd.DataFrame({'foo' : []}) \n",
        "#     df_train_filllabel = pd.DataFrame({'foo' : []})\n",
        "#     df_train_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "    df_test_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "  \n",
        "  \n",
        "  return df_test_fillfeatures\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbC327kRH3bV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#predictpostinfill(category, model, df_test_fillfeatures, \\\n",
        "#columnslist = []), function that takes as input \\\n",
        "#a category string, a model trained as part of automunge on the coresponding \\\n",
        "#column from the train set, the output of createpostMLinfillsets(.), a seed \\\n",
        "#for randomness, and a list of columns \\\n",
        "#produced by a text class preprocessor when applicable and returns \\\n",
        "#predicted infills for the test feature sets as df_testinfill based on \\\n",
        "#derivations using scikit-learn, with the lenth of \\\n",
        "#infill consistent with the number of True values from NArows\n",
        "\n",
        "\n",
        "#imports for numerical class training\n",
        "#from sklearn.linear_model import LinearRegression\n",
        "#from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "#from sklearn.linear_model import Ridge\n",
        "#from sklearn.linear_model import RidgeCV\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "#imports for binary and text class training\n",
        "from sklearn import preprocessing\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "def predictpostinfill(category, model, df_test_fillfeatures, \\\n",
        "                      columnslist = []):\n",
        "  \n",
        "  \n",
        "  #a reasonable extension of this funciton would be to allow ML inference with \\\n",
        "  #other ML architectures such a SVM or something SGD based for instance\n",
        "  \n",
        "  #convert dataframes to numpy arrays\n",
        "#   np_train_filltrain = df_train_filltrain.values\n",
        "#   np_train_filllabel = df_train_filllabel.values\n",
        "#   np_train_fillfeatures = df_train_fillfeatures.values\n",
        "  np_test_fillfeatures = df_test_fillfeatures.values\n",
        "  \n",
        "  #ony run the following if we have any rows needing infill\n",
        "#   if df_train_fillfeatures.shape[0] > 0:\n",
        "  #since we don't have df_train_fillfeatures to work with we'll look at the \n",
        "  #model which will be set to False if there was no infill model trained\n",
        "  #if model[0] != False:\n",
        "  if model != False:\n",
        "\n",
        "    if category == 'nmbr':\n",
        "\n",
        "#       #train linear regression model using scikit-learn for numerical prediction\n",
        "#       #model = LinearRegression()\n",
        "#       #model = PassiveAggressiveRegressor(random_state = randomseed)\n",
        "#       #model = Ridge(random_state = randomseed)\n",
        "#       #model = RidgeCV()\n",
        "#       #note that SVR doesn't have an argument for random_state\n",
        "#       model = SVR()\n",
        "#       model.fit(np_train_filltrain, np_train_filllabel)    \n",
        "      \n",
        "      \n",
        "#       #predict infill values\n",
        "#       np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "#       df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "#       print('category is nmbr, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "  \n",
        "  \n",
        "  \n",
        "    if category == 'bxcx':\n",
        "\n",
        "      \n",
        "#       model = SVR()\n",
        "#       model.fit(np_train_filltrain, np_train_filllabel)   \n",
        "      \n",
        "#       #predict infill values\n",
        "#       np_traininfill = model.predict(np_train_fillfeatures)\n",
        "\n",
        "      \n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "#       df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])     \n",
        "\n",
        "  \n",
        "    if category == 'bnry':\n",
        "\n",
        "#       #train logistic regression model using scikit-learn for binary classifier\n",
        "#       #model = LogisticRegression()\n",
        "#       #model = LogisticRegression(random_state = randomseed)\n",
        "#       #model = SGDClassifier(random_state = randomseed)\n",
        "#       model = SVC(random_state = randomseed)\n",
        "      \n",
        "#       model.fit(np_train_filltrain, np_train_filllabel)\n",
        "\n",
        "#       #predict infill values\n",
        "#       np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "#       df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "\n",
        "    if category == 'text':\n",
        "\n",
        "#       #first convert the one-hot encoded set via argmax to a 1D array\n",
        "#       np_train_filllabel_argmax = np.argmax(np_train_filllabel, axis=1)\n",
        "\n",
        "#       #train logistic regression model using scikit-learn for binary classifier\n",
        "#       #with multi_class argument activated\n",
        "#       #model = LogisticRegression()\n",
        "#       #model = SGDClassifier(random_state = randomseed)\n",
        "#       model = SVC(random_state = randomseed)\n",
        "      \n",
        "#       model.fit(np_train_filltrain, np_train_filllabel_argmax)\n",
        "\n",
        "#       #predict infill values\n",
        "#       np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        #this needs to have same number of columns as text category\n",
        "        np_testinfill = np.zeros(shape=(1,len(columnslist)))\n",
        "\n",
        "      #convert the 1D arrary back to one hot encoding\n",
        "#       labelbinarizertrain = preprocessing.LabelBinarizer()\n",
        "#       labelbinarizertrain.fit(np_traininfill)\n",
        "#       np_traininfill = labelbinarizertrain.transform(np_traininfill)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        labelbinarizertest = preprocessing.LabelBinarizer()\n",
        "        labelbinarizertest.fit(np_testinfill)\n",
        "        np_testinfill = labelbinarizertest.transform(np_testinfill)\n",
        "\n",
        "\n",
        "\n",
        "      #run function to ensure correct dimensions of re-encoded classifier array\n",
        "#       np_traininfill = labelbinarizercorrect(np_traininfill, columnslist)\n",
        "      \n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = labelbinarizercorrect(np_testinfill, columnslist)\n",
        "\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "#       df_traininfill = pd.DataFrame(np_traininfill, columns = [columnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [columnslist]) \n",
        "\n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'date':\n",
        "\n",
        "      #create empty sets for now\n",
        "      #an extension of this method would be to implement a comparable infill \\\n",
        "      #method for the time category, based on the columns output from the \\\n",
        "      #preprocessing\n",
        "#       df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "      \n",
        "#       model = False\n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "  \n",
        "  \n",
        "  #else if we didn't have any infill rows let's create some plug values\n",
        "  else:\n",
        "    \n",
        "    if category == 'text':\n",
        "#       np_traininfill = np.zeros(shape=(1,len(columnslist)))\n",
        "      np_testinfill = np.zeros(shape=(1,len(columnslist)))\n",
        "#       df_traininfill = pd.DataFrame(np_traininfill, columns = [columnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [columnslist]) \n",
        "    \n",
        "    else :\n",
        "#       df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "  \n",
        "#     model = False\n",
        "  \n",
        "  return df_testinfill\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6_t5epfWLXwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 8) update postmunge function for subsequent test data"
      ]
    },
    {
      "metadata": {
        "id": "hNjRgoEuLf5G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#postmunge(df_test, testID_column, postprocess_dict) Function that when fed a \\\n",
        "#test data set coresponding to a previously processed train data set which was \\\n",
        "#processed using the automunge function automates the process \\\n",
        "#of evaluating each column for determination and applicaiton of appropriate \\\n",
        "#preprocessing. Takes as arguement pandas dataframes of test data \\\n",
        "#(mdf_test), a string identifying the ID column for test (testID_column), a \\\n",
        "#dictionary containing keys for the processing which had been generated by the \\\n",
        "#original processing of the coresponding train set using automunge function. \\\n",
        "#Returns following sets as numpy arrays: \n",
        "#test, testID, labelsencoding_dict, finalcolumns_test\n",
        "\n",
        "#Requires consistent column naming and order as original train set pre \\\n",
        "#application of automunge. Requires postprocess_dict from original applicaiton. \\\n",
        "#Currently assumes coinbsistent columns carved out from application of munging \\\n",
        "#from original automunge, a potential future extension is to allow for additional \\\n",
        "#columns to be excluded from processing.\n",
        "\n",
        "\n",
        "\n",
        "def postmunge(postprocess_dict, df_test, testID_column = False):\n",
        "  \n",
        "  \n",
        "  #my understanding is it is good practice to convert any None values into NaN \\\n",
        "  #so I'll just get that out of the way\n",
        "  df_test.fillna(value=float('nan'), inplace=True)\n",
        "  \n",
        "  #extract the ID columns from test set\n",
        "  if testID_column != False:\n",
        "    df_testID = pd.DataFrame(df_test[testID_column])\n",
        "    del df_test[testID_column]\n",
        "  \n",
        "  #confirm consistency of train an test sets\n",
        "  \n",
        "  #check number of columns is consistent\n",
        "  if len(postprocess_dict['origtraincolumns'])!= df_test.shape[1]:\n",
        "    print(\"error, different number of original columns in train and test sets\")\n",
        "    return\n",
        "  \n",
        "  #check column headers are consistent (this works independent of order)\n",
        "  columns_train_set = set(postprocess_dict['origtraincolumns'])\n",
        "  columns_test_set = set(list(df_test))\n",
        "  if columns_train_set != columns_test_set:\n",
        "    print(\"error, different column labels in the train and test set\")\n",
        "    return\n",
        "  \n",
        "  #check order of column headers are consistent\n",
        "  columns_train = postprocess_dict['origtraincolumns']\n",
        "  columns_test = list(df_test)\n",
        "  if columns_train != columns_test:\n",
        "    print(\"error, different order of column labels in the train and test set\")\n",
        "    return\n",
        "  \n",
        "  #create an empty dataframe to serve as a store for each column's NArows\n",
        "  #the column id's for this df will follow convention from NArows of \n",
        "  #column+'_NArows' for each column in columns_train\n",
        "  #these are used in the ML infill methods\n",
        "  #masterNArows_train = pd.DataFrame()\n",
        "  masterNArows_test = pd.DataFrame()\n",
        "\n",
        "    \n",
        "  #For each column, determine appropriate processing function\n",
        "  #processing function will be based on evaluation of train set\n",
        "  for column in columns_train:\n",
        "\n",
        "    \n",
        "    #we're only going to process columns that weren't in our excluded set\n",
        "    #note a foreseeable workflow would be for there to be additional\\\n",
        "    #columns desired for exclusion in post processing, consider adding\\\n",
        "    #additional excluded columns as future extensionl\n",
        "    if column not in postprocess_dict['excludetransformscolumns']:\n",
        "\n",
        " \n",
        "      category = evalcategory(df_test, column)\n",
        "\n",
        "\n",
        "      \n",
        "      #ok postprocess_dict stores column data by the key of column names after\\\n",
        "      #they have gone through our pre-processing functions, which means the \\\n",
        "      #column will match for categories of 'nmbr' and 'bnry', but for the \\\n",
        "      #categories of 'date' and 'text' the act of processing will have \\\n",
        "      #created new columns and deleted the original column - so since we are \\\n",
        "      #currently walking through the original column names we'll need to \\\n",
        "      #pull a post-process column name for the associated columns to serve as \\\n",
        "      #a key for our postprocess_dict which we'll call columnkey. Also the  \\\n",
        "      #original category from train set (traincategory) will be accessed to \\\n",
        "      #serve as a check for consistency between train and test sets.\n",
        "      traincategory = False\n",
        "      \n",
        "      for postprocesscolumn in postprocess_dict['finalcolumns_train']:\n",
        "        \n",
        "        \n",
        "        \n",
        "        if postprocess_dict['column_dict'][postprocesscolumn]['category'] == 'text':\n",
        "          if column == postprocess_dict['column_dict'][postprocesscolumn]['origcolumn'] \\\n",
        "          and postprocesscolumn[-5:] != '_NArw':\n",
        "            traincategory = 'text'\n",
        "            columnkey = postprocesscolumn\n",
        "            break\n",
        "            \n",
        "        elif postprocess_dict['column_dict'][postprocesscolumn]['category'] == 'date':\n",
        "          if column == postprocess_dict['column_dict'][postprocesscolumn]['origcolumn'] \\\n",
        "          and postprocesscolumn[-5:] != '_NArw':\n",
        "            traincategory = 'date'\n",
        "            columnkey = postprocesscolumn\n",
        "            break\n",
        "            \n",
        "        elif postprocess_dict['column_dict'][postprocesscolumn]['category'] == 'bxcx':\n",
        "          if column == postprocess_dict['column_dict'][postprocesscolumn]['origcolumn'] \\\n",
        "          and postprocesscolumn[-5:] != '_NArw':\n",
        "            traincategory = 'bxcx'\n",
        "            columnkey = postprocesscolumn\n",
        "            break\n",
        "            \n",
        "        elif postprocess_dict['column_dict'][postprocesscolumn]['category'] == 'bnry':\n",
        "          if column == postprocess_dict['column_dict'][postprocesscolumn]['origcolumn'] \\\n",
        "          and postprocesscolumn[-5:] != '_NArw':\n",
        "            traincategory = 'bnry'\n",
        "            columnkey = postprocesscolumn\n",
        "            break\n",
        "            \n",
        "        elif postprocess_dict['column_dict'][postprocesscolumn]['category'] == 'nmbr':\n",
        "          if column == postprocess_dict['column_dict'][postprocesscolumn]['origcolumn'] \\\n",
        "          and postprocesscolumn[-5:] != '_NArw':\n",
        "            traincategory = 'nmbr'\n",
        "            columnkey = postprocesscolumn\n",
        "            break\n",
        "        \n",
        "        elif traincategory == False:\n",
        "          traincategory = 'null'\n",
        "          break\n",
        "#         elif postprocess_dict['column_dict'][postprocesscolumn]['category'] == 'null':\n",
        "#           if column == postprocess_dict['column_dict'][postprocesscolumn]['origcolumn'] \\\n",
        "#           and postprocesscolumn[-5:] != '_NArw':\n",
        "#             traincategory = 'null'\n",
        "#             columnkey = postprocesscolumn \n",
        "#             break\n",
        "            \n",
        "\n",
        "\n",
        "      \n",
        "      #for the special case of train category = bxcx and test category = nmbr\n",
        "      #(meaning there were no negative values in train but there were in test)\n",
        "      #we'll resolve by clipping all test values that were <0.001 and setting to \n",
        "      #NaN then resetting the test category to bxcx to be consistent with train\n",
        "      if traincategory == 'bxcx' and category == 'nmbr':\n",
        "        \n",
        "        #convert all values to either numeric or NaN\n",
        "        df_test[column] = pd.to_numeric(df_test[column], errors='coerce')\n",
        "        \n",
        "        \n",
        "        df_test[column] = df_test[column].mask(df_test[column] < 0.001)\n",
        "        category = 'bxcx'\n",
        "        print('Note that values < 0.001 found in test set were reset to NaN')\n",
        "        print('to allow consistent box-cox transform as train set.')\n",
        "      \n",
        "      #one more special case, if train category is nmbr and test category is bxcx\n",
        "      #default test category to nmbr\n",
        "      if traincategory == 'nmbr' and category == 'bxcx':\n",
        "        category = 'nmbr'\n",
        "      \n",
        "      \n",
        "      #let's make sure the category is consistent between train and test sets\n",
        "      if category != traincategory:\n",
        "        print('error - different category between train and test sets for column ',\\\n",
        "              column)\n",
        "      \n",
        "      \n",
        "      #here we'll delete any columns that returned a 'null' category\n",
        "      if category == 'null':\n",
        "        df_test = df_test.drop([column], axis=1)\n",
        "        \n",
        "      #so if we didn't delete the column let's proceed\n",
        "      else:\n",
        "        \n",
        "        #create NArows (column of True/False where True coresponds to missing data)\n",
        "        testNArows = NArows(df_test, column, category)\n",
        "        \n",
        "        #now append that NArows onto a master NA rows df\n",
        "        masterNArows_test = pd.concat([masterNArows_test, testNArows], axis=1)\n",
        "\n",
        "        #(now normalize as would normally)\n",
        "\n",
        "        #for binary class use the train majority field for missing plug value\n",
        "        if category == 'bnry':\n",
        "          binary_missing_plug = postprocess_dict['column_dict'][columnkey]['normalization_dict']['missing']\n",
        "          \n",
        "        #apply appropriate processing function to this column based on the result\n",
        "        #original bnry processing function still works since only had one df input\n",
        "        if category == 'bnry':\n",
        "          df_test, _1, _2 = process_binary_class(df_test, column, binary_missing_plug)\n",
        "          \n",
        "        #for nmbr category process test set with function postprocess_numerical_class\n",
        "        if category == 'nmbr':\n",
        "          df_test = postprocess_numerical_class(df_test, column, \\\n",
        "                                                postprocess_dict['column_dict'][columnkey]['normalization_dict']['mean'], \\\n",
        "                                                postprocess_dict['column_dict'][columnkey]['normalization_dict']['std'])\n",
        "        \n",
        "        #for bxcx category processing\n",
        "        if category == 'bxcx':          \n",
        "          \n",
        "          \n",
        "          df_test, nmbrcolumns, nmbrnormalization_dict, _ = \\\n",
        "          process_bxcx_class(df_test, column, bxcx_lmbda = \\\n",
        "                             postprocess_dict['column_dict'][columnkey]['normalization_dict']['bxcx_lmbda'], \\\n",
        "                             trnsfrm_mean = postprocess_dict['column_dict'][columnkey]['normalization_dict']['trnsfrm_mean'], \\\n",
        "                             trnsfrm_std = postprocess_dict['column_dict'][columnkey]['normalization_dict']['trnsfrm_std'])\n",
        "\n",
        "        \n",
        "        #for text category process test set with function postprocess_text_class\n",
        "        if category == 'text':\n",
        "\n",
        "          \n",
        "          df_test = postprocess_text_class(df_test, column, postprocess_dict['column_dict'][columnkey]['columnslist'])\n",
        "        \n",
        "        #for date category process test set with function postprocess_date_class\n",
        "        if category == 'date':\n",
        "          df_test = postprocess_time_class(df_test, column, \\\n",
        "                                           postprocess_dict['column_dict'][columnkey]['columnslist'], \\\n",
        "                                           postprocess_dict['column_dict'][columnkey]['normalization_dict'])\n",
        "          \n",
        "        \n",
        "  #now that we've pre-processed all of the columns, let's run through them again\\\n",
        "  #using ML to derive plug values for the previously missing cells\n",
        "  \n",
        "  #if MLinfill == True\n",
        "  if postprocess_dict['MLinfill'] == True:\n",
        "    \n",
        "    \n",
        "    #now let's create a list of columns just like we did in automunge\n",
        "    columns_test_ML = list(df_test)\n",
        "    \n",
        "    iteration = 0\n",
        "    #while iteration < infilliterate:\n",
        "    while iteration < postprocess_dict['infilliterate']:\n",
        "      \n",
        "          \n",
        "      #since we're reusing the text_dict and date_dict from our original automunge\n",
        "      #we're going to need to re-initialize the infillcomplete markers\n",
        "      #actually come to this of it we need to go back to automunge and do this\n",
        "      #for the MLinfill iterations as well\n",
        "      \n",
        "      #re-initialize the infillcomplete marker in column _dict's\n",
        "      for key in postprocess_dict['column_dict']:\n",
        "        postprocess_dict['column_dict'][key]['infillcomplete'] = False\n",
        "\n",
        "      \n",
        "      \n",
        "      for column in columns_test_ML:\n",
        "\n",
        "        \n",
        "        #we're only going to process columns that weren't in our excluded set\n",
        "        #if column not in excludetransformscolumns:\n",
        "        if column not in postprocess_dict['excludetransformscolumns'] \\\n",
        "        and column[-5:] != '_NArw':\n",
        "          \n",
        "          #If column id is found in the text_dict then will require different \\\n",
        "          #type of address since this category won't be found in our \\\n",
        "          #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "          #collective group of columns from the associated textcolumns array.\n",
        "          \n",
        "          #if column in text_dict:\n",
        "          #if column in postprocess_dict['text_dict']:\n",
        "          if column[-5:] == '_text':\n",
        "            \n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            #if postprocess_dict['text_dict'][column]['infillcomplete'] == False:\n",
        "            if postprocess_dict['column_dict'][column]['infillcomplete'] == False:\n",
        "              \n",
        "              #pull this column's textcolumns array\n",
        "              textcolumns = \\\n",
        "              postprocess_dict['column_dict'][column]['columnslist']\n",
        "              \n",
        "              category = postprocess_dict['column_dict'][column]['category']\n",
        "              origcolumn = postprocess_dict['column_dict'][column]['origcolumn']\n",
        "              columnslist = postprocess_dict['column_dict'][column]['columnslist']\n",
        "              categorylist = postprocess_dict['column_dict'][column]['categorylist']\n",
        "              \n",
        "              #now let's apply our functions for ML infill\n",
        "              \n",
        "              \n",
        "              df_test_fillfeatures = \\\n",
        "              createpostMLinfillsets(df_test, column, pd.DataFrame(masterNArows_test[origcolumn+'_NArows']), \\\n",
        "                                     category, columnslist = columnslist, \\\n",
        "                                     categorylist = categorylist)\n",
        "              \n",
        "              #predict infill values using defined function predictpostinfill(.)\n",
        "              df_testinfill = \\\n",
        "              predictpostinfill(category, postprocess_dict['column_dict'][column]['infillmodel'], \\\n",
        "                                df_test_fillfeatures, columnslist = columnslist)\n",
        "              \n",
        "              #it's a quirk of the ML models that if we don't train the\n",
        "              #train set model on any features, that we won't be able to apply\n",
        "              #the model to predict the test set infill. \n",
        "              #For now we'll only use insertilnfill if we had\n",
        "              #some missing points in the train set, a future extension would be\n",
        "              #to update our createMLinfillsets and predictinfill to create \n",
        "              #some arbitrary features to train the infill on train set for\n",
        "              #cases where there are NaN values in test set but not in train\n",
        "              #such that we could insert infill for missing values in test set.              \n",
        "              \n",
        "              #if model != False:\n",
        "              if postprocess_dict[column]['infillmodel'] != False:\n",
        "                \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[origcolumn+'_NArows']), \\\n",
        "                                       columnslist = columnslist)\n",
        "              \n",
        "              #now change the infillcomplete marker in the text_dict for each \\\n",
        "              #associated text column\n",
        "              for textcolumnname in categorylist:\n",
        "                postprocess_dict['column_dict'][textcolumnname]['infillcomplete'] == True\n",
        "                                                    \n",
        "                \n",
        "          #If column id found in the bxcx_dict then we'll address with the\n",
        "          #box-cox methods\n",
        "          #elif column in postprocess_dict['bxcx_dict']:\n",
        "          if column[-5:] == '_bxcx':\n",
        "            \n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            if postprocess_dict['column_dict'][column]['infillcomplete'] == False:\n",
        "              \n",
        "              #pull this column's columnslist\n",
        "              #for the record I know this is a list not an array\n",
        "              nmbrcolumns = postprocess_dict['column_dict'][column]['columnslist']\n",
        "              categorylist = postprocess_dict['column_dict'][column]['categorylist']\n",
        "              \n",
        "              #grab the original column name prior to preprocessing\n",
        "              nmbrorigcolumn = postprocess_dict['column_dict'][column]['origcolumn']\n",
        "              \n",
        "              #now get the transformed column name for the two categories of nmbr and bxcx\n",
        "              column_bxcx = nmbrorigcolumn + '_bxcx'\n",
        "              category_bxcx = 'bxcx'\n",
        "              column_nmbr = nmbrorigcolumn + '_nmbr'\n",
        "              category_nmbr = 'nmbr'\n",
        "              \n",
        "              #now let's apply our functions for ML infill\n",
        "              \n",
        "              #createMLinfillsets\n",
        "              \n",
        "              df_test_fillfeatures = \\\n",
        "              createpostMLinfillsets(df_test, column, \\\n",
        "                                     pd.DataFrame(masterNArows_test[nmbrorigcolumn + '_NArows']), \\\n",
        "                                     category_bxcx, columnslist = nmbrcolumns, \\\n",
        "                                     categorylist = categorylist)\n",
        "                                     \n",
        "              #predict infill values using defined function predictinfill(.)\n",
        "              #first we'll address the bxcx transformed column\n",
        "\n",
        "\n",
        "\n",
        "              df_testinfill_bxcx = predictpostinfill(category_bxcx, \\\n",
        "                                                     postprocess_dict['column_dict'][column_bxcx]['infillmodel'], \\\n",
        "                                                     df_test_fillfeatures, columnslist = [])\n",
        "              \n",
        "              #now we'll address the nmbr column\n",
        "              df_testinfill_nmbr = predictpostinfill(category_nmbr, \\\n",
        "                                                     postprocess_dict['column_dict'][column_nmbr]['infillmodel'], \\\n",
        "                                                     df_test_fillfeatures, columnslist = [])\n",
        "              \n",
        "              #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "              #to df's associated column\n",
        "\n",
        "              #only use insertilnfill if we had some missing points in the train set\n",
        "              if postprocess_dict['column_dict'][column]['infillmodel'] != False:\n",
        "                \n",
        "                              \n",
        "                df_test = insertinfill(df_test, column_bxcx, df_testinfill_bxcx, \\\n",
        "                                       category_bxcx, \\\n",
        "                                       pd.DataFrame(masterNArows_test[nmbrorigcolumn+'_NArows']), \\\n",
        "                                       columnslist = [])\n",
        "                \n",
        "                \n",
        "                df_test = insertinfill(df_test, column_nmbr, df_testinfill_nmbr, \\\n",
        "                                       category_nmbr, \\\n",
        "                                       pd.DataFrame(masterNArows_test[nmbrorigcolumn+'_NArows']), \\\n",
        "                                       columnslist = [])\n",
        "\n",
        "              #now change the infillcomplete marker in the text_dict for each \\\n",
        "              #associated text column\n",
        "              for nmbrcolumnname in nmbrcolumns:\n",
        "                postprocess_dict['column_dict'][nmbrcolumnname]['infillcomplete'] == True\n",
        "                \n",
        "                \n",
        "          #If column id is found in the date_dict then will require different \\\n",
        "          #type of address since this category won't be found in our \\\n",
        "          #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "          #collective group of columns from the associated datecolumns array. \\\n",
        "          #The development of this address for date columns is a future extension.\n",
        "          #elif column in postprocess_dict['date_dict']:\n",
        "          elif column[-5:] == '_date':\n",
        "\n",
        "            #this too shall be a future extension.\n",
        "            pass\n",
        "          \n",
        "          #elif column in bnry_dict:\n",
        "          elif column[-5:] == '_bnry':\n",
        "          \n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            if postprocess_dict['column_dict'][column]['infillcomplete'] == False:\n",
        "            \n",
        "              #grab a few column values from postprocess_dict  \n",
        "              category = postprocess_dict['column_dict'][column]['category']\n",
        "              origcolumn = postprocess_dict['column_dict'][column]['origcolumn']\n",
        "              columnslist = postprocess_dict['column_dict'][column]['columnslist']\n",
        "              categorylist = postprocess_dict['column_dict'][column]['categorylist']\n",
        "\n",
        "              #create MLinfill set using defined function createpostMLinfillsets(.)\n",
        "              df_test_fillfeatures = \\\n",
        "              createpostMLinfillsets(df_test, column, pd.DataFrame(masterNArows_test[origcolumn + '_NArows']), \\\n",
        "                                     category, columnslist = columnslist, \\\n",
        "                                     categorylist = categorylist)\n",
        "              \n",
        "              #predict infill values using defined function predictpostinfill(.)\n",
        "              df_testinfill = \\\n",
        "              predictpostinfill(category, postprocess_dict['column_dict'][column]['infillmodel'], \\\n",
        "                                df_test_fillfeatures, columnslist = columnslist)\n",
        "              \n",
        "              #(only insert infill to test set if we had NA rows in train set)\n",
        "              #note comments above in text class for potential future expansion\n",
        "              if postprocess_dict['column_dict'][column]['infillmodel'] != False:\n",
        "                \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[origcolumn + '_NArows']), \\\n",
        "                                       columnslist = columnslist, \\\n",
        "                                       categorylist = categorylist)\n",
        "              \n",
        "          #elif column in nmbr_dict:\n",
        "          elif column[-5:] == '_nmbr':    \n",
        "          \n",
        "            #check the status of dictionary's infillcomplete marker for this column\n",
        "            if postprocess_dict['column_dict'][column]['infillcomplete'] == False:\n",
        "            \n",
        "              #grab a few column values from postprocess_dict  \n",
        "              category = postprocess_dict['column_dict'][column]['category']\n",
        "              origcolumn = postprocess_dict['column_dict'][column]['origcolumn']\n",
        "              columnslist = postprocess_dict['column_dict'][column]['columnslist']\n",
        "              categorylist = postprocess_dict['column_dict'][column]['categorylist']\n",
        "              \n",
        "              #create MLinfill set using defined function createpostMLinfillsets(.)\n",
        "              df_test_fillfeatures = \\\n",
        "              createpostMLinfillsets(df_test, column, pd.DataFrame(masterNArows_test[origcolumn + '_NArows']), \\\n",
        "                                     category, columnslist = columnslist, \\\n",
        "                                     categorylist = categorylist)\n",
        "              \n",
        "              #predict infill values using defined function predictpostinfill(.)\n",
        "              df_testinfill = \\\n",
        "              predictpostinfill(category, postprocess_dict['column_dict'][column]['infillmodel'], \\\n",
        "                                df_test_fillfeatures, columnslist = [])\n",
        "              \n",
        "              #(only insert infill to test set if we had NA rows in train set)\n",
        "              #note comments above in text class for potential future expansion\n",
        "              if postprocess_dict['column_dict'][column]['infillmodel'] != False:\n",
        "                \n",
        "                df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                       pd.DataFrame(masterNArows_test[origcolumn + '_NArows']), \\\n",
        "                                       columnslist = [])\n",
        "          \n",
        "          \n",
        "\n",
        "          \n",
        "      iteration += 1          \n",
        "\n",
        "  #here's a list of final column names saving here since the translation to \\\n",
        "  #numpy arrays scrubs the column names\n",
        "  finalcolumns_test = list(df_test)\n",
        "  \n",
        "  #global processing to test set including conversion to numpy array\n",
        "  np_test = df_test.values\n",
        "  \n",
        "  if testID_column != False:\n",
        "    np_testID = df_testID.values\n",
        "    testID = np_testID\n",
        "  else:\n",
        "    testID = []\n",
        "    \n",
        "  test = np_test\n",
        "  \n",
        "  labelsencoding_dict = postprocess_dict['labelsencoding_dict']\n",
        "  \n",
        "  \n",
        "  return test, testID, labelsencoding_dict, finalcolumns_test\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyThHKNQZOar",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9) Test our functions"
      ]
    },
    {
      "metadata": {
        "id": "ReCBHoBAY9wr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#create sample test and train data for demonstration purposes\n",
        "\n",
        "#train data set from list of dictionaries\n",
        "#24 rows\n",
        "# train = [{'ID' : 101, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "#          {'ID' : 102, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "#          {'ID' : 103, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "#          {'ID' : 104, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "#          {'ID' : 105, 'number': -1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "#          {'ID' : 106, 'number': 'Q', 'Y-N': 'N', 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "#          {'ID' : 107, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "#          {'ID' : 108, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "#          {'ID' : 109, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "#          {'ID' : 110, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "#          {'ID' : 111, 'number': -1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "#          {'ID' : 112, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "#          {'ID' : 113, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "#          {'ID' : 114, 'number': 2, 'Y-N': None, 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "#          {'ID' : 115, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "#          {'ID' : 116, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "#          {'ID' : 117, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "#          {'ID' : 118, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'}]\n",
        "\n",
        "#revised with all positive number column for power transform\n",
        "train = [{'ID' : 101, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 102, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 103, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 104, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 105, 'number': 1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 106, 'number': 'Q', 'Y-N': 'N', 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "         {'ID' : 107, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 108, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 109, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 110, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 111, 'number': 1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 112, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "         {'ID' : 113, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 114, 'number': 2, 'Y-N': None, 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 115, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 116, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 117, 'number': 1, 'Y-N': 'N', 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 118, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'}]\n",
        "\n",
        "#convert train data to pandas dataframe\n",
        "df_train = pd.DataFrame(train)\n",
        "\n",
        "\n",
        "#test data set from list of dictionaries\n",
        "#21 rows\n",
        "# test = [{'ID' : 1, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "#         {'ID': 2, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "#         {'ID' : 3,'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "#         {'ID' : 4, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "#         {'ID' : 5, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "#         {'ID' : 6, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "#         {'ID' : 7, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "#         {'ID' : 8, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "#         {'ID' : 9, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "#         {'ID' : 10, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "#         {'ID' : 11, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "#         {'ID' : 12, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "#         {'ID' : 13, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "#         {'ID' : 14, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "#         {'ID' : 15, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "#         {'ID' : 16, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "#         {'ID' : 17, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "#         {'ID' : 18, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "#         {'ID' : 19, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "#         {'ID' : 20, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "#         {'ID' : 21, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'}]\n",
        "\n",
        "test = [{'ID' : 1, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID': 2, 'number': 1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 3,'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 4, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 5, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 6, 'number': 10, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 7, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 8, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 9, 'number': 1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 10, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 11, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 12, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 13, 'number': 10, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 14, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 15, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 16, 'number': 1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 17, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 18, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 19, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 20, 'number': 10, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 21, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'}]\n",
        "\n",
        "#convert test data to pandas dataframe\n",
        "df_test = pd.DataFrame(test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bq4i5jCrZbu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe012754-a082-47d2-98a8-0f6191893aa3"
      },
      "cell_type": "code",
      "source": [
        "#apply automunge\n",
        "#this application is primarily to serve as a quick check for bugs prior to more\\\n",
        "#computationally expensive applicaitons in cases of some update to automunge\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(df_train, df_test, labels_column = 'label', trainID_column = 'ID', \\\n",
        "         testID_column = 'ID', powertransform = True, MLinfill = True, infilliterate=1, \\\n",
        "         randomseed = 42, excludetransformscolumns = [])\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  0.24109430099997553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x0rxR8KOAe2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "27b39d47-d66a-4d52-922a-c34d91a92731"
      },
      "cell_type": "code",
      "source": [
        "#validate the output\n",
        "print('train shape =            ', train.shape)\n",
        "print('trainID shape =          ', trainID.shape)\n",
        "print('labels shape =           ', labels.shape)\n",
        "print('validation shape =       ', validation.shape)\n",
        "print('validationID shape =     ', validationID.shape)\n",
        "print('validationlabels shape = ', validationlabels.shape)\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_train = ')\n",
        "print(finalcolumns_train)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape =             (14, 16)\n",
            "trainID shape =           (14, 1)\n",
            "labels shape =            (14, 2)\n",
            "validation shape =        (4, 16)\n",
            "validationID shape =      (4, 1)\n",
            "validationlabels shape =  (4, 2)\n",
            "test shape =              (21, 16)\n",
            "testID shape =            (21, 1)\n",
            "labelsencoding_dict = \n",
            "{'bnry': {0: 'cat'}}\n",
            "finalcolumns_train = \n",
            "['shape_NArw', 'shape_circle', 'shape_oval', 'shape_square', 'Y-N_NArw', 'Y-N_bnry', 'date_NArw', 'date_year', 'date_month', 'date_day', 'date_hour', 'date_minute', 'date_second', 'number_NArw', 'number_bxcx', 'number_nmbr']\n",
            "finalcolumns_test = \n",
            "['shape_NArw', 'shape_circle', 'shape_oval', 'shape_square', 'Y-N_NArw', 'Y-N_bnry', 'date_NArw', 'date_year', 'date_month', 'date_day', 'date_hour', 'date_minute', 'date_second', 'number_NArw', 'number_bxcx', 'number_nmbr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HeW93eKCBAX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#demonstrate download of postprocess_dict\n",
        "\n",
        "#save postprocess_dict as pickle object\n",
        "with open('postprocess_dict.pickle', 'wb') as handle:\n",
        "    pickle.dump(postprocess_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "#download to local drive\n",
        "#(code specific to Colaboratory)\n",
        "files.download('postprocess_dict.pickle')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_fA6VIfKEVTz",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "cb2ce756-92e7-4a14-8453-e396a3d15050"
      },
      "cell_type": "code",
      "source": [
        "#demonstrate upload of postprocess_dict\n",
        "\n",
        "#upload postprocess_dict from local drive\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-51060f8a-2a44-4b82-9e8c-daf860b1b4ef\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-51060f8a-2a44-4b82-9e8c-daf860b1b4ef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving postprocess_dict.pickle to postprocess_dict (1).pickle\n",
            "User uploaded file \"postprocess_dict.pickle\" with length 6564 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KRtb8fWwEWAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#acces uploaded file with pickle\n",
        "#(code specific to Colaboratory)\n",
        "with open('postprocess_dict.pickle', 'rb') as handle:\n",
        "    postprocess_dict_upload = pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7bXlJzG-qsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test data set from list of dictionaries\n",
        "#21 rows\n",
        "# test = [{'ID' : 1, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "#         {'ID': 2, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "#         {'ID' : 3,'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "#         {'ID' : 4, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "#         {'ID' : 5, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "#         {'ID' : 6, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "#         {'ID' : 7, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "#         {'ID' : 8, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "#         {'ID' : 9, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "#         {'ID' : 10, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "#         {'ID' : 11, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "#         {'ID' : 12, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "#         {'ID' : 13, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "#         {'ID' : 14, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "#         {'ID' : 15, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "#         {'ID' : 16, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "#         {'ID' : 17, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "#         {'ID' : 18, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "#         {'ID' : 19, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "#         {'ID' : 20, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "#         {'ID' : 21, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'}]\n",
        "\n",
        "#revised test data with all positive values in number column for power transform\n",
        "test = [{'ID' : 1, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID': 2, 'number': 1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 3,'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 4, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 5, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 6, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 7, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 8, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 9, 'number': 1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 10, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 11, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 12, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 13, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 14, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 15, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 16, 'number': 1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 17, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 18, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 19, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 20, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 21, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'}]\n",
        "\n",
        "#convert test data to pandas dataframe\n",
        "df_test = pd.DataFrame(test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YOlPUvVr9O2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c002a001-5473-4d87-dc71-8044bbdb6eb7"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "#this application is primarily to serve as a quick check for bugs prior to more\\\n",
        "#computationally expensive applicaitons in cases of some update to automunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, df_test, testID_column = 'ID')\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, df_test, testID_column = 'ID')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note that values < 0.001 found in test set were reset to NaN\n",
            "to allow consistent box-cox transform as train set.\n",
            "seconds elapsed =  0.11346654699991632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VXBtBy8qAk4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "674b041f-47e9-42fa-e4ae-fe8cffe191ae"
      },
      "cell_type": "code",
      "source": [
        "#validate the output\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test shape =              (21, 16)\n",
            "testID shape =            (21, 1)\n",
            "labelsencoding_dict = \n",
            "{'bnry': {0: 'cat'}}\n",
            "finalcolumns_test = \n",
            "['shape_NArw', 'shape_circle', 'shape_oval', 'shape_square', 'Y-N_NArw', 'Y-N_bnry', 'date_NArw', 'date_year', 'date_month', 'date_day', 'date_hour', 'date_minute', 'date_second', 'number_NArw', 'number_bxcx', 'number_nmbr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sRK8M5saCYga",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "leMtlYjEro_2",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "8b77a60f-347c-4e69-8da4-e5d24456e9de"
      },
      "cell_type": "code",
      "source": [
        "#Now let's try a larger dataset, the Titanic dataset from Kaggle\n",
        "#available here: https://www.kaggle.com/c/titanic/data\n",
        "#(which I will upload form my local hard drive)\n",
        "#for more on data imports in Colaboratory see my medium post \n",
        "#https://medium.com/@_NicT_/colaboratorys-free-gpu-72ebc9272933\n",
        "#Following is as presented in the Colaboratory tutorial notebook\n",
        "#Once run this will allow you to manually select the path on local drive for file you wish to upload\n",
        "\n",
        "#import titanic train data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ca49d674-a454-4af1-8556-0eaa33840e71\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ca49d674-a454-4af1-8556-0eaa33840e71\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.csv to train.csv\n",
            "User uploaded file \"train.csv\" with length 60302 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "frC9fYhFsPzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "titanic_train_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "#titanic_train_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "btRfHQ68seZv",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "bf27c78a-79c2-435c-f4e2-052331a354eb"
      },
      "cell_type": "code",
      "source": [
        "#import titanic test data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-62471adb-605f-49b1-ae7c-a7b742cb7d74\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-62471adb-605f-49b1-ae7c-a7b742cb7d74\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n",
            "User uploaded file \"test.csv\" with length 28210 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wkXAzqELsiEw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "titanic_test_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "#titanic_test_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHurufIAsrS2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initialize data\n",
        "titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()\n",
        "\n",
        "\n",
        "#Now there are certain aspects of feature engineering that our automunge won't address\n",
        "#for example one could extract from the Mrs/Ms/Miss designation in the Name \\\n",
        "#column if a female is married. From Cabin field perhaps we could infer what \\\n",
        "#deck passenger was on or whether they even had a cabin. This type of evaluation \\\n",
        "#would need to be done prior to applicaiton of automunge. Because each column is \\\n",
        "#unique there won't be any learning for Cabin, Name, or Ticket I expect so we'll \\\n",
        "#go ahead and delete those rows for our demonstration. It is certainly \\\n",
        "#feasible that there is some feature buried in these columns that can be \\\n",
        "#extracted prior to applicaiton of automunge. PassengerId will serve as ID column.\n",
        "\n",
        "titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcWlP3Q7ucam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e03595e2-f9f6-4d9e-f465-1bcdc9de44b4"
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did, first we'll try \\\n",
        "#without the MLinfill:\n",
        "\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(titanic_train_df, titanic_test_df, labels_column = 'Survived', \\\n",
        "          trainID_column = 'PassengerId', testID_column = 'PassengerId', \\\n",
        "          MLinfill = False)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  1.0846949470001164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VZk284HfDQj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "f3bcec74-eb37-44ee-bf93-0a7635553700"
      },
      "cell_type": "code",
      "source": [
        "print('train shape =            ', train.shape)\n",
        "print('trainID shape =          ', trainID.shape)\n",
        "print('labels shape =           ', labels.shape)\n",
        "print('validation shape =       ', validation.shape)\n",
        "print('validationID shape =     ', validationID.shape)\n",
        "print('validationlabels shape = ', validationlabels.shape)\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_train = ')\n",
        "print(finalcolumns_train)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape =             (712, 18)\n",
            "trainID shape =           (712, 1)\n",
            "labels shape =            (712, 2)\n",
            "validation shape =        (179, 18)\n",
            "validationID shape =      (179, 1)\n",
            "validationlabels shape =  (179, 2)\n",
            "test shape =              (418, 18)\n",
            "testID shape =            (418, 1)\n",
            "labelsencoding_dict = \n",
            "{'bnry': {0: 0}}\n",
            "finalcolumns_train = \n",
            "['Embarked_C', 'Embarked_NArw', 'Embarked_Q', 'Embarked_S', 'Pclass_NArw', 'Pclass_bxcx', 'Pclass_nmbr', 'Sex_NArw', 'Sex_bnry', 'Age_NArw', 'Age_bxcx', 'Age_nmbr', 'SibSp_NArw', 'SibSp_nmbr', 'Parch_NArw', 'Parch_nmbr', 'Fare_NArw', 'Fare_nmbr']\n",
            "finalcolumns_test = \n",
            "['Embarked_C', 'Embarked_NArw', 'Embarked_Q', 'Embarked_S', 'Pclass_NArw', 'Pclass_bxcx', 'Pclass_nmbr', 'Sex_NArw', 'Sex_bnry', 'Age_NArw', 'Age_bxcx', 'Age_nmbr', 'SibSp_NArw', 'SibSp_nmbr', 'Parch_NArw', 'Parch_nmbr', 'Fare_NArw', 'Fare_nmbr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oEL_M8vgOB3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#re-initialize test data\n",
        "#titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()\n",
        "\n",
        "\n",
        "#titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JKlxPyitOL6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59bab4ed-1d7b-4d9d-d583-5738cf1b31f7"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, titanic_test_df, testID_column = 'PassengerId')\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, titanic_test_df, testID_column = 'PassengerId')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  0.32638505100021575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E-XrUZs0WOn7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#re-initialize data\n",
        "titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()\n",
        "\n",
        "titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MurnZAzKK5E9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "376d7aa3-ac95-4373-a0a5-805d77b9fb67"
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did, now we'll try \\\n",
        "#with the MLinfill included:\n",
        "\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(titanic_train_df, titanic_test_df, labels_column = 'Survived', \\\n",
        "          trainID_column = 'PassengerId', testID_column = 'PassengerId', \\\n",
        "          MLinfill = True)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  1.2998147769999377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ux3y2EcsRAou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#re-initialize test data\n",
        "#titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()\n",
        "\n",
        "\n",
        "#titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mt61ihoARBCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9770b709-8226-4c22-987e-6061026e6148"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, titanic_test_df, testID_column = 'PassengerId')\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, titanic_test_df, testID_column = 'PassengerId')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  0.4100760760002231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nd35fyC3cro2",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c9a025b2-fbd3-4f32-fda0-fd6bae65d1ae"
      },
      "cell_type": "code",
      "source": [
        "#ok let's try with a different set, how about the Kaggel house prices competition\n",
        "#available here: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
        "#(which I will upload form my local hard drive)\n",
        "#for more on data imports in Colaboratory see my medium post \n",
        "#https://medium.com/@_NicT_/colaboratorys-free-gpu-72ebc9272933\n",
        "#Following is as presented in the Colaboratory tutorial notebook\n",
        "#Once run this will allow you to manually select the path on local drive for file you wish to upload\n",
        "\n",
        "#upload train data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0e1e0a76-a61e-472b-aff1-78677f40e8a0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0e1e0a76-a61e-472b-aff1-78677f40e8a0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.csv to train (1).csv\n",
            "User uploaded file \"train.csv\" with length 460676 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WlbP-3Sfd4JQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "house_train_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "#house_train_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RgX6wD6eGJa",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "577f6c70-5a47-4b86-9e3b-02246c8c2099"
      },
      "cell_type": "code",
      "source": [
        "#upload test data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f66c8422-b05a-4643-8efc-91c8c47f21ff\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f66c8422-b05a-4643-8efc-91c8c47f21ff\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test (1).csv\n",
            "User uploaded file \"test.csv\" with length 451405 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZEJgz_qheTYS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "house_test_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "#house_test_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FxPUF2F9gBBu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Ok some bug with the fireplace column so let's just delete that one from our sets\n",
        "#(I believe the evalcategory(.) function is determining different category \\\n",
        "#between train and test sets, fixing this bug is future extension.)\n",
        "house_train_df = house_train_dforig.copy()\n",
        "house_train_df = house_train_df.drop(['FireplaceQu'], axis=1)\n",
        "\n",
        "house_test_df = house_test_dforig.copy()\n",
        "house_test_df = house_test_df.drop(['FireplaceQu'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4OeVv1Efl6Au",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "530400c9-1ac5-4a3a-dd61-982bff1678e6"
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did, first we'll try \\\n",
        "#without the MLinfill:\n",
        "\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(house_train_df, house_test_df, labels_column = 'SalePrice', \\\n",
        "          trainID_column = 'Id', testID_column = 'Id', MLinfill = False)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:901: RuntimeWarning: overflow encountered in square\n",
            "  llf -= N / 2.0 * np.log(np.sum((y - y_mean)**2. / N, axis=0))\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py:1850: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  tmp2 = (x - v) * (fx - fw)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "seconds elapsed =  26.274990118000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RLlA4N7cGhPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "d15bc4f8-9047-489d-a51e-f915a77fffea"
      },
      "cell_type": "code",
      "source": [
        "print('train shape =            ', train.shape)\n",
        "print('trainID shape =          ', trainID.shape)\n",
        "print('labels shape =           ', labels.shape)\n",
        "print('validation shape =       ', validation.shape)\n",
        "print('validationID shape =     ', validationID.shape)\n",
        "print('validationlabels shape = ', validationlabels.shape)\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_train = ')\n",
        "print(finalcolumns_train)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape =             (1168, 354)\n",
            "trainID shape =           (1168, 1)\n",
            "labels shape =            (1168, 1)\n",
            "validation shape =        (292, 354)\n",
            "validationID shape =      (292, 1)\n",
            "validationlabels shape =  (292, 1)\n",
            "test shape =              (1459, 354)\n",
            "testID shape =            (1459, 1)\n",
            "labelsencoding_dict = \n",
            "{'bxcx': {}}\n",
            "finalcolumns_train = \n",
            "['SaleCondition_NArw', 'SaleCondition_Abnorml', 'SaleCondition_AdjLand', 'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal', 'SaleCondition_Partial', 'SaleType_NArw', 'SaleType_COD', 'SaleType_CWD', 'SaleType_Con', 'SaleType_ConLD', 'SaleType_ConLI', 'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD', 'PavedDrive_NArw', 'PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y', 'GarageCond_Ex', 'GarageCond_Fa', 'GarageCond_Gd', 'GarageCond_NArw', 'GarageCond_Po', 'GarageCond_TA', 'GarageQual_Ex', 'GarageQual_Fa', 'GarageQual_Gd', 'GarageQual_NArw', 'GarageQual_Po', 'GarageQual_TA', 'GarageFinish_Fin', 'GarageFinish_NArw', 'GarageFinish_RFn', 'GarageFinish_Unf', 'GarageType_2Types', 'GarageType_Attchd', 'GarageType_Basment', 'GarageType_BuiltIn', 'GarageType_CarPort', 'GarageType_Detchd', 'GarageType_NArw', 'Functional_NArw', 'Functional_Maj1', 'Functional_Maj2', 'Functional_Min1', 'Functional_Min2', 'Functional_Mod', 'Functional_Sev', 'Functional_Typ', 'KitchenQual_NArw', 'KitchenQual_Ex', 'KitchenQual_Fa', 'KitchenQual_Gd', 'KitchenQual_TA', 'Electrical_FuseA', 'Electrical_FuseF', 'Electrical_FuseP', 'Electrical_Mix', 'Electrical_NArw', 'Electrical_SBrkr', 'HeatingQC_NArw', 'HeatingQC_Ex', 'HeatingQC_Fa', 'HeatingQC_Gd', 'HeatingQC_Po', 'HeatingQC_TA', 'Heating_NArw', 'Heating_Floor', 'Heating_GasA', 'Heating_GasW', 'Heating_Grav', 'Heating_OthW', 'Heating_Wall', 'BsmtFinType2_ALQ', 'BsmtFinType2_BLQ', 'BsmtFinType2_GLQ', 'BsmtFinType2_LwQ', 'BsmtFinType2_NArw', 'BsmtFinType2_Rec', 'BsmtFinType2_Unf', 'BsmtFinType1_ALQ', 'BsmtFinType1_BLQ', 'BsmtFinType1_GLQ', 'BsmtFinType1_LwQ', 'BsmtFinType1_NArw', 'BsmtFinType1_Rec', 'BsmtFinType1_Unf', 'BsmtExposure_Av', 'BsmtExposure_Gd', 'BsmtExposure_Mn', 'BsmtExposure_NArw', 'BsmtExposure_No', 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_NArw', 'BsmtCond_Po', 'BsmtCond_TA', 'BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_NArw', 'BsmtQual_TA', 'Foundation_NArw', 'Foundation_BrkTil', 'Foundation_CBlock', 'Foundation_PConc', 'Foundation_Slab', 'Foundation_Stone', 'Foundation_Wood', 'ExterCond_NArw', 'ExterCond_Ex', 'ExterCond_Fa', 'ExterCond_Gd', 'ExterCond_Po', 'ExterCond_TA', 'ExterQual_NArw', 'ExterQual_Ex', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA', 'MasVnrType_BrkCmn', 'MasVnrType_BrkFace', 'MasVnrType_NArw', 'MasVnrType_None', 'MasVnrType_Stone', 'Exterior2nd_NArw', 'Exterior2nd_AsbShng', 'Exterior2nd_AsphShn', 'Exterior2nd_Brk Cmn', 'Exterior2nd_BrkFace', 'Exterior2nd_CBlock', 'Exterior2nd_CmentBd', 'Exterior2nd_HdBoard', 'Exterior2nd_ImStucc', 'Exterior2nd_MetalSd', 'Exterior2nd_Other', 'Exterior2nd_Plywood', 'Exterior2nd_Stone', 'Exterior2nd_Stucco', 'Exterior2nd_VinylSd', 'Exterior2nd_Wd Sdng', 'Exterior2nd_Wd Shng', 'Exterior1st_NArw', 'Exterior1st_AsbShng', 'Exterior1st_AsphShn', 'Exterior1st_BrkComm', 'Exterior1st_BrkFace', 'Exterior1st_CBlock', 'Exterior1st_CemntBd', 'Exterior1st_HdBoard', 'Exterior1st_ImStucc', 'Exterior1st_MetalSd', 'Exterior1st_Plywood', 'Exterior1st_Stone', 'Exterior1st_Stucco', 'Exterior1st_VinylSd', 'Exterior1st_Wd Sdng', 'Exterior1st_WdShing', 'RoofMatl_NArw', 'RoofMatl_ClyTile', 'RoofMatl_CompShg', 'RoofMatl_Membran', 'RoofMatl_Metal', 'RoofMatl_Roll', 'RoofMatl_Tar&Grv', 'RoofMatl_WdShake', 'RoofMatl_WdShngl', 'RoofStyle_NArw', 'RoofStyle_Flat', 'RoofStyle_Gable', 'RoofStyle_Gambrel', 'RoofStyle_Hip', 'RoofStyle_Mansard', 'RoofStyle_Shed', 'HouseStyle_NArw', 'HouseStyle_1.5Fin', 'HouseStyle_1.5Unf', 'HouseStyle_1Story', 'HouseStyle_2.5Fin', 'HouseStyle_2.5Unf', 'HouseStyle_2Story', 'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'BldgType_NArw', 'BldgType_1Fam', 'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE', 'Condition2_NArw', 'Condition2_Artery', 'Condition2_Feedr', 'Condition2_Norm', 'Condition2_PosA', 'Condition2_PosN', 'Condition2_RRAe', 'Condition2_RRAn', 'Condition2_RRNn', 'Condition1_NArw', 'Condition1_Artery', 'Condition1_Feedr', 'Condition1_Norm', 'Condition1_PosA', 'Condition1_PosN', 'Condition1_RRAe', 'Condition1_RRAn', 'Condition1_RRNe', 'Condition1_RRNn', 'Neighborhood_NArw', 'Neighborhood_Blmngtn', 'Neighborhood_Blueste', 'Neighborhood_BrDale', 'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_CollgCr', 'Neighborhood_Crawfor', 'Neighborhood_Edwards', 'Neighborhood_Gilbert', 'Neighborhood_IDOTRR', 'Neighborhood_MeadowV', 'Neighborhood_Mitchel', 'Neighborhood_NAmes', 'Neighborhood_NPkVill', 'Neighborhood_NWAmes', 'Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_SWISU', 'Neighborhood_Sawyer', 'Neighborhood_SawyerW', 'Neighborhood_Somerst', 'Neighborhood_StoneBr', 'Neighborhood_Timber', 'Neighborhood_Veenker', 'LandSlope_NArw', 'LandSlope_Gtl', 'LandSlope_Mod', 'LandSlope_Sev', 'LotConfig_NArw', 'LotConfig_Corner', 'LotConfig_CulDSac', 'LotConfig_FR2', 'LotConfig_FR3', 'LotConfig_Inside', 'LandContour_NArw', 'LandContour_Bnk', 'LandContour_HLS', 'LandContour_Low', 'LandContour_Lvl', 'LotShape_NArw', 'LotShape_IR1', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg', 'MSZoning_NArw', 'MSZoning_C (all)', 'MSZoning_FV', 'MSZoning_RH', 'MSZoning_RL', 'MSZoning_RM', 'MSSubClass_NArw', 'MSSubClass_bxcx', 'MSSubClass_nmbr', 'LotFrontage_NArw', 'LotFrontage_bxcx', 'LotFrontage_nmbr', 'LotArea_NArw', 'LotArea_bxcx', 'LotArea_nmbr', 'Street_NArw', 'Street_bnry', 'Utilities_NArw', 'Utilities_bnry', 'OverallQual_NArw', 'OverallQual_bxcx', 'OverallQual_nmbr', 'OverallCond_NArw', 'OverallCond_bxcx', 'OverallCond_nmbr', 'YearBuilt_NArw', 'YearBuilt_bxcx', 'YearBuilt_nmbr', 'YearRemodAdd_NArw', 'YearRemodAdd_bxcx', 'YearRemodAdd_nmbr', 'MasVnrArea_NArw', 'MasVnrArea_nmbr', 'BsmtFinSF1_NArw', 'BsmtFinSF1_nmbr', 'BsmtFinSF2_NArw', 'BsmtFinSF2_nmbr', 'BsmtUnfSF_NArw', 'BsmtUnfSF_nmbr', 'TotalBsmtSF_NArw', 'TotalBsmtSF_nmbr', 'CentralAir_NArw', 'CentralAir_bnry', '1stFlrSF_NArw', '1stFlrSF_bxcx', '1stFlrSF_nmbr', '2ndFlrSF_NArw', '2ndFlrSF_nmbr', 'LowQualFinSF_NArw', 'LowQualFinSF_nmbr', 'GrLivArea_NArw', 'GrLivArea_bxcx', 'GrLivArea_nmbr', 'BsmtFullBath_NArw', 'BsmtFullBath_nmbr', 'BsmtHalfBath_NArw', 'BsmtHalfBath_nmbr', 'FullBath_NArw', 'FullBath_nmbr', 'HalfBath_NArw', 'HalfBath_nmbr', 'BedroomAbvGr_NArw', 'BedroomAbvGr_nmbr', 'KitchenAbvGr_NArw', 'KitchenAbvGr_nmbr', 'TotRmsAbvGrd_NArw', 'TotRmsAbvGrd_bxcx', 'TotRmsAbvGrd_nmbr', 'Fireplaces_NArw', 'Fireplaces_nmbr', 'GarageYrBlt_NArw', 'GarageYrBlt_bxcx', 'GarageYrBlt_nmbr', 'GarageCars_NArw', 'GarageCars_nmbr', 'GarageArea_NArw', 'GarageArea_nmbr', 'WoodDeckSF_NArw', 'WoodDeckSF_nmbr', 'OpenPorchSF_NArw', 'OpenPorchSF_nmbr', 'EnclosedPorch_NArw', 'EnclosedPorch_nmbr', '3SsnPorch_NArw', '3SsnPorch_nmbr', 'ScreenPorch_NArw', 'ScreenPorch_nmbr', 'PoolArea_NArw', 'PoolArea_nmbr', 'MiscVal_NArw', 'MiscVal_nmbr', 'MoSold_NArw', 'MoSold_bxcx', 'MoSold_nmbr', 'YrSold_NArw', 'YrSold_bxcx', 'YrSold_nmbr']\n",
            "finalcolumns_test = \n",
            "['SaleCondition_NArw', 'SaleCondition_Abnorml', 'SaleCondition_AdjLand', 'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal', 'SaleCondition_Partial', 'SaleType_NArw', 'SaleType_COD', 'SaleType_CWD', 'SaleType_Con', 'SaleType_ConLD', 'SaleType_ConLI', 'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD', 'PavedDrive_NArw', 'PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y', 'GarageCond_Ex', 'GarageCond_Fa', 'GarageCond_Gd', 'GarageCond_NArw', 'GarageCond_Po', 'GarageCond_TA', 'GarageQual_Ex', 'GarageQual_Fa', 'GarageQual_Gd', 'GarageQual_NArw', 'GarageQual_Po', 'GarageQual_TA', 'GarageFinish_Fin', 'GarageFinish_NArw', 'GarageFinish_RFn', 'GarageFinish_Unf', 'GarageType_2Types', 'GarageType_Attchd', 'GarageType_Basment', 'GarageType_BuiltIn', 'GarageType_CarPort', 'GarageType_Detchd', 'GarageType_NArw', 'Functional_NArw', 'Functional_Maj1', 'Functional_Maj2', 'Functional_Min1', 'Functional_Min2', 'Functional_Mod', 'Functional_Sev', 'Functional_Typ', 'KitchenQual_NArw', 'KitchenQual_Ex', 'KitchenQual_Fa', 'KitchenQual_Gd', 'KitchenQual_TA', 'Electrical_FuseA', 'Electrical_FuseF', 'Electrical_FuseP', 'Electrical_Mix', 'Electrical_NArw', 'Electrical_SBrkr', 'HeatingQC_NArw', 'HeatingQC_Ex', 'HeatingQC_Fa', 'HeatingQC_Gd', 'HeatingQC_Po', 'HeatingQC_TA', 'Heating_NArw', 'Heating_Floor', 'Heating_GasA', 'Heating_GasW', 'Heating_Grav', 'Heating_OthW', 'Heating_Wall', 'BsmtFinType2_ALQ', 'BsmtFinType2_BLQ', 'BsmtFinType2_GLQ', 'BsmtFinType2_LwQ', 'BsmtFinType2_NArw', 'BsmtFinType2_Rec', 'BsmtFinType2_Unf', 'BsmtFinType1_ALQ', 'BsmtFinType1_BLQ', 'BsmtFinType1_GLQ', 'BsmtFinType1_LwQ', 'BsmtFinType1_NArw', 'BsmtFinType1_Rec', 'BsmtFinType1_Unf', 'BsmtExposure_Av', 'BsmtExposure_Gd', 'BsmtExposure_Mn', 'BsmtExposure_NArw', 'BsmtExposure_No', 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_NArw', 'BsmtCond_Po', 'BsmtCond_TA', 'BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_NArw', 'BsmtQual_TA', 'Foundation_NArw', 'Foundation_BrkTil', 'Foundation_CBlock', 'Foundation_PConc', 'Foundation_Slab', 'Foundation_Stone', 'Foundation_Wood', 'ExterCond_NArw', 'ExterCond_Ex', 'ExterCond_Fa', 'ExterCond_Gd', 'ExterCond_Po', 'ExterCond_TA', 'ExterQual_NArw', 'ExterQual_Ex', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA', 'MasVnrType_BrkCmn', 'MasVnrType_BrkFace', 'MasVnrType_NArw', 'MasVnrType_None', 'MasVnrType_Stone', 'Exterior2nd_NArw', 'Exterior2nd_AsbShng', 'Exterior2nd_AsphShn', 'Exterior2nd_Brk Cmn', 'Exterior2nd_BrkFace', 'Exterior2nd_CBlock', 'Exterior2nd_CmentBd', 'Exterior2nd_HdBoard', 'Exterior2nd_ImStucc', 'Exterior2nd_MetalSd', 'Exterior2nd_Other', 'Exterior2nd_Plywood', 'Exterior2nd_Stone', 'Exterior2nd_Stucco', 'Exterior2nd_VinylSd', 'Exterior2nd_Wd Sdng', 'Exterior2nd_Wd Shng', 'Exterior1st_NArw', 'Exterior1st_AsbShng', 'Exterior1st_AsphShn', 'Exterior1st_BrkComm', 'Exterior1st_BrkFace', 'Exterior1st_CBlock', 'Exterior1st_CemntBd', 'Exterior1st_HdBoard', 'Exterior1st_ImStucc', 'Exterior1st_MetalSd', 'Exterior1st_Plywood', 'Exterior1st_Stone', 'Exterior1st_Stucco', 'Exterior1st_VinylSd', 'Exterior1st_Wd Sdng', 'Exterior1st_WdShing', 'RoofMatl_NArw', 'RoofMatl_ClyTile', 'RoofMatl_CompShg', 'RoofMatl_Membran', 'RoofMatl_Metal', 'RoofMatl_Roll', 'RoofMatl_Tar&Grv', 'RoofMatl_WdShake', 'RoofMatl_WdShngl', 'RoofStyle_NArw', 'RoofStyle_Flat', 'RoofStyle_Gable', 'RoofStyle_Gambrel', 'RoofStyle_Hip', 'RoofStyle_Mansard', 'RoofStyle_Shed', 'HouseStyle_NArw', 'HouseStyle_1.5Fin', 'HouseStyle_1.5Unf', 'HouseStyle_1Story', 'HouseStyle_2.5Fin', 'HouseStyle_2.5Unf', 'HouseStyle_2Story', 'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'BldgType_NArw', 'BldgType_1Fam', 'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE', 'Condition2_NArw', 'Condition2_Artery', 'Condition2_Feedr', 'Condition2_Norm', 'Condition2_PosA', 'Condition2_PosN', 'Condition2_RRAe', 'Condition2_RRAn', 'Condition2_RRNn', 'Condition1_NArw', 'Condition1_Artery', 'Condition1_Feedr', 'Condition1_Norm', 'Condition1_PosA', 'Condition1_PosN', 'Condition1_RRAe', 'Condition1_RRAn', 'Condition1_RRNe', 'Condition1_RRNn', 'Neighborhood_NArw', 'Neighborhood_Blmngtn', 'Neighborhood_Blueste', 'Neighborhood_BrDale', 'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_CollgCr', 'Neighborhood_Crawfor', 'Neighborhood_Edwards', 'Neighborhood_Gilbert', 'Neighborhood_IDOTRR', 'Neighborhood_MeadowV', 'Neighborhood_Mitchel', 'Neighborhood_NAmes', 'Neighborhood_NPkVill', 'Neighborhood_NWAmes', 'Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_SWISU', 'Neighborhood_Sawyer', 'Neighborhood_SawyerW', 'Neighborhood_Somerst', 'Neighborhood_StoneBr', 'Neighborhood_Timber', 'Neighborhood_Veenker', 'LandSlope_NArw', 'LandSlope_Gtl', 'LandSlope_Mod', 'LandSlope_Sev', 'LotConfig_NArw', 'LotConfig_Corner', 'LotConfig_CulDSac', 'LotConfig_FR2', 'LotConfig_FR3', 'LotConfig_Inside', 'LandContour_NArw', 'LandContour_Bnk', 'LandContour_HLS', 'LandContour_Low', 'LandContour_Lvl', 'LotShape_NArw', 'LotShape_IR1', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg', 'MSZoning_NArw', 'MSZoning_C (all)', 'MSZoning_FV', 'MSZoning_RH', 'MSZoning_RL', 'MSZoning_RM', 'MSSubClass_NArw', 'MSSubClass_bxcx', 'MSSubClass_nmbr', 'LotFrontage_NArw', 'LotFrontage_bxcx', 'LotFrontage_nmbr', 'LotArea_NArw', 'LotArea_bxcx', 'LotArea_nmbr', 'Street_NArw', 'Street_bnry', 'Utilities_NArw', 'Utilities_bnry', 'OverallQual_NArw', 'OverallQual_bxcx', 'OverallQual_nmbr', 'OverallCond_NArw', 'OverallCond_bxcx', 'OverallCond_nmbr', 'YearBuilt_NArw', 'YearBuilt_bxcx', 'YearBuilt_nmbr', 'YearRemodAdd_NArw', 'YearRemodAdd_bxcx', 'YearRemodAdd_nmbr', 'MasVnrArea_NArw', 'MasVnrArea_nmbr', 'BsmtFinSF1_NArw', 'BsmtFinSF1_nmbr', 'BsmtFinSF2_NArw', 'BsmtFinSF2_nmbr', 'BsmtUnfSF_NArw', 'BsmtUnfSF_nmbr', 'TotalBsmtSF_NArw', 'TotalBsmtSF_nmbr', 'CentralAir_NArw', 'CentralAir_bnry', '1stFlrSF_NArw', '1stFlrSF_bxcx', '1stFlrSF_nmbr', '2ndFlrSF_NArw', '2ndFlrSF_nmbr', 'LowQualFinSF_NArw', 'LowQualFinSF_nmbr', 'GrLivArea_NArw', 'GrLivArea_bxcx', 'GrLivArea_nmbr', 'BsmtFullBath_NArw', 'BsmtFullBath_nmbr', 'BsmtHalfBath_NArw', 'BsmtHalfBath_nmbr', 'FullBath_NArw', 'FullBath_nmbr', 'HalfBath_NArw', 'HalfBath_nmbr', 'BedroomAbvGr_NArw', 'BedroomAbvGr_nmbr', 'KitchenAbvGr_NArw', 'KitchenAbvGr_nmbr', 'TotRmsAbvGrd_NArw', 'TotRmsAbvGrd_bxcx', 'TotRmsAbvGrd_nmbr', 'Fireplaces_NArw', 'Fireplaces_nmbr', 'GarageYrBlt_NArw', 'GarageYrBlt_bxcx', 'GarageYrBlt_nmbr', 'GarageCars_NArw', 'GarageCars_nmbr', 'GarageArea_NArw', 'GarageArea_nmbr', 'WoodDeckSF_NArw', 'WoodDeckSF_nmbr', 'OpenPorchSF_NArw', 'OpenPorchSF_nmbr', 'EnclosedPorch_NArw', 'EnclosedPorch_nmbr', '3SsnPorch_NArw', '3SsnPorch_nmbr', 'ScreenPorch_NArw', 'ScreenPorch_nmbr', 'PoolArea_NArw', 'PoolArea_nmbr', 'MiscVal_NArw', 'MiscVal_nmbr', 'MoSold_NArw', 'MoSold_bxcx', 'MoSold_nmbr', 'YrSold_NArw', 'YrSold_bxcx', 'YrSold_nmbr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TUZCSQxNTChT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#re-initialize the test set\n",
        "# house_train_df = house_train_dforig.copy()\n",
        "# house_train_df = house_train_df.drop(['FireplaceQu'], axis=1)\n",
        "\n",
        "house_test_df = house_test_dforig.copy()\n",
        "house_test_df = house_test_df.drop(['FireplaceQu'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hii-Rfc6TMqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dab91583-578a-416d-f137-a2e7928ea894"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, house_test_df, testID_column = 'Id')\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, house_test_df, testID_column = 'Id')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error - different category between train and test sets for column  Alley\n",
            "error - different category between train and test sets for column  PoolQC\n",
            "error - different category between train and test sets for column  Fence\n",
            "error - different category between train and test sets for column  MiscFeature\n",
            "seconds elapsed =  13.238316189000216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EbuRzVqbT-xy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#go ahead and ignore these error messages, these are the columns that returned '\n",
        "#a category of 'null' from applicaiton of evalcagteghory and will have been dropped\\\n",
        "#I'll have to put some thought in to how to better compare the train and test \\\n",
        "#column categories. (to confirm see look at the validation array shapes below)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QsL6w7kPUdE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "41fda03c-e236-4247-cc6d-332852f0925d"
      },
      "cell_type": "code",
      "source": [
        "#validate the output\n",
        "print('test shape =             ', test.shape)\n",
        "print('testID shape =           ', testID.shape)\n",
        "print('labelsencoding_dict = ')\n",
        "print(labelsencoding_dict)\n",
        "print('finalcolumns_test = ')\n",
        "print(finalcolumns_test)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test shape =              (1459, 354)\n",
            "testID shape =            (1459, 1)\n",
            "labelsencoding_dict = \n",
            "{'bxcx': {}}\n",
            "finalcolumns_test = \n",
            "['SaleCondition_NArw', 'SaleCondition_Abnorml', 'SaleCondition_AdjLand', 'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal', 'SaleCondition_Partial', 'SaleType_NArw', 'SaleType_COD', 'SaleType_CWD', 'SaleType_Con', 'SaleType_ConLD', 'SaleType_ConLI', 'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD', 'PavedDrive_NArw', 'PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y', 'GarageCond_Ex', 'GarageCond_Fa', 'GarageCond_Gd', 'GarageCond_NArw', 'GarageCond_Po', 'GarageCond_TA', 'GarageQual_Ex', 'GarageQual_Fa', 'GarageQual_Gd', 'GarageQual_NArw', 'GarageQual_Po', 'GarageQual_TA', 'GarageFinish_Fin', 'GarageFinish_NArw', 'GarageFinish_RFn', 'GarageFinish_Unf', 'GarageType_2Types', 'GarageType_Attchd', 'GarageType_Basment', 'GarageType_BuiltIn', 'GarageType_CarPort', 'GarageType_Detchd', 'GarageType_NArw', 'Functional_NArw', 'Functional_Maj1', 'Functional_Maj2', 'Functional_Min1', 'Functional_Min2', 'Functional_Mod', 'Functional_Sev', 'Functional_Typ', 'KitchenQual_NArw', 'KitchenQual_Ex', 'KitchenQual_Fa', 'KitchenQual_Gd', 'KitchenQual_TA', 'Electrical_FuseA', 'Electrical_FuseF', 'Electrical_FuseP', 'Electrical_Mix', 'Electrical_NArw', 'Electrical_SBrkr', 'HeatingQC_NArw', 'HeatingQC_Ex', 'HeatingQC_Fa', 'HeatingQC_Gd', 'HeatingQC_Po', 'HeatingQC_TA', 'Heating_NArw', 'Heating_Floor', 'Heating_GasA', 'Heating_GasW', 'Heating_Grav', 'Heating_OthW', 'Heating_Wall', 'BsmtFinType2_ALQ', 'BsmtFinType2_BLQ', 'BsmtFinType2_GLQ', 'BsmtFinType2_LwQ', 'BsmtFinType2_NArw', 'BsmtFinType2_Rec', 'BsmtFinType2_Unf', 'BsmtFinType1_ALQ', 'BsmtFinType1_BLQ', 'BsmtFinType1_GLQ', 'BsmtFinType1_LwQ', 'BsmtFinType1_NArw', 'BsmtFinType1_Rec', 'BsmtFinType1_Unf', 'BsmtExposure_Av', 'BsmtExposure_Gd', 'BsmtExposure_Mn', 'BsmtExposure_NArw', 'BsmtExposure_No', 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_NArw', 'BsmtCond_Po', 'BsmtCond_TA', 'BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_NArw', 'BsmtQual_TA', 'Foundation_NArw', 'Foundation_BrkTil', 'Foundation_CBlock', 'Foundation_PConc', 'Foundation_Slab', 'Foundation_Stone', 'Foundation_Wood', 'ExterCond_NArw', 'ExterCond_Ex', 'ExterCond_Fa', 'ExterCond_Gd', 'ExterCond_Po', 'ExterCond_TA', 'ExterQual_NArw', 'ExterQual_Ex', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA', 'MasVnrType_BrkCmn', 'MasVnrType_BrkFace', 'MasVnrType_NArw', 'MasVnrType_None', 'MasVnrType_Stone', 'Exterior2nd_NArw', 'Exterior2nd_AsbShng', 'Exterior2nd_AsphShn', 'Exterior2nd_Brk Cmn', 'Exterior2nd_BrkFace', 'Exterior2nd_CBlock', 'Exterior2nd_CmentBd', 'Exterior2nd_HdBoard', 'Exterior2nd_ImStucc', 'Exterior2nd_MetalSd', 'Exterior2nd_Other', 'Exterior2nd_Plywood', 'Exterior2nd_Stone', 'Exterior2nd_Stucco', 'Exterior2nd_VinylSd', 'Exterior2nd_Wd Sdng', 'Exterior2nd_Wd Shng', 'Exterior1st_NArw', 'Exterior1st_AsbShng', 'Exterior1st_AsphShn', 'Exterior1st_BrkComm', 'Exterior1st_BrkFace', 'Exterior1st_CBlock', 'Exterior1st_CemntBd', 'Exterior1st_HdBoard', 'Exterior1st_ImStucc', 'Exterior1st_MetalSd', 'Exterior1st_Plywood', 'Exterior1st_Stone', 'Exterior1st_Stucco', 'Exterior1st_VinylSd', 'Exterior1st_Wd Sdng', 'Exterior1st_WdShing', 'RoofMatl_NArw', 'RoofMatl_ClyTile', 'RoofMatl_CompShg', 'RoofMatl_Membran', 'RoofMatl_Metal', 'RoofMatl_Roll', 'RoofMatl_Tar&Grv', 'RoofMatl_WdShake', 'RoofMatl_WdShngl', 'RoofStyle_NArw', 'RoofStyle_Flat', 'RoofStyle_Gable', 'RoofStyle_Gambrel', 'RoofStyle_Hip', 'RoofStyle_Mansard', 'RoofStyle_Shed', 'HouseStyle_NArw', 'HouseStyle_1.5Fin', 'HouseStyle_1.5Unf', 'HouseStyle_1Story', 'HouseStyle_2.5Fin', 'HouseStyle_2.5Unf', 'HouseStyle_2Story', 'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'BldgType_NArw', 'BldgType_1Fam', 'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE', 'Condition2_NArw', 'Condition2_Artery', 'Condition2_Feedr', 'Condition2_Norm', 'Condition2_PosA', 'Condition2_PosN', 'Condition2_RRAe', 'Condition2_RRAn', 'Condition2_RRNn', 'Condition1_NArw', 'Condition1_Artery', 'Condition1_Feedr', 'Condition1_Norm', 'Condition1_PosA', 'Condition1_PosN', 'Condition1_RRAe', 'Condition1_RRAn', 'Condition1_RRNe', 'Condition1_RRNn', 'Neighborhood_NArw', 'Neighborhood_Blmngtn', 'Neighborhood_Blueste', 'Neighborhood_BrDale', 'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_CollgCr', 'Neighborhood_Crawfor', 'Neighborhood_Edwards', 'Neighborhood_Gilbert', 'Neighborhood_IDOTRR', 'Neighborhood_MeadowV', 'Neighborhood_Mitchel', 'Neighborhood_NAmes', 'Neighborhood_NPkVill', 'Neighborhood_NWAmes', 'Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_SWISU', 'Neighborhood_Sawyer', 'Neighborhood_SawyerW', 'Neighborhood_Somerst', 'Neighborhood_StoneBr', 'Neighborhood_Timber', 'Neighborhood_Veenker', 'LandSlope_NArw', 'LandSlope_Gtl', 'LandSlope_Mod', 'LandSlope_Sev', 'LotConfig_NArw', 'LotConfig_Corner', 'LotConfig_CulDSac', 'LotConfig_FR2', 'LotConfig_FR3', 'LotConfig_Inside', 'LandContour_NArw', 'LandContour_Bnk', 'LandContour_HLS', 'LandContour_Low', 'LandContour_Lvl', 'LotShape_NArw', 'LotShape_IR1', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg', 'MSZoning_NArw', 'MSZoning_C (all)', 'MSZoning_FV', 'MSZoning_RH', 'MSZoning_RL', 'MSZoning_RM', 'MSSubClass_NArw', 'MSSubClass_bxcx', 'MSSubClass_nmbr', 'LotFrontage_NArw', 'LotFrontage_bxcx', 'LotFrontage_nmbr', 'LotArea_NArw', 'LotArea_bxcx', 'LotArea_nmbr', 'Street_NArw', 'Street_bnry', 'Utilities_NArw', 'Utilities_bnry', 'OverallQual_NArw', 'OverallQual_bxcx', 'OverallQual_nmbr', 'OverallCond_NArw', 'OverallCond_bxcx', 'OverallCond_nmbr', 'YearBuilt_NArw', 'YearBuilt_bxcx', 'YearBuilt_nmbr', 'YearRemodAdd_NArw', 'YearRemodAdd_bxcx', 'YearRemodAdd_nmbr', 'MasVnrArea_NArw', 'MasVnrArea_nmbr', 'BsmtFinSF1_NArw', 'BsmtFinSF1_nmbr', 'BsmtFinSF2_NArw', 'BsmtFinSF2_nmbr', 'BsmtUnfSF_NArw', 'BsmtUnfSF_nmbr', 'TotalBsmtSF_NArw', 'TotalBsmtSF_nmbr', 'CentralAir_NArw', 'CentralAir_bnry', '1stFlrSF_NArw', '1stFlrSF_bxcx', '1stFlrSF_nmbr', '2ndFlrSF_NArw', '2ndFlrSF_nmbr', 'LowQualFinSF_NArw', 'LowQualFinSF_nmbr', 'GrLivArea_NArw', 'GrLivArea_bxcx', 'GrLivArea_nmbr', 'BsmtFullBath_NArw', 'BsmtFullBath_nmbr', 'BsmtHalfBath_NArw', 'BsmtHalfBath_nmbr', 'FullBath_NArw', 'FullBath_nmbr', 'HalfBath_NArw', 'HalfBath_nmbr', 'BedroomAbvGr_NArw', 'BedroomAbvGr_nmbr', 'KitchenAbvGr_NArw', 'KitchenAbvGr_nmbr', 'TotRmsAbvGrd_NArw', 'TotRmsAbvGrd_bxcx', 'TotRmsAbvGrd_nmbr', 'Fireplaces_NArw', 'Fireplaces_nmbr', 'GarageYrBlt_NArw', 'GarageYrBlt_bxcx', 'GarageYrBlt_nmbr', 'GarageCars_NArw', 'GarageCars_nmbr', 'GarageArea_NArw', 'GarageArea_nmbr', 'WoodDeckSF_NArw', 'WoodDeckSF_nmbr', 'OpenPorchSF_NArw', 'OpenPorchSF_nmbr', 'EnclosedPorch_NArw', 'EnclosedPorch_nmbr', '3SsnPorch_NArw', '3SsnPorch_nmbr', 'ScreenPorch_NArw', 'ScreenPorch_nmbr', 'PoolArea_NArw', 'PoolArea_nmbr', 'MiscVal_NArw', 'MiscVal_nmbr', 'MoSold_NArw', 'MoSold_bxcx', 'MoSold_nmbr', 'YrSold_NArw', 'YrSold_bxcx', 'YrSold_nmbr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1WBJ4qoPc86Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now let's try again with the MLinfill and compare results. I would expect to \\\n",
        "#see hopefully a more pronounced result than the delta from titanic set due \\\n",
        "#to higher frequency of missing data in the set."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Q21WOBgtMq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#reinitialize the data\n",
        "house_train_df = house_train_dforig.copy()\n",
        "house_train_df = house_train_df.drop(['FireplaceQu'], axis=1)\n",
        "\n",
        "house_test_df = house_test_dforig.copy()\n",
        "house_test_df = house_test_df.drop(['FireplaceQu'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NxVuKp93yW_y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#full disclosure the below error is unexpected behavior, I'll have to put some\n",
        "#troubleshooting time in. Might have something to do with the overflow error\n",
        "#message which is also new. These might be related to the Random Forest model\n",
        "#to be continued"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_-TqGqJeE5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "outputId": "bb73e811-361a-4f9d-d7f9-c6f11d9027f1"
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did with the MLinfill:\n",
        "\n",
        "start = timer()\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, \\\n",
        "testID, labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
        "postprocess_dict = \\\n",
        "automunge(house_train_df, house_test_df, labels_column = 'SalePrice', \\\n",
        "          trainID_column = 'Id', testID_column = 'Id', MLinfill = True)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:901: RuntimeWarning: overflow encountered in square\n",
            "  llf -= N / 2.0 * np.log(np.sum((y - y_mean)**2. / N, axis=0))\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py:1850: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  tmp2 = (x - v) * (fx - fw)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-cf38cf932204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidationID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidationlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsencoding_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalcolumns_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalcolumns_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautomunge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhouse_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhouse_test_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mtrainID_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestID_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLinfill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b1cda21a2743>\u001b[0m in \u001b[0;36mautomunge\u001b[0;34m(df_train, df_test, labels_column, trainID_column, testID_column, valpercent, powertransform, MLinfill, infilliterate, randomseed, excludetransformscolumns)\u001b[0m\n\u001b[1;32m    366\u001b[0m               \u001b[0;31m#predict infill values using defined function predictinfill(.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m               \u001b[0;31m#first we'll address the bxcx transformed column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m               \u001b[0mdf_traininfill_bxcx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_testinfill_bxcx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_bxcx\u001b[0m \u001b[0;34m=\u001b[0m                             \u001b[0mpredictinfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_bxcx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_filltrain\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mdf_train_filllabel\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mdf_train_fillfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test_fillfeatures\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mrandomseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumnslist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmbrcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-001ae2bfd366>\u001b[0m in \u001b[0;36mpredictinfill\u001b[0;34m(category, df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures, randomseed, columnslist)\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_train_filltrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_train_filllabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0;31m#predict infill values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3cbnxgUl4HkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#reinitialize the test data\n",
        "# house_train_df = house_train_dforig.copy()\n",
        "# house_train_df = house_train_df.drop(['FireplaceQu'], axis=1)\n",
        "\n",
        "house_test_df = house_test_dforig.copy()\n",
        "house_test_df = house_test_df.drop(['FireplaceQu'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIUXqP_dZ9w0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "510143cc-1731-4ea7-92f3-e5dd52c46d85"
      },
      "cell_type": "code",
      "source": [
        "#apply postmunge\n",
        "\n",
        "start = timer()\n",
        "\n",
        "test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "postmunge(postprocess_dict, house_test_df, testID_column = 'Id')\n",
        "\n",
        "# test, testID, labelsencoding_dict, finalcolumns_test = \\\n",
        "# postmunge(postprocess_dict_upload, house_test_df, testID_column = 'Id')\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('seconds elapsed = ', end - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "troubleshooting process_bxcx_class function\n",
            "column =  MSSubClass\n",
            "bxcx_lmbda =  -0.2108406694646882\n",
            "after stats .boxcox transform, bxcx_lmbda =  -0.2108406694646882\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  LotFrontage\n",
            "bxcx_lmbda =  0.43778995839151413\n",
            "after stats .boxcox transform, bxcx_lmbda =  0.43778995839151413\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  LotArea\n",
            "bxcx_lmbda =  0.030946318971935564\n",
            "after stats .boxcox transform, bxcx_lmbda =  0.030946318971935564\n",
            "error - different category between train and test sets for column  Alley\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  OverallQual\n",
            "bxcx_lmbda =  0.7622455699934552\n",
            "after stats .boxcox transform, bxcx_lmbda =  0.7622455699934552\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  OverallCond\n",
            "bxcx_lmbda =  0.41391221535396094\n",
            "after stats .boxcox transform, bxcx_lmbda =  0.41391221535396094\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  YearBuilt\n",
            "bxcx_lmbda =  22.116548766131825\n",
            "after stats .boxcox transform, bxcx_lmbda =  22.116548766131825\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  YearRemodAdd\n",
            "bxcx_lmbda =  40.568166755406786\n",
            "after stats .boxcox transform, bxcx_lmbda =  40.568166755406786\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  1stFlrSF\n",
            "bxcx_lmbda =  -0.07883221340966222\n",
            "after stats .boxcox transform, bxcx_lmbda =  -0.07883221340966222\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  GrLivArea\n",
            "bxcx_lmbda =  0.006304886038020439\n",
            "after stats .boxcox transform, bxcx_lmbda =  0.006304886038020439\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  TotRmsAbvGrd\n",
            "bxcx_lmbda =  0.21807487903984202\n",
            "after stats .boxcox transform, bxcx_lmbda =  0.21807487903984202\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  GarageYrBlt\n",
            "bxcx_lmbda =  28.777542622324226\n",
            "after stats .boxcox transform, bxcx_lmbda =  28.777542622324226\n",
            "error - different category between train and test sets for column  PoolQC\n",
            "error - different category between train and test sets for column  Fence\n",
            "error - different category between train and test sets for column  MiscFeature\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  MoSold\n",
            "bxcx_lmbda =  0.7759250696784257\n",
            "after stats .boxcox transform, bxcx_lmbda =  0.7759250696784257\n",
            "troubleshooting process_bxcx_class function\n",
            "column =  YrSold\n",
            "bxcx_lmbda =  -3.580423966384246\n",
            "after stats .boxcox transform, bxcx_lmbda =  -3.580423966384246\n",
            "seconds elapsed =  26.79885795300288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k_CGXW5sQ9pn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_MLinfill_9-04-18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "hQJGsdymZDsD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmjR5Ze0U-JC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1) insert preprocess and evalcategory functions from prior notebook. \n",
        "Note the process_text_class(.) has been updated to include an additional output and evalcategory(.) has been updated to improve address of NaN"
      ]
    },
    {
      "metadata": {
        "id": "psx9E_ckUsPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#process_numerical_class(mdf_train, mdf_test, column)\n",
        "#function to normalize data to mean of 0 and standard deviation of 1 from training distribution\n",
        "#takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\\\n",
        "#and the name of the column string ('column') \n",
        "#replaces missing or improperly formatted data with mean of remaining values\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#expect this approach works better when the numerical distribution is thin tailed\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "#imports\n",
        "from pandas import Series\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def process_numerical_class(mdf_train, mdf_test, column):\n",
        "     \n",
        "    \n",
        "  #convert all values to either numeric or NaN\n",
        "  mdf_train[column] = pd.to_numeric(mdf_train[column], errors='coerce')\n",
        "  mdf_test[column] = pd.to_numeric(mdf_test[column], errors='coerce')\n",
        "\n",
        "  #get mean of training data\n",
        "  mean = mdf_train[column].mean()    \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_train[column] = mdf_train[column].fillna(mean)\n",
        "  mdf_test[column] = mdf_test[column].fillna(mean)\n",
        "\n",
        "  #subtract mean from column for both train and test\n",
        "  mdf_train[column] = mdf_train[column] - mean\n",
        "  mdf_test[column] = mdf_test[column] - mean\n",
        "\n",
        "  #get standard deviation of training data\n",
        "  std = mdf_train[column].std()\n",
        "\n",
        "  #divide column values by std for both training and test data\n",
        "  mdf_train[column] = mdf_train[column] / std\n",
        "  mdf_test[column] = mdf_test[column] / std\n",
        "\n",
        "\n",
        "  return mdf_train, mdf_test\n",
        "  \n",
        "\n",
        "  \n",
        "#process_binary_class(mdf, column, missing)\n",
        "#converts binary classification values to 0 or 1\n",
        "#takes as arguement a pandas dataframe (mdf), \\\n",
        "#the name of the column string ('column') \\\n",
        "#and the string classification to assign to missing data ('missing')\n",
        "#replaces original specified column in dataframe\n",
        "#returns transformed dataframe\n",
        "\n",
        "#missing category must be identical to one of the two existing categories\n",
        "#returns error message if more than two categories remain\n",
        "\n",
        "\n",
        "def process_binary_class(mdf, column, missing):\n",
        "    \n",
        "  #replace missing data with specified classification\n",
        "  mdf[column] = mdf[column].fillna(missing)\n",
        "\n",
        "  #if more than two remaining classifications, return error message    \n",
        "  if len(mdf[column].unique()) > 2:\n",
        "      print('ERROR: number of categories in column for process_binary_class() call >2')\n",
        "      return mdf\n",
        "\n",
        "  #convert column to binary 0/1 classification\n",
        "  lb = preprocessing.LabelBinarizer()\n",
        "  mdf[column] = lb.fit_transform(mdf[column])\n",
        "\n",
        "  return mdf\n",
        "\n",
        "  \n",
        "#process_text_class(mdf_train, mdf_test, column)\n",
        "#preprocess column with text classifications\n",
        "#takes as arguement two pandas dataframe containing training and test data respectively \n",
        "#(mdf_train, mdf_test), and the name of the column string ('column')\n",
        "\n",
        "#note this trains both training and test data simultaneously due to unique treatment if any category\n",
        "#missing from training set but not from test set to ensure consistent formatting \n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with onehot encodings\n",
        "#with columns named after column_ + text classifications\n",
        "#missing data replaced with category label 'missing'+column\n",
        "#any categories missing from the training set removed from test set\n",
        "#any category present in training but missing from test set given a column of zeros for consistent formatting\n",
        "#ensures order of all new columns consistent between both sets\n",
        "#returns two transformed dataframe (mdf_train, mdf_test) \\\n",
        "#and a list of the new column names (textcolumns)\n",
        "\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "\n",
        "def process_text_class(mdf_train, mdf_test, column):\n",
        "\n",
        "  #replace NA with a dummy variable\n",
        "  mdf_train[column] = mdf_train[column].fillna('_missing')\n",
        "  mdf_test[column] = mdf_test[column].fillna('_missing')\n",
        "\n",
        "\n",
        "  #extract categories for column labels\n",
        "  #note that .unique() extracts the labels as a numpy array\n",
        "  labels_train = mdf_train[column].unique()\n",
        "  labels_train.sort(axis=0)\n",
        "  labels_test = mdf_test[column].unique()\n",
        "  labels_test.sort(axis=0)\n",
        "\n",
        "  #transform text classifications to numerical id\n",
        "  encoder = LabelEncoder()\n",
        "  cat_train = mdf_train[column]\n",
        "  cat_train_encoded = encoder.fit_transform(cat_train)\n",
        "\n",
        "  cat_test = mdf_test[column]\n",
        "  cat_test_encoded = encoder.fit_transform(cat_test)\n",
        "\n",
        "\n",
        "  #apply onehotencoding\n",
        "  onehotencoder = OneHotEncoder()\n",
        "  cat_train_1hot = onehotencoder.fit_transform(cat_train_encoded.reshape(-1,1))\n",
        "  cat_test_1hot = onehotencoder.fit_transform(cat_test_encoded.reshape(-1,1))\n",
        "\n",
        "  #append column header name to each category listing\n",
        "  #note the iteration is over a numpy array hence the [...] approach\n",
        "  labels_train[...] = column + '_' + labels_train[...]\n",
        "  labels_test[...] = column + '_' + labels_test[...]\n",
        "\n",
        "\n",
        "  #convert sparse array to pandas dataframe with column labels\n",
        "  df_train_cat = pd.DataFrame(cat_train_1hot.toarray(), columns=labels_train)\n",
        "  df_test_cat = pd.DataFrame(cat_test_1hot.toarray(), columns=labels_test)\n",
        "\n",
        "\n",
        "  #Get missing columns in test set that are present in training set\n",
        "  missing_cols = set( df_train_cat.columns ) - set( df_test_cat.columns )\n",
        "  #Add a missing column in test set with default value equal to 0\n",
        "  for c in missing_cols:\n",
        "      df_test_cat[c] = 0\n",
        "  #Ensure the order of column in the test set is in the same order than in train set\n",
        "  #Note this also removes categories in test set that aren't present in training set\n",
        "  df_test_cat = df_test_cat[df_train_cat.columns]\n",
        "\n",
        "\n",
        "  #concatinate the sparse set with the rest of our training data\n",
        "  mdf_train = pd.concat([df_train_cat, mdf_train], axis=1)\n",
        "  mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)\n",
        "\n",
        "\n",
        "  #delete original column from training data\n",
        "  del mdf_train[column]    \n",
        "  del mdf_test[column]\n",
        "  \n",
        "  #**************************\n",
        "  #to support the ML infill method, I am updating this function to include an \\\n",
        "  #additional output of a list of the created column names\n",
        "  \n",
        "  textcolumns = labels_train\n",
        "  \n",
        "  #***************\n",
        "  \n",
        "\n",
        "  return mdf_train, mdf_test, textcolumns\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rnoan3niU53R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#process_time_class(mdf_train, mdf_test, column)\n",
        "#preprocess column with time classifications\n",
        "#takes as arguement two pandas dataframe containing training and test data respectively \n",
        "#(mdf_train, mdf_test), and the name of the column string ('column')\n",
        "\n",
        "#note this trains both training and test data simultaneously due to unique treatment if any category\n",
        "#missing from training set but not from test set to ensure consistent formatting \n",
        "\n",
        "#deletes the original column from master dataframe and\n",
        "#replaces with distinct columns for year, month, day, hour, minute, second\n",
        "#each normalized to the mean and std, with missing values plugged with the mean\n",
        "#with columns named after column_ + time category\n",
        "#returns two transformed dataframe (mdf_train, mdf_test)\n",
        "\n",
        "#if only have training but not test data handy, use same training data for both dataframe inputs\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "def process_time_class(mdf_train, mdf_test, column):\n",
        "  \n",
        "  #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data\n",
        "  mdf_train[column] = pd.to_datetime(mdf_train[column], errors = 'coerce')\n",
        "  mdf_test[column] = pd.to_datetime(mdf_test[column], errors = 'coerce')\n",
        "  \n",
        "  #mdf_train[column].replace(-np.Inf, np.nan)\n",
        "  #mdf_test[column].replace(-np.Inf, np.nan)\n",
        "  \n",
        "  #get mean of various categories of datetime objects to use to plug in missing cells\n",
        "  meanyear = mdf_train[column].dt.year.mean()    \n",
        "  meanmonth = mdf_train[column].dt.month.mean()\n",
        "  meanday = mdf_train[column].dt.day.mean()\n",
        "  meanhour = mdf_train[column].dt.hour.mean()\n",
        "  meanminute = mdf_train[column].dt.minute.mean()\n",
        "  meansecond = mdf_train[column].dt.second.mean()\n",
        "  \n",
        "  #get standard deviation of training data\n",
        "  stdyear = mdf_train[column].dt.year.std()  \n",
        "  stdmonth = mdf_train[column].dt.month.std()\n",
        "  stdday = mdf_train[column].dt.day.std()\n",
        "  stdhour = mdf_train[column].dt.hour.std()\n",
        "  stdminute = mdf_train[column].dt.minute.std()\n",
        "  stdsecond = mdf_train[column].dt.second.std()\n",
        "  \n",
        "  \n",
        "  #create new columns for each category in train set\n",
        "  mdf_train[column + '_year'] = mdf_train[column].dt.year\n",
        "  mdf_train[column + '_month'] = mdf_train[column].dt.month\n",
        "  mdf_train[column + '_day'] = mdf_train[column].dt.day\n",
        "  mdf_train[column + '_hour'] = mdf_train[column].dt.hour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column].dt.minute\n",
        "  mdf_train[column + '_second'] = mdf_train[column].dt.second\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column].dt.year\n",
        "  mdf_test[column + '_month'] = mdf_test[column].dt.month\n",
        "  mdf_test[column + '_day'] = mdf_test[column].dt.day\n",
        "  mdf_test[column + '_hour'] = mdf_test[column].dt.hour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column].dt.minute \n",
        "  mdf_test[column + '_second'] = mdf_test[column].dt.second\n",
        "  \n",
        "\n",
        "  #replace missing data with training set mean\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(meanyear)\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(meanmonth)\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(meanday)\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(meanhour)\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(meanminute)\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(meanyear)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(meanmonth)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(meanday)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(meanhour)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(meanminute)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(meansecond)\n",
        "  \n",
        "  #subtract mean from column for both train and test\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'] - meanyear\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'] - meanmonth\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'] - meanday\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'] - meanhour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'] - meanminute\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'] - meansecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] - meanyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] - meanmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] - meanday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] - meanhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] - meanminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] - meansecond\n",
        "  \n",
        "  \n",
        "  #divide column values by std for both training and test data\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'] / stdyear\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'] / stdmonth\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'] / stdday\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'] / stdhour\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'] / stdminute\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'] / stdsecond\n",
        "  \n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'] / stdyear\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'] / stdmonth\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'] / stdday\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'] / stdhour\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'] / stdminute\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'] / stdsecond\n",
        "  \n",
        "  \n",
        "  #now replace NaN with 0\n",
        "  mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(0)\n",
        "  mdf_train[column + '_month'] = mdf_train[column + '_month'].fillna(0)\n",
        "  mdf_train[column + '_day'] = mdf_train[column + '_day'].fillna(0)\n",
        "  mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(0)\n",
        "  mdf_train[column + '_minute'] = mdf_train[column + '_minute'].fillna(0)\n",
        "  mdf_train[column + '_second'] = mdf_train[column + '_second'].fillna(0)\n",
        "  \n",
        "  #do same for test set\n",
        "  mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(0)\n",
        "  mdf_test[column + '_month'] = mdf_test[column + '_month'].fillna(0)\n",
        "  mdf_test[column + '_day'] = mdf_test[column + '_day'].fillna(0)\n",
        "  mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(0)\n",
        "  mdf_test[column + '_minute'] = mdf_test[column + '_minute'].fillna(0)\n",
        "  mdf_test[column + '_second'] = mdf_test[column + '_second'].fillna(0)\n",
        "  \n",
        "  \n",
        "  \n",
        "  #this is to address an issue I found when parsing columns with only time no date\n",
        "  #which returned -inf vlaues\n",
        "  checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])\n",
        "  if checkyear:\n",
        "    del mdf_train[column + '_year']\n",
        "    if column + '_year' in mdf_test.columns:\n",
        "      del mdf_test[column + '_year']\n",
        "\n",
        "  checkmonth = np.isinf(mdf_train.iloc[0][column + '_month'])\n",
        "  if checkmonth:\n",
        "    del mdf_train[column + '_month']\n",
        "    if column + '_month' in mdf_test.columns:\n",
        "      del mdf_test[column + '_month']\n",
        "\n",
        "  checkday = np.isinf(mdf_train.iloc[0][column + '_day'])\n",
        "  if checkmonth:\n",
        "    del mdf_train[column + '_day']\n",
        "    if column + '_day' in mdf_test.columns:\n",
        "      del mdf_test[column + '_day']\n",
        "  \n",
        "  \n",
        "  #delete original column from training data\n",
        "  del mdf_train[column]    \n",
        "  if column in mdf_test.columns:\n",
        "    del mdf_test[column]  \n",
        "  \n",
        "  #**************************\n",
        "  #to support the ML infill method, I am updating this function to include an \\\n",
        "  #additional output of a list of the created column names\n",
        "  \n",
        "  datecolumns = [column + '_year', column + '_month', column + '_day', \\\n",
        "                column + '_hour', column + '_minute', column + '_second']\n",
        "  \n",
        "  #***************\n",
        "  \n",
        "  \n",
        "  \n",
        "  return mdf_train, mdf_test, datecolumns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "05IuLqR-vU72",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#evalcategory(df, column)\n",
        "#Function that dakes as input a dataframe and associated column id \\\n",
        "#evaluates the contents of cells and classifies the column into one of four categories\n",
        "#category 1, 'bnry', is for columns with only two categorys of text or integer\n",
        "#category 2, 'nmbr', is for columns with numerical integer or float values\n",
        "#category 3, 'text', is for columns with multiple categories appropriate for one-hot\n",
        "#category 4, 'date', is for columns with Timestamp data\n",
        "#returns category id as a string\n",
        "\n",
        "import collections\n",
        "import datetime as dt\n",
        "\n",
        "def evalcategory(df, column):\n",
        "  \n",
        "  \n",
        "  #I couldn't find a good pandas tool for evaluating data class, \\\n",
        "  #So will iterate an array through each row of the dataframe column and \\\n",
        "  #evaluation for most common variable using the collections library \\\n",
        "  #this probably isn't extremely efficient for big data scale\n",
        "  \n",
        "  #the if/else here is to address a bug I found when iterating through \\\n",
        "  #in a dataframe with single column vs one with multiple columns\n",
        "  \n",
        "  array = []\n",
        "  \n",
        "  if df.shape[1] > 1:\n",
        "    for index, row in df.iterrows():\n",
        "      array = np.append(array, type(row[column]))\n",
        "      \n",
        "  else:\n",
        "    for row in df.iterrows():\n",
        "      array = np.append(array, type(row[0]))\n",
        "\n",
        "  c = collections.Counter(array)\n",
        "  mc = c.most_common(1)\n",
        "  mc2 = c.most_common(2)\n",
        "  \n",
        "  #additional array needed to check for time series\n",
        "  datearray = []\n",
        "  \n",
        "  if df.shape[1] > 1:\n",
        "    for index, row in df.iterrows():\n",
        "      datearray = np.append(datearray,type(pd.to_datetime(row[column], errors = 'coerce')))\n",
        "  \n",
        "  else:\n",
        "    for row in df.iterrows():\n",
        "      datearray = np.append(datearray,type(pd.to_datetime(row[0], errors = 'coerce')))\n",
        "  \n",
        "  datec = collections.Counter(datearray)\n",
        "  datemc = datec.most_common(1)\n",
        "  datemc2 = datec.most_common(2)\n",
        "  \n",
        "  #an extension of this approach could be for those columns that produce a text\\\n",
        "  #category to implement an additional text to determine the number of \\\n",
        "  #common groupings / or the amount of uniquity. For example if every row has\\\n",
        "  #a unique value then one-hot-encoding would not be appropriate. It would \\\n",
        "  #probably be apopropraite to either return an error message if this is found \\\n",
        "  #or alternatively find a furhter way to automate this processing such as \\\n",
        "  #look for contextual clues to groupings that can be inferred.\n",
        "    \n",
        "  #This is kind of hack to evaluate class by comparing these with output of mc\n",
        "  checkint = 1\n",
        "  checkfloat = 1.1\n",
        "  checkstring = 'string'\n",
        "  checkNAN = None\n",
        "\n",
        "  #there's probably easier way to do this, here will create a check for date\n",
        "  df_checkdate = pd.DataFrame([{'checkdate' : '7/4/2018'}])\n",
        "  df_checkdate['checkdate'] = pd.to_datetime(df_checkdate['checkdate'], errors = 'coerce')\n",
        "  \n",
        "\n",
        "  #create dummy variable to store determined class (default is text class)\n",
        "  category = 'text'\n",
        "\n",
        "\n",
        "  #if most common in column is string and > two values, set category to text\n",
        "  if isinstance(checkstring, mc[0][0]) and df[column].nunique() > 2:\n",
        "    category = 'text'\n",
        "  \n",
        "  #if most common is date, set category to date\n",
        "  if isinstance(df_checkdate['checkdate'][0], datemc[0][0]):\n",
        "    category = 'date'\n",
        "  \n",
        "  #if most common in column is integer and > two values, set category to number\n",
        "  if isinstance(checkint, mc[0][0]) and df[column].nunique() > 2:\n",
        "    category = 'nmbr'\n",
        "    \n",
        "  #if most common in column is float, set category to number\n",
        "  if isinstance(checkfloat, mc[0][0]):\n",
        "    category = 'nmbr'\n",
        "  \n",
        "  #if most common in column is integer and <= two values, set category to binary\n",
        "  if isinstance(checkint, mc[0][0]) and df[column].nunique() <= 2:\n",
        "    category = 'bnry'\n",
        "  \n",
        "  #if most common in column is string and <= two values, set category to binary\n",
        "  if isinstance(checkstring, mc[0][0]) and df[column].nunique() <= 2:\n",
        "    category = 'bnry'\n",
        "    \n",
        "      \n",
        "  #if most common in column is NaN, re-evaluate using the second most common type\n",
        "  if 2 * df.isnull().sum()[0] >= df.shape[0]:\n",
        "    \n",
        "    #if 2nd most common in column is string and > two values, set category to text\n",
        "    if isinstance(checkstring, mc2[1][0]) and df[column].nunique() > 2:\n",
        "      category = 'text'\n",
        "  \n",
        "    #if 2nd most common is date, set category to date   \n",
        "    if isinstance(df_checkdate['checkdate'][0], datemc2[0][0]):\n",
        "      category = 'date'\n",
        "  \n",
        "    #if 2nd most common in column is integer and > two values, set category to number\n",
        "    if isinstance(checkint, mc2[1][0]) and df[column].nunique() > 2:\n",
        "      category = 'nmbr'\n",
        "    \n",
        "    #if 2nd most common in column is float, set category to number\n",
        "    if isinstance(checkfloat, mc2[1][0]):\n",
        "      category = 'nmbr'\n",
        "  \n",
        "    #if 2nd most common in column is integer and <= two values, set category to binary\n",
        "    if isinstance(checkint, mc2[1][0]) and df[column].nunique() <= 2:\n",
        "      category = 'bnry'\n",
        "  \n",
        "    #if 2nd most common in column is string and <= two values, set category to binary\n",
        "    if isinstance(checkstring, mc2[1][0]) and df[column].nunique() <= 2:\n",
        "      category = 'bnry'\n",
        "    \n",
        "     \n",
        "  \n",
        "  return category"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Nd6_y_jVcO3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2) Define new functions"
      ]
    },
    {
      "metadata": {
        "id": "VCm4Vo6zVlv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#********************\n",
        "\n",
        "#NArows(df, column), function that when fed a dataframe, \\\n",
        "#column id, and category label outputs a single column dataframe composed of \\\n",
        "#True and False with the same number of rows as the input and the True's \\\n",
        "#coresponding to those rows of the input that had missing or NaN data. This \\\n",
        "#output can later be used to identify which rows for a column to infill with ML\\\n",
        "# derived plug data\n",
        "\n",
        "\n",
        "def NArows(df, column, category):\n",
        "  \n",
        "\n",
        "  \n",
        "  if category == 'text':\n",
        "  \n",
        "    #returns dataframe of True and False, where True coresponds to the NaN's\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.isna(df[column])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "  \n",
        "  if category == 'bnry':\n",
        "    \n",
        "    #returns dataframe of True and False, where True coresponds to the NaN's\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.isna(df[column])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "    \n",
        "  if category == 'nmbr':\n",
        "  \n",
        "    #convert all values to either numeric or NaN\n",
        "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "    \n",
        "    #returns dataframe of True and False, where True coresponds to the NaN's\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.isna(df[column])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "    \n",
        "  if category == 'date':\n",
        "    \n",
        "    #returns dataframe column of all False\n",
        "    #renames column name to column + '_NArows'\n",
        "    NArows = pd.DataFrame(False, index=np.arange(df.shape[0]), columns=[column+'NA'])\n",
        "    NArows = pd.DataFrame(NArows)\n",
        "    NArows = NArows.rename(columns = {column:column+'_NArows'})\n",
        "  \n",
        "\n",
        "  return NArows\n",
        "\n",
        "\n",
        "#createMLinfillsets(df_train, df_test, column, trainNArows, testNArows, \\\n",
        "#category, textcolumnslist = []) function that when fed dataframes of train and\\\n",
        "#test sets, column id, df of True/False corresponding to rows from original \\\n",
        "#sets with missing values, a string category of 'text', 'date', 'nmbr', or \\\n",
        "#'bnry', and a list of column id's for the text category. The \\\n",
        "#function returns a seris of dataframes which can be applied to training a \\\n",
        "#machine learning model to predict apppropriate infill values for those points \\\n",
        "#that had missing values from the original sets, indlucing returns of \\\n",
        "#df_train_filltrain, df_train_filllabel, df_train_fillfeatures, \\\n",
        "#and df_test_fillfeatures\n",
        "\n",
        "\n",
        "def createMLinfillsets(df_train, df_test, column, trainNArows, testNArows, \\\n",
        "                       category, textcolumnslist = []):\n",
        "\n",
        "  \n",
        "  #create 3 new dataframes for each train column - the train and labels \\\n",
        "  #for rows not needing infill, and the features for rows needing infill \\\n",
        "  #also create a test features column \n",
        "\n",
        "  #reminder:\n",
        "    #for numerical there won't be a new column\n",
        "    #for binary there won't be a new column\n",
        "    #for text the new column has a defined name as column+'_missing'\n",
        "\n",
        "  #note that for text class the labels will be a little more complicated \\\n",
        "  #since will be multi-column\n",
        "\n",
        "  if category == 'nmbr' or category == 'bnry':\n",
        "\n",
        "    #first concatinate the NArows True/False designations to df_train & df_test\n",
        "    df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "    df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "    \n",
        "    #create copy of df_train to serve as training set for fill\n",
        "    df_train_filltrain = df_train.copy()\n",
        "    #now delete rows coresponding to True\n",
        "    df_train_filltrain = df_train_filltrain[df_train_filltrain[column+'_NArows'] == False]\n",
        "    \n",
        "    #now delete [column] and the NA labels (column+'NA') from this df\n",
        "    df_train_filltrain = df_train_filltrain.drop([column, column+'_NArows'], axis=1)\n",
        "    \n",
        "    #create a copy of df_train[column] for fill train labels\n",
        "    df_train_filllabel = pd.DataFrame(df_train[column].copy())\n",
        "    #concatinate with the NArows\n",
        "    df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "    #drop rows corresponding to True\n",
        "    df_train_filllabel = df_train_filllabel[df_train_filllabel[column+'_NArows'] == False]\n",
        "    \n",
        "    #delete the NArows column\n",
        "    df_train_filllabel = df_train_filllabel.drop([column+'_NArows'], axis=1)\n",
        "\n",
        "    #create features df_train for rows needing infill\n",
        "    #create copy of df_train (note it already has NArows included)\n",
        "    df_train_fillfeatures = df_train.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[column+'_NArows'])]\n",
        "    #delete column and column+'_NArows'\n",
        "    df_train_fillfeatures = df_train_fillfeatures.drop([column, column+'_NArows'], axis=1)\n",
        "\n",
        "    #create features df_test for rows needing infill\n",
        "    #create copy of df_test (note it already has NArows included)\n",
        "    df_test_fillfeatures = df_test.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[column+'_NArows'])]\n",
        "    #delete column and column+'_NArows'\n",
        "    df_test_fillfeatures = df_test_fillfeatures.drop([column, column+'_NArows'], axis=1)\n",
        "    \n",
        "\n",
        "    #delete NArows from df_train, df_test\n",
        "    df_train = df_train.drop([column+'_NArows'], axis=1)\n",
        "    df_test = df_test.drop([column+'_NArows'], axis=1)\n",
        "\n",
        "\n",
        "  if category == 'text':\n",
        "\n",
        "    #first concatinate the NArows True/False designations to df_train & df_test\n",
        "    df_train = pd.concat([df_train, trainNArows], axis=1)\n",
        "    df_test = pd.concat([df_test, testNArows], axis=1)\n",
        "\n",
        "    #create copy of df_train to serve as training set for fill\n",
        "    df_train_filltrain = df_train.copy()\n",
        "    #now delete rows coresponding to True\n",
        "    df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns.get_values()[0]] == False]\n",
        "    \n",
        "    #now delete columns = textcolumnslist and the NA labels (orig column+'_NArows') from this df\n",
        "    df_train_filltrain = df_train_filltrain.drop(textcolumnslist, axis=1)\n",
        "    df_train_filltrain = df_train_filltrain.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "    #create a copy of df_train[textcolumnslist] for fill train labels\n",
        "    df_train_filllabel = df_train[textcolumnslist].copy()\n",
        "    #concatinate with the NArows\n",
        "    df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)\n",
        "    #drop rows corresponding to True\n",
        "    df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns.get_values()[0]] == False]\n",
        "    \n",
        "    #delete the NArows column\n",
        "    df_train_filllabel = df_train_filllabel.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "    #create features df_train for rows needing infill\n",
        "    #create copy of df_train (note it already has NArows included)\n",
        "    df_train_fillfeatures = df_train.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns.get_values()[0]])]\n",
        "    #delete textcolumnslist and column+'_NArows'\n",
        "    df_train_fillfeatures = df_train_fillfeatures.drop(textcolumnslist, axis=1)\n",
        "    df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "    #create features df_test for rows needing infill\n",
        "    #create copy of df_test (note it already has NArows included)\n",
        "    df_test_fillfeatures = df_test.copy()\n",
        "    #delete rows coresponding to False\n",
        "    df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns.get_values()[0]])]\n",
        "    #delete column and column+'_NArows'\n",
        "    df_test_fillfeatures = df_test_fillfeatures.drop(textcolumnslist, axis=1)\n",
        "    df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "    #delete NArows from df_train, df_test\n",
        "    df_train = df_train.drop([trainNArows.columns.get_values()[0]], axis=1)\n",
        "    df_test = df_test.drop([testNArows.columns.get_values()[0]], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "  if category == 'date':\n",
        "\n",
        "    #create empty sets for now\n",
        "    #an extension of this method would be to implement a comparable method \\\n",
        "    #for the time category, based on the columns output from the preprocessing\n",
        "    df_train_filltrain = pd.DataFrame({'foo' : []}) \n",
        "    df_train_filllabel = pd.DataFrame({'foo' : []})\n",
        "    df_train_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "    df_test_fillfeatures = pd.DataFrame({'foo' : []})\n",
        "\n",
        "\n",
        "  return df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures\n",
        "\n",
        "\n",
        "#labelbinarizercorrect(npinput, columnslist), function that takes as input the output\\\n",
        "#array from scikit learn's LabelBinarizer() and ensures that the re-encoding is\\\n",
        "#consistent with the original array prior to performing the argmax. This is \\\n",
        "#needed because LabelBinarizer automatically takes two class sets to a binary\\\n",
        "#setting and doesn't account for columns above index of active values based on\\\n",
        "#my understanding. For a large enough dataset this probably won't be an issue \\\n",
        "#but just trying to be thorough. Outputs a one-hot encoded array comparable to \\\n",
        "#the format of our input to argmax.\n",
        "\n",
        "def labelbinarizercorrect(npinput, columnslist):\n",
        "  \n",
        "  \n",
        "  #if our array post application of LabelBinarizer has few coloumns than our \\\n",
        "  #column list then run through these loops\n",
        "  if npinput.shape[1] < len(columnslist):\n",
        "    \n",
        "    #if only one column in our array means LabelEncoder must have binarized \\\n",
        "    #since we already established that there are more columns\n",
        "    if npinput.shape[1] == 1:\n",
        "      \n",
        "      #this transfers from the binary encoding to two columns of one hot\n",
        "      npinput = np.hstack((1 - npinput, npinput))\n",
        "      \n",
        "      np_corrected = npinput\n",
        "      \n",
        "    #if we still have fewer columns than the column list, means we'll need to \\\n",
        "    #pad out with columns containing zeros\n",
        "    if npinput.shape[1] < len(columnslist):\n",
        "      missingcols = len(columnslist) - npinput.shape[1]\n",
        "      append = np.zeros((npinput.shape[0], missingcols))\n",
        "      np_corrected = np.concatenate((npinput, append), axis=1)\n",
        "  \n",
        "  else:\n",
        "    #otherwise just return the input array because it is in good shape\n",
        "    np_corrected = npinput\n",
        "\n",
        "  \n",
        "  return np_corrected\n",
        "\n",
        "\n",
        "\n",
        "#predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "#df_train_fillfeatures, df_test_fillfeatures), function that takes as input \\\n",
        "#a category string, the output of createMLinfillsets(.), and a list of columns \\\n",
        "#produced by a text class preprocessor when applicable and returns \\\n",
        "#predicted infills for the train and test feature sets as df_traininfill, \\\n",
        "#df_testinfill based on derivations using scikit-learn, with the lenth of \\\n",
        "#infill consistent with the number of True values from NArows\n",
        "\n",
        "\n",
        "#imports for numerical class training\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "#imports for binary and text class training\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                  df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                  textcolumnslist = []):\n",
        "  \n",
        "  #a reasonable extension of this funciton would be to allow ML inference with \\\n",
        "  #other ML architectures such a SVM or something SGD based for instance\n",
        "  \n",
        "  #convert dataframes to numpy arrays\n",
        "  np_train_filltrain = df_train_filltrain.values\n",
        "  np_train_filllabel = df_train_filllabel.values\n",
        "  np_train_fillfeatures = df_train_fillfeatures.values\n",
        "  np_test_fillfeatures = df_test_fillfeatures.values\n",
        "  \n",
        "  #ony run the following if we have any rows needing infill\n",
        "  if df_train_fillfeatures.shape[0] > 0:\n",
        "\n",
        "    if category == 'nmbr':\n",
        "\n",
        "      #train linear regression model using scikit-learn for numerical prediction\n",
        "      model = LinearRegression()\n",
        "      model.fit(np_train_filltrain, np_train_filllabel)\n",
        "\n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "#       print('category is nmbr, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'bnry':\n",
        "\n",
        "      #train logistic regression model using scikit-learn for binary classifier\n",
        "      model = LogisticRegression()\n",
        "      model.fit(np_train_filltrain, np_train_filllabel)\n",
        "\n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        np_testinfill = np.array([0])\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = ['infill'])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = ['infill'])\n",
        "\n",
        "#       print('category is bnry, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'text':\n",
        "\n",
        "      #first convert the one-hot encoded set via argmax to a 1D array\n",
        "      np_train_filllabel_argmax = np.argmax(np_train_filllabel, axis=1)\n",
        "\n",
        "      #train logistic regression model using scikit-learn for binary classifier\n",
        "      #with multi_class argument activated\n",
        "      #model = LogisticRegression(multi_class='multinomial')\n",
        "      model = LogisticRegression()\n",
        "      model.fit(np_train_filltrain, np_train_filllabel_argmax)\n",
        "\n",
        "      #predict infill values\n",
        "      np_traininfill = model.predict(np_train_fillfeatures)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = model.predict(np_test_fillfeatures)\n",
        "      else:\n",
        "        #this needs to have same number of columns as text category\n",
        "        np_testinfill = np.zeros(shape=(1,len(textcolumnslist)))\n",
        "\n",
        "      #convert the 1D arrary back to one hot encoding\n",
        "      labelbinarizertrain = preprocessing.LabelBinarizer()\n",
        "      labelbinarizertrain.fit(np_traininfill)\n",
        "      np_traininfill = labelbinarizertrain.transform(np_traininfill)\n",
        "      \n",
        "      #only run following if we have any test rows needing infill\n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        labelbinarizertest = preprocessing.LabelBinarizer()\n",
        "        labelbinarizertest.fit(np_testinfill)\n",
        "        np_testinfill = labelbinarizertest.transform(np_testinfill)\n",
        "\n",
        "\n",
        "\n",
        "      #run function to ensure correct dimensions of re-encoded classifier array\n",
        "      np_traininfill = labelbinarizercorrect(np_traininfill, textcolumnslist)\n",
        "      \n",
        "      if df_test_fillfeatures.shape[0] > 0:\n",
        "        np_testinfill = labelbinarizercorrect(np_testinfill, textcolumnslist)\n",
        "\n",
        "\n",
        "      #convert infill values to dataframe\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = [textcolumnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [textcolumnslist]) \n",
        "\n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "\n",
        "    if category == 'date':\n",
        "\n",
        "      #create empty sets for now\n",
        "      #an extension of this method would be to implement a comparable infill \\\n",
        "      #method for the time category, based on the columns output from the \\\n",
        "      #preprocessing\n",
        "      df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "\n",
        "#       print('category is text, df_traininfill is')\n",
        "#       print(df_traininfill)\n",
        "  \n",
        "  \n",
        "  #else if we didn't have any infill rows let's create some plug values\n",
        "  else:\n",
        "    \n",
        "    if category == 'text':\n",
        "      np_traininfill = np.zeros(shape=(1,len(textcolumnslist)))\n",
        "      np_testinfill = np.zeros(shape=(1,len(textcolumnslist)))\n",
        "      df_traininfill = pd.DataFrame(np_traininfill, columns = [textcolumnslist])\n",
        "      df_testinfill = pd.DataFrame(np_testinfill, columns = [textcolumnslist]) \n",
        "    \n",
        "    else :\n",
        "      df_traininfill = pd.DataFrame({'infill' : [0]}) \n",
        "      df_testinfill = pd.DataFrame({'infill' : [0]}) \n",
        "  \n",
        "  \n",
        "  \n",
        "  return df_traininfill, df_testinfill\n",
        "\n",
        "\n",
        "\n",
        "#insertinfill(df, column, infill, category, NArows, textcolumnslist = [])\n",
        "#function that takes as input a dataframe, column id, category string of either\\\n",
        "#'nmbr'/'text'/'bnry'/'date', a df column of True/False identifiying row id of\\\n",
        "#rows that will recieve infill, and and a list of columns produced by a text \\\n",
        "#class preprocessor when applicable. Replaces the column cells in rows \\\n",
        "#coresponding to the NArows True values with the values from infill, returns\\\n",
        "#the associated transformed dataframe.\n",
        "\n",
        "\n",
        "def insertinfill(df, column, infill, category, NArows, textcolumnslist = []):\n",
        "\n",
        "  \n",
        "  \n",
        "  if category == 'nmbr' or category == 'bnry':\n",
        "    \n",
        "    #create new dataframe for infills wherein the infill values are placed in \\\n",
        "    #rows coresponding to NArows True values and rows coresponding to NArows \\\n",
        "    #False values are filled with a 0\n",
        "    NAarray = []\n",
        "    i=0\n",
        "    for index, row in NArows.iterrows():\n",
        "      if row[column+'_NArows'] == False:\n",
        "        NAarray = np.append(NAarray, 0)\n",
        "      if row[column+'_NArows'] == True:\n",
        "        NAarray = np.append(NAarray, infill.iloc[i]['infill'])\n",
        "        i += 1\n",
        "    df_infill_full = pd.DataFrame(NAarray, columns = ['infill'])\n",
        "      \n",
        "\n",
        "    #concatinate the dataframes df, NArows, and infill\n",
        "    df = pd.concat([df, NArows], axis=1)\n",
        "    df = pd.concat([df, df_infill_full], axis=1)\n",
        "    \n",
        "    #for rows where NArows is true, replace value in column column with the \\\n",
        "    #value from infill column\n",
        "    df.loc[df[column+'_NArows'], column] = df['infill']\n",
        "    \n",
        "    #now delete the helper columns\n",
        "    df = df.drop([column+'_NArows'], axis=1)\n",
        "    df = df.drop(['infill'], axis=1)\n",
        "    \n",
        "    \n",
        "  if category == 'text':  \n",
        "\n",
        "    \n",
        "    #create new dataframe for infills wherein the infill values are placed in \\\n",
        "    #rows coresponding to NArows True values and rows coresponding to NArows \\\n",
        "    #False values are filled with a 0\n",
        "    \n",
        "    #text infill contains multiple columns for each predicted calssification\n",
        "    #which were derived from one-hot encoding the original column in preprocessing\n",
        "    for textcolumnname in textcolumnslist:\n",
        "      \n",
        "      #create newcolumn which will serve as the NArows specific to textcolumnname\n",
        "      df['textNArows'] = NArows\n",
        "      \n",
        "      df['textNArows'] = df['textNArows'].replace(0, False)\n",
        "      df['textNArows'] = df['textNArows'].replace(1, True)\n",
        "      \n",
        "      \n",
        "      #this will give us an infill array specific to textcolumnname without 0's\n",
        "      textarray = []\n",
        "      i=0\n",
        "      for index, row in df.iterrows():\n",
        "        if row['textNArows'] == True:\n",
        "          textarray = np.append(textarray, row[textcolumnname])\n",
        "          i += 1\n",
        "      \n",
        "      #now we'll use a comparable approach as for 'nmbr' and 'bnry'\n",
        "      NAarray = []\n",
        "      i=0\n",
        "      j = infill.columns.get_loc(textcolumnname)\n",
        "      \n",
        "      for index, row in df.iterrows():\n",
        "\n",
        "        if row['textNArows'] == False:\n",
        "          NAarray = np.append(NAarray, 0)\n",
        "        if row['textNArows'] == True:\n",
        "          NAarray = np.append(NAarray, infill.iloc[i][textcolumnname][0])\n",
        "          i += 1\n",
        "      df_infill_full = pd.DataFrame(NAarray, columns = ['infill'])\n",
        "    \n",
        "      #concatinate the dataframes df, NArows, and infill\n",
        "      #note we won't need to concatinate the NArows this time since we created\\\n",
        "      #a column specific one called 'textNArows' which is already in place\n",
        "      df = pd.concat([df, df_infill_full], axis=1)\n",
        "      \n",
        "      #for rows where textNArows is true, replace value in column column with \\\n",
        "      #the value from infill column\n",
        "      df.loc[df['textNArows'], textcolumnname] = df['infill']\n",
        "      \n",
        "      #now delete the helper columns\n",
        "      df = df.drop(['textNArows'], axis=1)\n",
        "      df = df.drop(['infill'], axis=1)\n",
        "  \n",
        "  \n",
        "  if category == 'date':\n",
        "    #this spot reserved for future update to incorporate address of datetime\\\n",
        "    #category data\n",
        "    df = df\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CSWHrHUvYrYo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3) Update automunge(.) function"
      ]
    },
    {
      "metadata": {
        "id": "6gLKZbliVumS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#automunge(df_train, df_test, labels_column, valpercent=0.20, MLinfill = True)\n",
        "#Function that when fed a train and test data set automates the process \\\n",
        "#of evaluating each column for determination and applicaiton of appropriate \\\n",
        "#preprocessing. Takes as arguement pandas dataframes of training and test data \\\n",
        "#(mdf_train), (mdf_test), the name of the column from train set containing \\\n",
        "#labels, a string identifying the ID column for train and test, a value for \\\n",
        "#percent of training data to be applied to a validation set, and a True/False \\\n",
        "#selector to determine if MLinfill methods will be applied to any missing \\\n",
        "#points, and a random seed. (If MLinfill = False, missing points are addressed \\\n",
        "#with mean for numerical, most common value for binary, new coluymn for one-hot \\\n",
        "#encoding, and mean for datetime). Note that the ML method for datetime data is \\\n",
        "#future extension. Based on an evaluation of columns selectively applies one of \\\n",
        "#four preprocessing functions to each. Shuffles the data and splits the training \\\n",
        "#set into train and validation sets. Returns following sets as numpy arrays: \\\n",
        "#train, trainID, labels, validation, validationID, validationlabels, test, testID\n",
        "\n",
        "#Note that this approach assumes that the test data is available at time of training\n",
        "#A different approach may be required if processing of test data is not simultaneous\n",
        "#although one potential solution is to apply this function intiially with a dummy\\\n",
        "#dataframe for test set and then when test data becomes available reapply \\\n",
        "#with original train set used for training the model along with the test set.\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def automunge(df_train, df_test, labels_column, trainID_column = False, \\\n",
        "              testID_column = False, valpercent=0.20, MLinfill = True, \\\n",
        "              randomseed = 42):\n",
        "  \n",
        "  #An extension could be to test the input data here for non-dataframe format \\\n",
        "  #(such as csv) to convert it to pandas within the function. \n",
        "  \n",
        "  #my understanding is it is good practice to convert any None values into NaN \\\n",
        "  #so I'll just get that out of the way\n",
        "  df_train.fillna(value=float('nan'), inplace=True)\n",
        "  df_test.fillna(value=float('nan'), inplace=True)\n",
        "  \n",
        "  #extract the ID columns from train and test set\n",
        "  if trainID_column != False:\n",
        "    df_trainID = pd.DataFrame(df_train[trainID_column])\n",
        "    del df_train[trainID_column]\n",
        "    \n",
        "  if testID_column != False:\n",
        "    df_testID = pd.DataFrame(df_test[testID_column])\n",
        "    del df_test[testID_column]\n",
        "  \n",
        "  #extract labels from train set\n",
        "  #an extension to this function could be to delete the training set rows\\\n",
        "  #where the labels are missing or improperly formatted prior to performing\\\n",
        "  #this step\n",
        "  df_labels = pd.DataFrame(df_train[labels_column])\n",
        "  del df_train[labels_column]\n",
        "  \n",
        "  \n",
        "  #confirm consistency of train an test sets\n",
        "  \n",
        "  #check number of columns is consistent\n",
        "  if df_train.shape[1] != df_test.shape[1]:\n",
        "    print(\"error, different number of columns in train and test sets\")\n",
        "    return\n",
        "  \n",
        "  #check column headers are consistent (this works independent of order)\n",
        "  columns_train = set(list(df_train))\n",
        "  columns_test = set(list(df_test))\n",
        "  if columns_train != columns_test:\n",
        "    print(\"error, different column labels in the train and test set\")\n",
        "    return\n",
        "   \n",
        "  #sort columns alphabetically to ensure same order\n",
        "  #to be honest I'm not positive that this piece is working correctly\n",
        "  df_train = df_train.sort_index(axis=0)\n",
        "  df_test = df_test.sort_index(axis=0)\n",
        "  \n",
        "  \n",
        "  #extract column lists again but this time as a list\n",
        "  columns_train = list(df_train)\n",
        "  columns_test = list(df_test)\n",
        "\n",
        "  \n",
        "  #create an empty dataframe to serve as a store for each column's NArows\n",
        "  #the column id's for this df will follow convention from NArows of \n",
        "  #column+'_NArows' for each column in columns_train\n",
        "  #these are used in the ML infill methods\n",
        "  masterNArows_train = pd.DataFrame()\n",
        "  masterNArows_test = pd.DataFrame()\n",
        "  \n",
        "  #create an empty dictionary to serve as a store for each column's category\n",
        "  #this dictionary will store the key of the original column id with entry of \\\n",
        "  #the associated category string - these are used in the ML infill methods\n",
        "  mastercategory_dict = {}\n",
        "  \n",
        "  #create an empty dictionary to serve as a store specific to the text category\n",
        "  #our entries to this dictionary will store a master key from each point in \\\n",
        "  #the textcolumns array, with a nested name of original column under \\\n",
        "  #'origcolumn', the full textcolumns array under 'textcolumnsarray', and a \\\n",
        "  #True/False marker we'll call 'infillcomplete' for use in the ML infill methods\n",
        "  text_dict = {}\n",
        "  \n",
        "  #create an empty dictionary to serve as a store specific to the date category\n",
        "  #our entries to this dictionary will store a master key from each point in \\\n",
        "  #the datecolumns array, with a nested name of original column under \\\n",
        "  #'origcolumn', the full datecolumns array under 'textcolumnsarray', and a \\\n",
        "  #True/False marker we'll call 'infillcomplete' for use in the ML infill methods\n",
        "  date_dict = {}\n",
        "  \n",
        "\n",
        "  \n",
        "  #For each column, determine appropriate processing function\n",
        "  #processing function will be based on evaluation of train set\n",
        "  for column in columns_train:\n",
        "\n",
        "    category = evalcategory(df_train, column)\n",
        "    \n",
        "    #let's make sure the category is consistent between train and test sets\n",
        "    category_test = evalcategory(df_test, column)\n",
        "    if category != category_test:\n",
        "      print('error - different category between train and test sets for column ',\\\n",
        "           column)\n",
        "    \n",
        "    \n",
        "    \n",
        "    #append this category onto our mastercategory_dict\n",
        "    mastercategory_dict.update({column+'cat': category})\n",
        "    \n",
        "    \n",
        "\n",
        "    #create NArows (column of True/False where True coresponds to missing data)\n",
        "    trainNArows = NArows(df_train, column, category)\n",
        "    testNArows = NArows(df_test, column, category)\n",
        "    \n",
        "    #now append that NArows onto a master NA rows df\n",
        "    masterNArows_train = pd.concat([masterNArows_train, trainNArows], axis=1)\n",
        "    masterNArows_test = pd.concat([masterNArows_test, testNArows], axis=1)\n",
        "    \n",
        "\n",
        "    #(now normalize as would normally)\n",
        "\n",
        "    \n",
        "\n",
        "    #for binary class use the majority field for missing plug value\n",
        "    if category == 'bnry':\n",
        "      binary_missing_plug = df_train[column].value_counts().index.tolist()[0]\n",
        "    \n",
        "    \n",
        "    #apply appropriate processing function to this column based on the result\n",
        "    if category == 'bnry':\n",
        "      df_train = process_binary_class(df_train, column, binary_missing_plug)\n",
        "      df_test = process_binary_class(df_test, column, binary_missing_plug)\n",
        "      \n",
        "    if category == 'nmbr':\n",
        "      df_train, df_test = process_numerical_class(df_train, df_test, column)\n",
        "      \n",
        "    if category == 'text':\n",
        "      df_train, df_test, textcolumns = process_text_class(df_train, df_test, column)\n",
        "      \n",
        "\n",
        "      #store some values in the text_dict{} for use later in ML infill methods\n",
        "      \n",
        "      for tc in textcolumns:\n",
        "        text_dict.update({tc : {'origcolumn' : column, 'textcolumnsarray' : \\\n",
        "                               textcolumns, 'infillcomplete' : False}})\n",
        "\n",
        "      \n",
        "    \n",
        "    if category == 'date':\n",
        "      df_train, df_test, datecolumns = process_time_class(df_train, df_test, column)\n",
        "      \n",
        "      #store some values in the date_dict{} for use later in ML infill methods\n",
        "      \n",
        "      for dc in datecolumns:\n",
        "        date_dict.update({dc : {'origcolumn' : column, 'textcolumnsarray' : \\\n",
        "                               datecolumns, 'infillcomplete' : False}})\n",
        "\n",
        "\n",
        "  \n",
        "  #now that we've pre-processed all of the columns, let's run through them again\\\n",
        "  #using ML to derive plug values for the previously missing cells\n",
        "    \n",
        "  \n",
        "  if MLinfill == True:\n",
        "    \n",
        "    columns_train_ML = list(df_train)\n",
        "    columns_test_ML = list(df_test)\n",
        "    \n",
        "\n",
        "    for column in columns_train_ML:\n",
        "      \n",
        "      #If column id is found in the text_dict then will require different \\\n",
        "      #type of address since this category won't be found in our \\\n",
        "      #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "      #collective group of columns from the associated textcolumns array.\n",
        "\n",
        "      if column in text_dict:\n",
        "\n",
        "        #check the status of dictionary's infillcomplete marker for this column\n",
        "        if text_dict[column]['infillcomplete'] == False:\n",
        "\n",
        "          #pull this column's textcolumns array\n",
        "          textcolumns = text_dict[column]['textcolumnsarray']\n",
        "\n",
        "          category = 'text'\n",
        "\n",
        "          #now let's apply our functions for ML infill\n",
        "\n",
        "          #createMLinfillsets(df_train, df_test, column, trainNArows, \\\n",
        "          #testNArows, category, textcolumnslist = []), return \\\n",
        "          #df_train_filltrain, df_train_filllabel, df_train_fillfeatures, \\\n",
        "          #df_test_fillfeatures\n",
        "          df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures = \\\n",
        "          createMLinfillsets(df_train, df_test, column, pd.DataFrame(masterNArows_train[text_dict[column]['origcolumn']+'_NArows']), \\\n",
        "                             pd.DataFrame(masterNArows_test[text_dict[column]['origcolumn']+'_NArows']), category, \\\n",
        "                             textcolumnslist = textcolumns)          \n",
        "\n",
        "\n",
        "          #predict infill values using defined function predictinfill(.)\n",
        "          df_traininfill, df_testinfill = \\\n",
        "          predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                        df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                        textcolumnslist = textcolumns)\n",
        "\n",
        "          #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "          #to df's associated column\n",
        "          df_train = insertinfill(df_train, column, df_traininfill, category, \\\n",
        "                                  pd.DataFrame(masterNArows_train[text_dict[column]['origcolumn']+'_NArows']), \\\n",
        "                                  textcolumnslist = textcolumns)\n",
        "\n",
        "          df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                 pd.DataFrame(masterNArows_test[text_dict[column]['origcolumn']+'_NArows']), \\\n",
        "                                 textcolumnslist = textcolumns)\n",
        "\n",
        "          #now change the infillcomplete marker in the text_dict for each \\\n",
        "          #associated text column\n",
        "          for textcolumnname in textcolumns:\n",
        "            text_dict[textcolumnname]['infillcomplete'] = True\n",
        "\n",
        "      \n",
        "      #If column id is found in the date_dict then will require different \\\n",
        "      #type of address since this category won't be found in our \\\n",
        "      #mastercategory_dict and we'll need to apply the ML infill to the \\\n",
        "      #collective group of columns from the associated datecolumns array. \\\n",
        "      #The development of this address for date columns is a future extension.\n",
        "      elif column in date_dict:\n",
        "        \n",
        "        #this section to be a future extension.\n",
        "        pass\n",
        "      \n",
        "        #this is for columns that weren't found in the text_dict or date_dict\n",
        "      else:\n",
        "\n",
        "        #For each column, determine appropriate processing function\n",
        "        #processing function will be based on evaluation of train set\n",
        "\n",
        "        #pull category from dictionary\n",
        "        category = mastercategory_dict[column+'cat']\n",
        "\n",
        "\n",
        "        #createMLinfillsets(df_train, df_test, column, trainNArows, testNArows, category, textcolumnslist = [])\n",
        "        #return df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures\n",
        "\n",
        "        if category == 'nmbr' or category == 'bnry': #or category == 'date'\n",
        "          df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures = \\\n",
        "          createMLinfillsets(df_train, df_test, column, pd.DataFrame(masterNArows_train[column+'_NArows']), \\\n",
        "                             masterNArows_test[column+'_NArows'], category)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #predict infill values using defined function predictinfill(.)\n",
        "\n",
        "        if category == 'nmbr' or category == 'bnry': #or category == 'date':\n",
        "          df_traininfill, df_testinfill = \\\n",
        "          predictinfill(category, df_train_filltrain, df_train_filllabel, \\\n",
        "                        df_train_fillfeatures, df_test_fillfeatures, \\\n",
        "                        textcolumnslist = [])\n",
        "\n",
        "\n",
        "\n",
        "        #apply the function insertinfill(.) to insert missing value predicitons \\\n",
        "        #to df's associated column\n",
        "\n",
        "        if category == 'nmbr' or category == 'bnry': #or category == 'date':\n",
        "          df_train = insertinfill(df_train, column, df_traininfill, category, \\\n",
        "                                  pd.DataFrame(masterNArows_train[column+'_NArows']), \\\n",
        "                                  textcolumnslist = [])\n",
        "\n",
        "          df_test = insertinfill(df_test, column, df_testinfill, category, \\\n",
        "                                 pd.DataFrame(masterNArows_test[column+'_NArows']), \\\n",
        "                                 textcolumnslist = [])\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  #determine labels category and apply appropriate function\n",
        "  labelscategory = evalcategory(df_labels, labels_column)\n",
        "  \n",
        "  #empty dummy labels \"test\" df for our preprocessing functions\n",
        "  labelsdummy = pd.DataFrame()\n",
        "  \n",
        "  #apply appropriate processing function to this column based on the result\n",
        "  if labelscategory == 'bnry':\n",
        "    labels_binary_missing_plug = df_labels[labels_column].value_counts().index.tolist()[0]\n",
        "    df_labels = process_binary_class(df_labels, labels_column, labels_binary_missing_plug)\n",
        "      \n",
        "  if labelscategory == 'nmbr':\n",
        "    df_labels, labelsdummy = process_numerical_class(df_labels, labelsdummy, labels_column)\n",
        " \n",
        "  #it occurs to me there might be an argument for preferring a single numerical \\\n",
        "  #classifier for labels to keep this to a single column, if so scikitlearn's \\\n",
        "  #LabelEcncoder could be used here, will assume that onehot encoding is acceptable\n",
        "  if labelscategory == 'text':\n",
        "    df_labels, labelsdummy, labelcolumnsdummy = process_text_class(df_labels, labelsdummy, column)\n",
        "  \n",
        "\n",
        "  \n",
        "  #great the data is processed now let's do a few moore global training preps\n",
        "  \n",
        "  #convert all of our dataframes to numpy arrays (train, test, labels, and ID)\n",
        "  #    df_trainID, df_testID\n",
        "  np_train = df_train.values\n",
        "  np_test = df_test.values\n",
        "  np_labels = df_labels.values\n",
        "  \n",
        "  if trainID_column != False:\n",
        "    np_trainID = df_trainID.values\n",
        "  if testID_column != False:\n",
        "    np_testID = df_testID.values\n",
        "  \n",
        "  \n",
        "  #set randomness seed number\n",
        "  answer = randomseed\n",
        "  #a reasonable extension would be to tie this in with randomness seed for \\\n",
        "  #ML infill methods calls to scikit learn\n",
        "  \n",
        "  #shuffle training set and labels\n",
        "  np_train = shuffle(np_train, random_state = answer)\n",
        "  np_test = shuffle(np_test, random_state = answer)\n",
        "  np_labels = shuffle(np_labels, random_state = answer)\n",
        "  \n",
        "  if trainID_column != False:\n",
        "    np_trainID = shuffle(np_trainID, random_state = answer)\n",
        "  if testID_column != False:\n",
        "    np_testID = shuffle(np_testID, random_state = answer)\n",
        "  \n",
        "  \n",
        "  #split validation sets from training and labels\n",
        "  train, validation, labels, validationlabels = \\\n",
        "  train_test_split(np_train, np_labels, test_size=valpercent, shuffle = False)\n",
        "  \n",
        "  if trainID_column != False:\n",
        "    trainID, validationID = \\\n",
        "    train_test_split(np_trainID, test_size=valpercent, shuffle = False)\n",
        "  else:\n",
        "    trainID = []\n",
        "    validationID = []\n",
        "  if testID_column != False:\n",
        "    testID = np_testID\n",
        "  else:\n",
        "    testID = []\n",
        "  \n",
        "  test = np_test\n",
        "  \n",
        "  \n",
        "  #a reasonable extension would be to perform some validation functions on the\\\n",
        "  #sets here (or also prior to transofrm to numpuy arrays) and confirm things \\\n",
        "  #like consistency between format of columns and data between our train and \\\n",
        "  #test sets and if any issues return a coresponding error message to alert user\n",
        "  \n",
        "  \n",
        "  return train, trainID, labels, validation, validationID, validationlabels, test, testID\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyThHKNQZOar",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4) Test our functions"
      ]
    },
    {
      "metadata": {
        "id": "ReCBHoBAY9wr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#create sample test and train data for demonstration purposes\n",
        "\n",
        "#train data set from list of dictionaries\n",
        "#24 rows\n",
        "train = [{'ID' : 101, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 102, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 103, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 104, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 105, 'number': -1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 106, 'number': 'Q', 'Y-N': 'N', 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "         {'ID' : 107, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 108, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 109, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 110, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 111, 'number': -1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 112, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "         {'ID' : 113, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 114, 'number': 2, 'Y-N': None, 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 115, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 116, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 117, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 118, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'}]\n",
        "\n",
        "#convert train data to pandas dataframe\n",
        "df_train = pd.DataFrame(train)\n",
        "\n",
        "\n",
        "#test data set from list of dictionaries\n",
        "#21 rows\n",
        "test = [{'ID' : 1, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID': 2, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 3,'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 4, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 5, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 6, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 7, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 8, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 9, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 10, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 11, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 12, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 13, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 14, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 15, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 16, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 17, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 18, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 19, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 20, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 21, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'}]\n",
        "\n",
        "#convert test data to pandas dataframe\n",
        "df_test = pd.DataFrame(test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bq4i5jCrZbu7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#apply automunge\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, testID = \\\n",
        "automunge(df_train, df_test, labels_column = 'label', trainID_column = 'ID', \\\n",
        "         testID_column = 'ID', MLinfill = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IJJlTcC5ZiqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "ce6132b8-311b-43f7-ba4f-e734f9e521ed"
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "        -0.21691867,  1.12710708, -1.        ,  1.14830512,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         0.5718765 , -0.48304589,  1.        ,  1.14830512,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "         0.88586843,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         2.9819328 , -1.28812238, -1.33333333, -1.19083494,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         1.43955119, -0.48304589,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         0.5718765 , -0.48304589,  1.        ,  1.14830512,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        -1.79450901,  1.12710708,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         1.43955119, -0.48304589,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         2.9819328 , -1.28812238, -1.33333333, -1.19083494,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "         0.88586843,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         1.43955119, -0.48304589,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         2.9819328 , -1.28812238, -1.33333333, -1.19083494,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        -1.79450901,  1.12710708,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "        -0.21691867,  1.12710708, -1.        ,  1.14830512,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "metadata": {
        "id": "iro7NMvPT-gC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#create sample test and train data for demonstration purposes\n",
        "\n",
        "#train data set from list of dictionaries\n",
        "#24 rows\n",
        "train = [{'ID' : 101, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 102, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 103, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 104, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 105, 'number': -1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 106, 'number': 'Q', 'Y-N': 'N', 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "         {'ID' : 107, 'number': 1, 'Y-N': None, 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 108, 'number': 2, 'Y-N': 'N', 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 109, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 110, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 111, 'number': -1, 'Y-N': None, 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 112, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'},\n",
        "         {'ID' : 113, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : '2/12/18', 'label': 'cat'}, \n",
        "         {'ID' : 114, 'number': 2, 'Y-N': None, 'shape': 'square', 'date' : 'August 12, 2016', 'label': 'dog'}, \n",
        "         {'ID' : 115, 'number': None, 'Y-N': 'Y', 'shape': 'circle', 'date' : None, 'label': 'cat'},\n",
        "         {'ID' : 116, 'number': 3.1, 'Y-N': None, 'shape': 'square', 'date' : 'July 4, 2016', 'label': 'cat'}, \n",
        "         {'ID' : 117, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'Jul 4, 2018', 'label': 'dog'}, \n",
        "         {'ID' : 118, 'number': 'Q', 'Y-N': None, 'shape': 'oval', 'date' : '2015', 'label': 'dog'}]\n",
        "\n",
        "#convert train data to pandas dataframe\n",
        "df_train = pd.DataFrame(train)\n",
        "\n",
        "\n",
        "#test data set from list of dictionaries\n",
        "#21 rows\n",
        "test = [{'ID' : 1, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID': 2, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 3,'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 4, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 5, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 6, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 7, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 8, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 9, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 10, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 11, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 12, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 13, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 14, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'},\n",
        "        {'ID' : 15, 'number': 2.1, 'Y-N': 'N', 'shape': 'square', 'date' : '4/14/18'}, \n",
        "        {'ID' : 16, 'number': -1, 'Y-N': 'N', 'shape': None, 'date' : 'August 12, 2016'},\n",
        "        {'ID' : 17, 'number': 1, 'Y-N': 'Y', 'shape': 'circle', 'date' : 'July 4, 2018'}, \n",
        "        {'ID' : 18, 'number': None, 'Y-N': 'Y', 'shape': 'square', 'date' : None}, \n",
        "        {'ID' : 19, 'number': 3, 'Y-N': None, 'shape': 'circle', 'date' : 'Aug 31, 2018'}, \n",
        "        {'ID' : 20, 'number': 0, 'Y-N': 'N', 'shape': 'octogon', 'date' : '2017'}, \n",
        "        {'ID' : 21, 'number': 'Q', 'Y-N': 'Y', 'shape': 'square', 'date' : 'Jan 1, 2019'}]\n",
        "\n",
        "#convert test data to pandas dataframe\n",
        "df_test = pd.DataFrame(test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1geHT1orrbbw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#apply automunge without ML infill, compare results\n",
        "\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, testID = \\\n",
        "automunge(df_train, df_test, labels_column = 'label',  trainID_column = 'ID', \\\n",
        "         testID_column = 'ID', MLinfill = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MK1NhTRfrmdu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "8d00f540-4522-4532-fd5f-df495751d083"
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
              "        -0.21691867,  1.12710708, -1.        ,  1.14830512,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         0.5718765 , -0.48304589,  1.        ,  1.14830512,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        , -1.28812238, -1.33333333, -1.19083494,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         1.43955119, -0.48304589,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         0.5718765 , -0.48304589,  1.        ,  1.14830512,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        -1.79450901,  1.12710708,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         1.43955119, -0.48304589,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        , -1.28812238, -1.33333333, -1.19083494,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         1.43955119, -0.48304589,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        , -1.28812238, -1.33333333, -1.19083494,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        -1.79450901,  1.12710708,  0.66666667, -0.55288765,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
              "        -0.21691867,  1.12710708, -1.        ,  1.14830512,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "metadata": {
        "id": "leMtlYjEro_2",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "b894bf94-07ab-4046-b2d7-c9fa44c462a7"
      },
      "cell_type": "code",
      "source": [
        "#Now let's try a larger dataset, the Titanic dataset from Kaggle\n",
        "#available here: https://www.kaggle.com/c/titanic/data\n",
        "#(which I will upload form my local hard drive)\n",
        "#for more on data imports in Colaboratory see my medium post \n",
        "#https://medium.com/@_NicT_/colaboratorys-free-gpu-72ebc9272933\n",
        "#Following is as presented in the Colaboratory tutorial notebook\n",
        "#Once run this will allow you to manually select the path on local drive for file you wish to upload\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a553b744-215d-410e-ae40-1a554f8ec8a8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a553b744-215d-410e-ae40-1a554f8ec8a8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.csv to train (1).csv\n",
            "User uploaded file \"train.csv\" with length 60302 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "frC9fYhFsPzI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "69208371-ebb7-470f-dcf0-43ff4d2b858b"
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "titanic_train_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "titanic_train_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "4            5         0       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                           Allen, Mr. William Henry    male  35.0      0   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3      0            113803  53.1000  C123        S  \n",
              "4      0            373450   8.0500   NaN        S  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "btRfHQ68seZv",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "180df937-9e13-4da3-85d5-bc7c5d6a7a30"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for train in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=train, length=len(uploaded[train])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-37a1c7f2-6bc9-4e0c-a86f-ace96f87d632\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-37a1c7f2-6bc9-4e0c-a86f-ace96f87d632\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test (1).csv\n",
            "User uploaded file \"test.csv\" with length 28210 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wkXAzqELsiEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "db647259-06b7-4d6a-cce5-98a5ed44db67"
      },
      "cell_type": "code",
      "source": [
        "#Here is some additional detail for converting \n",
        "#the resulting upload into a dataframe\n",
        "\n",
        "from io import BytesIO\n",
        "titanic_test_dforig = pd.read_csv(BytesIO(uploaded[train]), encoding='latin-1')\n",
        "titanic_test_dforig.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>896</td>\n",
              "      <td>3</td>\n",
              "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
              "      <td>female</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3101298</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass                                          Name     Sex  \\\n",
              "0          892       3                              Kelly, Mr. James    male   \n",
              "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
              "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
              "3          895       3                              Wirz, Mr. Albert    male   \n",
              "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
              "\n",
              "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
              "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
              "1  47.0      1      0   363272   7.0000   NaN        S  \n",
              "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
              "3  27.0      0      0   315154   8.6625   NaN        S  \n",
              "4  22.0      1      1  3101298  12.2875   NaN        S  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "i2ZVi5FEqwUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHurufIAsrS2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Now there are certain aspects of feature engineering that our automunge won't address\n",
        "#for example one could extract from the Mrs/Ms/Miss designation in the Name \\\n",
        "#column if a female is married. From Cabin field perhaps we could infer what \\\n",
        "#deck passenger was on or whether they even had a cabin. This type of evaluation \\\n",
        "#would need to be done prior to applicaiton of automunge. Because each column is \\\n",
        "#unique there won't be any learning for Cabin, Name, or Ticket I expect so we'll \\\n",
        "#go ahead and delete those rows for our demonstration. It is certainly \\\n",
        "#feasible that there is some feature buried in these columns that can be \\\n",
        "#extracted prior to applicaiton of automunge. PassengerId will serve as ID column.\n",
        "\n",
        "titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcWlP3Q7ucam",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did, first we'll try \\\n",
        "#without the MLinfill:\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, testID = \\\n",
        "automunge(titanic_train_df, titanic_test_df, labels_column = 'Survived', \\\n",
        "          trainID_column = 'PassengerId', testID_column = 'PassengerId', \\\n",
        "          MLinfill = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dV1YC5zEWs_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "778d2ed4-690f-47a5-8e31-416f2e1f53d1"
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        ,  0.        ,  0.        , ...,  0.43255043,\n",
              "         0.76719899, -0.34126057],\n",
              "       [ 0.        ,  0.        ,  1.        , ..., -0.47427882,\n",
              "        -0.47340772, -0.43676213],\n",
              "       [ 0.        ,  0.        ,  1.        , ..., -0.47427882,\n",
              "        -0.47340772, -0.48857985],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  1.        , ..., -0.47427882,\n",
              "        -0.47340772, -0.48916745],\n",
              "       [ 0.        ,  0.        ,  1.        , ...,  3.15303818,\n",
              "         0.76719899,  0.15058917],\n",
              "       [ 1.        ,  0.        ,  0.        , ..., -0.47427882,\n",
              "         0.76719899, -0.47667284]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "metadata": {
        "id": "QWTHJwXG99me",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#re-initialize the data\n",
        "\n",
        "titanic_train_df = titanic_train_dforig.copy()\n",
        "titanic_test_df = titanic_test_dforig.copy()\n",
        "\n",
        "titanic_train_df = titanic_train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "titanic_test_df = titanic_test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wZOpxDa-5oqp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now let's run our automunge function and see how we did with the MLinfill:\n",
        "\n",
        "train, trainID, labels, validation, validationID, validationlabels, test, testID = \\\n",
        "automunge(titanic_train_df, titanic_test_df, labels_column = 'Survived', \\\n",
        "          trainID_column = 'PassengerId', testID_column = 'PassengerId', \\\n",
        "          MLinfill = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cj3xVzc6MXr_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "95b4240e-4ae6-40e3-fcfb-481d456ed584"
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        ,  0.        ,  0.        , ...,  0.43255043,\n",
              "         0.76719899, -0.34126057],\n",
              "       [ 0.        ,  0.        ,  1.        , ..., -0.47427882,\n",
              "        -0.47340772, -0.43676213],\n",
              "       [ 0.        ,  0.        ,  1.        , ..., -0.47427882,\n",
              "        -0.47340772, -0.48857985],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  1.        , ..., -0.47427882,\n",
              "        -0.47340772, -0.48916745],\n",
              "       [ 0.        ,  0.        ,  1.        , ...,  3.15303818,\n",
              "         0.76719899,  0.15058917],\n",
              "       [ 1.        ,  0.        ,  0.        , ..., -0.47427882,\n",
              "         0.76719899, -0.47667284]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "metadata": {
        "id": "Ki1LInqYWy1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}